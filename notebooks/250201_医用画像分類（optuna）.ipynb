{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b54e9f0a-0d3d-40ba-ba68-b2038abd3a62",
   "metadata": {},
   "source": [
    "# ポートフォリオ「CNNを用いた医用画像分類（病理組織）optunaチューニング」（仲本夏生）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334b7a1b-46bd-4673-844d-c846c5ac1969",
   "metadata": {},
   "source": [
    "## Optunaを用いてのチューニング結果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcb5b1b-dae2-4511-81ec-4e501a4cd00f",
   "metadata": {},
   "source": [
    "- 本ノートブックは、深層学習（CNN）を用いた医用画像分類の前回のポートフォリオをベースに、Optuna によるハイパーパラメータチューニングの結果を示したものである。\n",
    "- 調整対象のパラメータとその調整方法は以下のとおり。前回作成したsmallVGGモデルを用い、2blocks構成または3blocks構成で性能を確認する。\n",
    "    - 学習率（`learning_rate`）: $10^{-4}$から$10^{-2}$の範囲で実数から選択\n",
    "    - 畳み込み層のドロップアウト率（`conv_dropout_rate`）: 0.0~0.3の範囲で実数から選択\n",
    "    - 全結合層のドロップアウト率（`linear_dropout_rate`）: 0.0~0.5の範囲で実数から選択\n",
    "    - バッチ正則化（`batch_norm`）: バッチ正則化のありなしを選択\n",
    "- なお、Docker環境でOptuna Dashbordを用いてインタラクティブに結果を可視化している。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc36aca-33c5-47bf-9e4b-dda4cb841b61",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b03a6cd-4532-4c3a-b739-3bf0d0dce97a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "bf0a2175-8461-469c-8360-76018069ea56",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "0c45d256-a069-4509-f54f-f25d6689a83a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-wdwyoqa7 because the default path (/home/jovyan/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: medmnist in /usr/local/lib/python3.8/dist-packages (3.0.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from medmnist) (1.24.4)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from medmnist) (2.0.3)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from medmnist) (1.3.2)\n",
      "Requirement already satisfied: scikit-image in /usr/local/lib/python3.8/dist-packages (from medmnist) (0.21.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from medmnist) (4.66.5)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from medmnist) (10.4.0)\n",
      "Requirement already satisfied: fire in /usr/local/lib/python3.8/dist-packages (from medmnist) (0.7.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from medmnist) (2.4.1)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from medmnist) (0.19.1)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.8/dist-packages (from fire->medmnist) (2.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from pandas->medmnist) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->medmnist) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.8/dist-packages (from pandas->medmnist) (2025.1)\n",
      "Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.8/dist-packages (from scikit-image->medmnist) (1.10.1)\n",
      "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.8/dist-packages (from scikit-image->medmnist) (3.1)\n",
      "Requirement already satisfied: imageio>=2.27 in /usr/local/lib/python3.8/dist-packages (from scikit-image->medmnist) (2.35.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.8/dist-packages (from scikit-image->medmnist) (2023.7.10)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image->medmnist) (1.4.1)\n",
      "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.8/dist-packages (from scikit-image->medmnist) (24.2)\n",
      "Requirement already satisfied: lazy_loader>=0.2 in /usr/local/lib/python3.8/dist-packages (from scikit-image->medmnist) (0.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->medmnist) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->medmnist) (3.5.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from torch->medmnist) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from torch->medmnist) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch->medmnist) (1.13.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch->medmnist) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from torch->medmnist) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.8/dist-packages (from torch->medmnist) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.8/dist-packages (from torch->medmnist) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.8/dist-packages (from torch->medmnist) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.8/dist-packages (from torch->medmnist) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.8/dist-packages (from torch->medmnist) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.8/dist-packages (from torch->medmnist) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.8/dist-packages (from torch->medmnist) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.8/dist-packages (from torch->medmnist) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.8/dist-packages (from torch->medmnist) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.8/dist-packages (from torch->medmnist) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.8/dist-packages (from torch->medmnist) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.8/dist-packages (from torch->medmnist) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.8/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->medmnist) (12.8.61)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.2->pandas->medmnist) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch->medmnist) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from sympy->torch->medmnist) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/medmnist/info.py:17: UserWarning: Failed to setup default root.\n",
      "  warnings.warn(\"Failed to setup default root.\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import japanize_matplotlib\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "!pip install medmnist\n",
    "import medmnist\n",
    "from medmnist import INFO, Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c43e75ab-2fb8-415d-8793-e34e343cfb88",
   "metadata": {
    "id": "9dcf101b-d5d3-42c4-bf40-09f38b924037"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "# 再現性のためにランダムシードを設定\n",
    "def set_seed(seed):\n",
    "    random.seed(seed) # Python\n",
    "    np.random.seed(seed) # NumPy\n",
    "    torch.manual_seed(seed) # PyTorch\n",
    "    # GPUを使う場合\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)  # 任意のシード値"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36fdebf1-cc7d-4f15-90f1-55ffac4b9164",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "84b9468c-e81c-46c3-a07e-74b4b9923f5d",
    "outputId": "ac71b17a-a6bc-46f3-b7e0-8fa9d7939178"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/medmnist/pathmnist.npz\n",
      "Using downloaded and verified file: ./data/medmnist/pathmnist.npz\n",
      "Using downloaded and verified file: ./data/medmnist/pathmnist.npz\n",
      "Using downloaded and verified file: ./data/medmnist/pathmnist.npz\n"
     ]
    }
   ],
   "source": [
    "# データセットのダウンロード\n",
    "from medmnist import PathMNIST\n",
    "import os\n",
    "data_flag = 'pathmnist'\n",
    "info = INFO[data_flag]\n",
    "# getattrはオブジェクトの属性を取得する組み込み関数（mesmnistがオブジェクト、info['python_class']が取得したい属性の名前）\n",
    "DataClass = getattr(medmnist, info['python_class'])\n",
    "\n",
    "# データ保存先を指定\n",
    "root_dir = './data/medmnist'\n",
    "\n",
    "# ディレクトリが存在しない場合は作成\n",
    "os.makedirs(root_dir, exist_ok=True)\n",
    "\n",
    "# 学習用前処理\n",
    "data_transform = transforms.Compose([ # 複数の前処理を順番に適用\n",
    "    transforms.ToTensor(), # 出力の形状は (チャンネル, 高さ, 幅)\n",
    "    transforms.Normalize(mean=[.5], std=[.5]) # テンソルデータをゼロ平均・単位分散に標準化\n",
    "])\n",
    "# データセットの作成（学習用）\n",
    "train_data = DataClass(split='train', transform=data_transform, download=True, root=root_dir)\n",
    "val_data = DataClass(split='val', transform=data_transform, download=True, root=root_dir)\n",
    "test_data = DataClass(split='test', transform=data_transform, download=True, root=root_dir)\n",
    "\n",
    "# 可視化用前処理\n",
    "visual_transform = transforms.Compose([\n",
    "    transforms.ToTensor()  # Normalizeは不要\n",
    "])\n",
    "# データセットの作成（可視化用）\n",
    "visual_data = medmnist.PathMNIST(split='train', transform=visual_transform, download=True, root=root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c21fd552-3b6e-473b-9c9c-e29fc7de8568",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ac19b6f0-c838-4603-b990-b2363aba8708",
    "outputId": "ff37f3c9-b817-4534-d92a-e0e5ce5023c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset PathMNIST of size 28 (pathmnist)\n",
      "    Number of datapoints: 89996\n",
      "    Root location: ./data/medmnist\n",
      "    Split: train\n",
      "    Task: multi-class\n",
      "    Number of channels: 3\n",
      "    Meaning of labels: {'0': 'adipose', '1': 'background', '2': 'debris', '3': 'lymphocytes', '4': 'mucus', '5': 'smooth muscle', '6': 'normal colon mucosa', '7': 'cancer-associated stroma', '8': 'colorectal adenocarcinoma epithelium'}\n",
      "    Number of samples: {'train': 89996, 'val': 10004, 'test': 7180}\n",
      "    Description: The PathMNIST is based on a prior study for predicting survival from colorectal cancer histology slides, providing a dataset (NCT-CRC-HE-100K) of 100,000 non-overlapping image patches from hematoxylin & eosin stained histological images, and a test dataset (CRC-VAL-HE-7K) of 7,180 image patches from a different clinical center. The dataset is comprised of 9 types of tissues, resulting in a multi-class classification task. We resize the source images of 3×224×224 into 3×28×28, and split NCT-CRC-HE-100K into training and validation set with a ratio of 9:1. The CRC-VAL-HE-7K is treated as the test set.\n",
      "    License: CC BY 4.0\n",
      "===================\n",
      "Dataset PathMNIST of size 28 (pathmnist)\n",
      "    Number of datapoints: 10004\n",
      "    Root location: ./data/medmnist\n",
      "    Split: val\n",
      "    Task: multi-class\n",
      "    Number of channels: 3\n",
      "    Meaning of labels: {'0': 'adipose', '1': 'background', '2': 'debris', '3': 'lymphocytes', '4': 'mucus', '5': 'smooth muscle', '6': 'normal colon mucosa', '7': 'cancer-associated stroma', '8': 'colorectal adenocarcinoma epithelium'}\n",
      "    Number of samples: {'train': 89996, 'val': 10004, 'test': 7180}\n",
      "    Description: The PathMNIST is based on a prior study for predicting survival from colorectal cancer histology slides, providing a dataset (NCT-CRC-HE-100K) of 100,000 non-overlapping image patches from hematoxylin & eosin stained histological images, and a test dataset (CRC-VAL-HE-7K) of 7,180 image patches from a different clinical center. The dataset is comprised of 9 types of tissues, resulting in a multi-class classification task. We resize the source images of 3×224×224 into 3×28×28, and split NCT-CRC-HE-100K into training and validation set with a ratio of 9:1. The CRC-VAL-HE-7K is treated as the test set.\n",
      "    License: CC BY 4.0\n",
      "===================\n",
      "Dataset PathMNIST of size 28 (pathmnist)\n",
      "    Number of datapoints: 7180\n",
      "    Root location: ./data/medmnist\n",
      "    Split: test\n",
      "    Task: multi-class\n",
      "    Number of channels: 3\n",
      "    Meaning of labels: {'0': 'adipose', '1': 'background', '2': 'debris', '3': 'lymphocytes', '4': 'mucus', '5': 'smooth muscle', '6': 'normal colon mucosa', '7': 'cancer-associated stroma', '8': 'colorectal adenocarcinoma epithelium'}\n",
      "    Number of samples: {'train': 89996, 'val': 10004, 'test': 7180}\n",
      "    Description: The PathMNIST is based on a prior study for predicting survival from colorectal cancer histology slides, providing a dataset (NCT-CRC-HE-100K) of 100,000 non-overlapping image patches from hematoxylin & eosin stained histological images, and a test dataset (CRC-VAL-HE-7K) of 7,180 image patches from a different clinical center. The dataset is comprised of 9 types of tissues, resulting in a multi-class classification task. We resize the source images of 3×224×224 into 3×28×28, and split NCT-CRC-HE-100K into training and validation set with a ratio of 9:1. The CRC-VAL-HE-7K is treated as the test set.\n",
      "    License: CC BY 4.0\n"
     ]
    }
   ],
   "source": [
    "print(train_data)\n",
    "print(\"===================\")\n",
    "print(val_data)\n",
    "print(\"===================\")\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7aae5be7-b08c-4e10-aabb-92b11fa859b9",
   "metadata": {
    "id": "b9245f7b-06fc-45cd-bd6a-7a1d8cd3d6f6",
    "outputId": "472ba54b-0289-472d-cf09-37a3dc5a6f1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image:\n",
      " tensor([[[0.8627, 0.8588, 0.8627,  ..., 0.8627, 0.8588, 0.8667],\n",
      "         [0.8549, 0.8627, 0.8588,  ..., 0.7725, 0.7529, 0.7451],\n",
      "         [0.8627, 0.8627, 0.8588,  ..., 0.8157, 0.8118, 0.8196],\n",
      "         ...,\n",
      "         [0.8549, 0.8510, 0.8667,  ..., 0.8667, 0.8627, 0.8667],\n",
      "         [0.8353, 0.8510, 0.8667,  ..., 0.8667, 0.8667, 0.8667],\n",
      "         [0.8431, 0.8627, 0.8667,  ..., 0.8627, 0.8667, 0.8706]],\n",
      "\n",
      "        [[0.8157, 0.8118, 0.8118,  ..., 0.8157, 0.8118, 0.8157],\n",
      "         [0.8078, 0.8118, 0.8078,  ..., 0.6941, 0.6745, 0.6588],\n",
      "         [0.8157, 0.8118, 0.8039,  ..., 0.7490, 0.7529, 0.7608],\n",
      "         ...,\n",
      "         [0.8039, 0.7882, 0.8157,  ..., 0.8157, 0.8157, 0.8196],\n",
      "         [0.7529, 0.7843, 0.8157,  ..., 0.8157, 0.8196, 0.8157],\n",
      "         [0.7647, 0.8118, 0.8157,  ..., 0.8157, 0.8157, 0.8196]],\n",
      "\n",
      "        [[0.8902, 0.8902, 0.8902,  ..., 0.8902, 0.8902, 0.8902],\n",
      "         [0.8863, 0.8863, 0.8863,  ..., 0.7922, 0.7725, 0.7647],\n",
      "         [0.8863, 0.8863, 0.8824,  ..., 0.8353, 0.8353, 0.8471],\n",
      "         ...,\n",
      "         [0.8824, 0.8706, 0.8902,  ..., 0.8902, 0.8902, 0.8902],\n",
      "         [0.8549, 0.8706, 0.8902,  ..., 0.8902, 0.8902, 0.8902],\n",
      "         [0.8627, 0.8863, 0.8902,  ..., 0.8902, 0.8902, 0.8941]]])\n",
      "Label:\n",
      " [0]\n",
      "Image shape: torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# 要素を確認\n",
    "img = visual_data[0][0]\n",
    "label = visual_data[0][1]\n",
    "\n",
    "print(f\"Image:\\n {img}\")\n",
    "print(f\"Label:\\n {label}\")\n",
    "print(f\"Image shape: {img.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "330787e8-cfec-4a14-9c5b-bf69ba24d886",
   "metadata": {
    "id": "eb380058-2d18-443f-a5af-e925d7c11cc8",
    "outputId": "e60ab538-9a62-402d-ccd8-ef13d845a63e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '脂肪組織',\n",
       " 1: '背景',\n",
       " 2: '破片',\n",
       " 3: 'リンパ球',\n",
       " 4: '粘液',\n",
       " 5: '平滑筋',\n",
       " 6: '正常な結腸粘膜',\n",
       " 7: '癌関連間質',\n",
       " 8: '結腸直腸腺癌上皮'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# クラス名の辞書\n",
    "class_labels = {\n",
    "    0: '脂肪組織',\n",
    "    1: '背景',\n",
    "    2: '破片',\n",
    "    3: 'リンパ球',\n",
    "    4: '粘液',\n",
    "    5: '平滑筋',\n",
    "    6: '正常な結腸粘膜',\n",
    "    7: '癌関連間質',\n",
    "    8: '結腸直腸腺癌上皮'\n",
    "}\n",
    "class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43e470b0-ac83-4383-b6e7-c493066e62ec",
   "metadata": {
    "id": "57e8f2bb-bc21-4c6f-a204-d07fe6a84532",
    "outputId": "51976b5a-59ff-465d-8ce1-8b49be5e8512"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset PathMNIST of size 28 (pathmnist)\n",
       "    Number of datapoints: 89996\n",
       "    Root location: ./data/medmnist\n",
       "    Split: train\n",
       "    Task: multi-class\n",
       "    Number of channels: 3\n",
       "    Meaning of labels: {'0': 'adipose', '1': 'background', '2': 'debris', '3': 'lymphocytes', '4': 'mucus', '5': 'smooth muscle', '6': 'normal colon mucosa', '7': 'cancer-associated stroma', '8': 'colorectal adenocarcinoma epithelium'}\n",
       "    Number of samples: {'train': 89996, 'val': 10004, 'test': 7180}\n",
       "    Description: The PathMNIST is based on a prior study for predicting survival from colorectal cancer histology slides, providing a dataset (NCT-CRC-HE-100K) of 100,000 non-overlapping image patches from hematoxylin & eosin stained histological images, and a test dataset (CRC-VAL-HE-7K) of 7,180 image patches from a different clinical center. The dataset is comprised of 9 types of tissues, resulting in a multi-class classification task. We resize the source images of 3×224×224 into 3×28×28, and split NCT-CRC-HE-100K into training and validation set with a ratio of 9:1. The CRC-VAL-HE-7K is treated as the test set.\n",
       "    License: CC BY 4.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# データの中身を確認\n",
    "visual_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b37852-41dd-4ac4-8e21-a76357a3c6f7",
   "metadata": {
    "id": "d2e9040e-f3b2-4cf7-9ec4-955eaae9870a"
   },
   "source": [
    "- 画像テンソルとそのラベルのペアを含むデータセット\n",
    "- （データ, ラベル）の形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15077942-683b-4836-9c6e-96ad2ec5153b",
   "metadata": {
    "id": "e51593a2-afa2-46de-8713-90b9ff2bec73",
    "outputId": "73c9f4a9-4295-4457-cb4c-eceff2cab2c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "リンパ球: 10401 枚\n",
      "平滑筋: 12182 枚\n",
      "正常な結腸粘膜: 7886 枚\n",
      "癌関連間質: 9401 枚\n",
      "破片: 10360 枚\n",
      "粘液: 8006 枚\n",
      "結腸直腸腺癌上皮: 12885 枚\n",
      "背景: 9509 枚\n",
      "脂肪組織: 9366 枚\n"
     ]
    }
   ],
   "source": [
    "# 各クラス（ラベル）に属するデータの枚数をカウントし、クラス名と対応する枚数を出力\n",
    "from collections import Counter\n",
    "\n",
    "# データセット内の全てのラベルをリストに格納（ラベルだけ抽出）\n",
    "all_labels = [int(label.item()) for _, label in visual_data] # テンソルで表されるラベルを整数値に変換\n",
    "\n",
    "# 各ラベルに属するデータの枚数をカウント\n",
    "label_counts = Counter(all_labels) # キーがラベル、値がそのラベルの枚数という構造\n",
    "\n",
    "# ラベルIDに対応するクラス名を取得\n",
    "class_labels_list = [class_labels[class_id] for class_id in label_counts.keys()] # ラベルIDのリストをループし、対応するクラス名を取得\n",
    "# 各ラベルのデータ枚数をリストとして抽出\n",
    "counts = list(label_counts.values())\n",
    "\n",
    "# クラス名をソートして分布を表示\n",
    "sorted_items = sorted(zip(class_labels_list, counts), key=lambda x: x[0]) # クラス名とその枚数をペアにする\n",
    "for class_name, count in sorted_items:\n",
    "    print(f\"{class_name}: {count} 枚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31efc19f-0431-4811-87da-d7e96c06f1e4",
   "metadata": {
    "id": "a4bfa39c-e4e9-4a9c-919e-ce88ded7e292",
    "outputId": "2f49f992-5033-4b55-b506-688e49d00948"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 分布を可視化\n",
    "plt.bar(class_labels_list, counts)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749ffb98-2172-45ba-893b-23489c440df3",
   "metadata": {
    "id": "d4927371-6c83-4e38-af39-1841f2d2a143"
   },
   "source": [
    "- 大きなクラス内不均衡は見られない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26d0b6a9-e878-4e41-919d-162cdc401611",
   "metadata": {
    "id": "4bedc9cf-e86c-4ca2-b6cd-4c63e5f6fd8b"
   },
   "outputs": [],
   "source": [
    "# classIDに対応する画像を収集する\n",
    "class_images = {} # クラスIDをキー、画像データを値とする辞書\n",
    "for image, label in visual_data:\n",
    "    class_id = int(label.item() if hasattr(label, \"item\") else label[0]) # クラスIDを取得\n",
    "    if class_id not in class_images: # クラスごとに1枚の画像を収集\n",
    "        class_images[class_id] = image\n",
    "    if len(class_images) == len(class_labels): # 全クラス分が集まったら終了\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043b5a9f-04f4-46d4-be33-b64baf972c14",
   "metadata": {
    "id": "4f596588-df97-4d0a-9b5c-cc740c32956c"
   },
   "source": [
    "- hasattr関数\n",
    "    - Pythonの組み込み関数で、オブジェクトが特定の属性を持っているかを確認するもの\n",
    "    - hasattr(object, name)で使う（objectに属性を調べたい対象のオブジェクト、nameにチェックしたい属性名を入れる）\n",
    "    - 今回はlabelがitemというメソッドを持っているかどうかを確認する\n",
    "    - Trueの場合はlabel.item()を実行し、スカラー値を取得、Falseの場合はlabel[0]を実行して配列の最初の要素を取得する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fa5f6a1-79c4-42a5-832e-859074099f1f",
   "metadata": {
    "id": "b7e7b506-db24-4087-84f1-c243219da7a7",
    "outputId": "775a9fc5-9b0b-4b84-f527-38cb2dc5246c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuYAAAMUCAYAAADwtOL1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAADakElEQVR4nOzdd3hUdfo+/nv6pFdICKE3KSpSRFQEFRUUOyiusrir61o/ytp7V7CsZXXtCpZVWSyIIjZAsYsgTaS3AElIb9Pn/fuDX+bLmDzPhIB6Fu7XdeVS5plT55z3eXIyc4/NGGNARERERER/KPsfvQJERERERMTGnIiIiIjIEtiYExERERFZABtzIiIiIiILYGNORERERGQBbMyJiIiIiCyAjTkR7fdWrlyJqqqqP3o1aDdt3LgRxx13HNq0aYNx48ahvLz8j16lvWLz5s3Yvn17k8fnz5+PTZs2/QFrRES/FzbmRPuwWbNmoaKi4o9eDUsLh8MYN24chg8fjpKSkoTPX79+PX744YffYc0okT//+c847bTTsHHjRnTv3h1XXXXVH71Ke8W9996Lvn37IhqNxh5bs2YNjj/+eEyePPkPXDMi+q3Z+AVDRPumBQsW4KijjsLxxx+Pjz766I9eHUuaMmUKUlNTcfLJJ2P48OHIzc3FF198gaSkpGaf7/f7kZWVhYKCAqxbt+53XtvdF4lEUFtbC4/H0+w2zZw5E8uWLcNll12GrKysP2AN90x6ejqKi4uRnJyMn3/+Geeccw6WLFnS4ukDgQAAwOPxtOj5W7duxaxZs1q1rs0ZNGgQBg0a1OTxfv36oUePHnjnnXdij40ePRpz5sxp8tysrCx8+umnGDBgwF5bLyL64zj/6BUgot/GQw89BAC46KKLWj2PMWPG4IMPPmjx81966SWcf/756nOWLl2KCRMmIBqNIhqNIhKJxP7/14+lp6dj1qxZ6N69e4vX4ZdffkFKSgoyMjLg8XjgdrsRiUQQCATg9/tRVlaG4uJidOnSBbW1tbjhhhvgcrnw1ltv4cgjj8ScOXNw+umnNztvr9eLYcOG4ZNPPkFRUREKCwtbvF4tsWHDBnzyySdYtGgRduzYAb/fj+zsbBx44IE46aST0Ldv392a3+LFizF48GBcdtlleOKJJ5rU//vf/+K1117D+PHjW9yYf/DBBzj00EPRpk2b3VqX38KYMWNw0UUX4ZxzzsHjjz+OU045RXxucXExjDFo165d7LFevXqhrKwMdXV1LVreypUrcckll+zxeje6/fbbmzTmlZWV+Pnnn+OW89hjj+Gjjz7Cyy+/jMGDBwMAKioqcOKJJ6JPnz448MAD99o6EdEfi4050T7oxx9/xKxZs2Cz2fDpp59i3rx5TZ5z7LHHig1oo5EjRyI3N7fFy21JAx0MBrF06dImj9tsNjgcDjidTjgcDhhjsG3bNlx00UWYO3dui9fh2GOPxbZt2xI+78orr8Sjjz6KiooKXHHFFZg/fz5WrVqFDh06qNOdcMIJ+OSTT/D555/j3HPPbfF6aZYtW4ZbbrkF7733HtxuN3r06IEVK1agc+fOyMjIwPTp03H99dfjuOOOw7/+9S/06tVrryx3d33++ec49dRTkZycjLvuugtXXHEFHA7HHs/3rrvuwocffohIJIJIJIJwONzs/+/6b7vdjn//+9/46aef8NRTT+G4447DpEmTxGUMGDAAxphm37vdUiNHjkSiPzL/9NNPOOSQQ/D6669j/Pjxu72M2bNnAwBOPfVUADvfjnbNNdfg2muvxYQJEwDsPIcuv/xy5Ofn491334XL5drt5RCRRRki2qeEw2EzYMAAAyDhz48//viHrF91dbWpq6szfr/fhEIhE41Gmzzv9ttvNwDMo48+ulvzf/HFF81tt91mrrnmGjNp0iRz0003mXvuuccceeSRBoBJSkoyF198cWzbfT6fGTlypPn6669bNP9ly5YZAGbixIm7tV6SF154wXg8HtOuXTvz5JNPmurqavPll18aAOaOO+4wxhhTUVFhHnvsMZOWlmbS09PNRx991KJ5//DDDwaAueyyy5qtn3vuuQaAWbNmTYvmV1paGpsGgDniiCPMhg0bEk4XCoXU+q7z3PXHZrMZl8tlkpOTTUZGhsnNzTXt2rUz+fn5BoAZOXJki9bbGGPatm1r2rdvH/dYp06dTEpKSovn0RKLFy82AMzrr7/equnPOOMMM3ToUGOMMTNnzjRer7dF53LjzyeffLI3N4eIfmdszIn2MQ899JABYI477rhmG15jjDnggANMWlqaCQQCv/PatYzf7zd5eXkmPT3dVFZW7tG8iouLzamnnmoAmPPPP99s27Ztj9evT58+JiMjw/j9/rjHQ6GQmTlzpnnyySdbNJ/nnnsu1mCWl5fHHn/55ZcNADNt2rS4569bt84UFhYar9drli1blnD+e7sxb/Txxx+bdu3aGQAmOzvbfPPNN+rzH3zwQXPBBReIDXplZaUpLi425eXlpqamxvh8PhMOh8X5zZgxwwAwN9xwQ4vXOScnx3Tu3Dnusb3RmB988MEtbpp37Nihzquurs4kJyebCy64wMyaNcvY7XZjs9lMQUGBWblyZezn7rvvNgDM8uXLY49NmzaNjTnRPoBvZSHah8ydOxc33HAD2rRpg2nTpsFmszV5zpo1a/DLL7/gtNNOg9vtbnY+zz77LL744otWrcPIkSMTvs88kWeeeQYlJSW4+eabkZmZ2er5LFy4ECeddBKi0Sg+/PBDjBo1ao/Wq9HEiRNx/fXXY/bs2Tj99NOxbt06PPfcc5g6dSpKSkrgcrlw8sknq2+LWbhwIS655BKMGDECs2bNgtfrjdV++eUXAECPHj3ipunatSumT5+Oww8/HH//+9/x1VdfxdWvu+66uLcJ1dTUANj5doi1a9c2WYfG5/71r39FcnJy7PHU1FTMmDFDXPfjjjsOP/74I8aOHYutW7eqb2Gqra3F5MmT0dDQgHvuuQf5+flNnrO7r/GCBQsAIPZ+65bw+XzIzs7ereU0p/HtPDNnzsTw4cMBAKeddhruv/9+ADujDk844QQ89dRTGDFiBABgxowZuPXWWwHs/EBuTk4Orr32Wtx8881x837llVfQ0NAAYOdbb4466ijk5+fjm2++wQEHHBB7XuM+7NWrF5zOnZfx4uLiPd42IvrjsTEn2kesWbMGY8eOhc1mw4wZM+I+5Larl156CQDU90d//fXXeO2111q1Hq+99hoGDx682x9UbFRVVYW77roLbdu2xbXXXtuqeTQ65JBDcN1112HcuHHo2LHjHs1rV+eddx5uvPFGPPTQQ3jmmWfw8ccfwxiDbt264S9/+QvGjRuX8L3qV111FbxeL15++eW4phwAvv/+ezidTvTv37/JdEOHDsWwYcOwYMEC/PLLL3EN29atW+Ma8MbUkerq6mYb89raWgA7m8nGBg/YmXaSSLt27TBv3jwUFRWpn0O49dZbUV5ejuuuu67Zprw1PvjgA7jdbowcObLF0/h8vhZtVyKhUAjV1dUIhUKxxzIyMmKvQ+N+LCwsjD2263YbY1BdXQ2fz9dk3k8++WTs/wsKCvDZZ5/hoosuQigUiv2yBvy/JnzVqlWx9/hv3rx5j7eNiP54bMyJ9gHLly/HCSecgKqqKrzwwgs46qijmn2ez+fDs88+i/z8fJx22mni/KZOnYqpU6fu1jqsX78e3bt3R2FhIfr06bNb0+5q0qRJKC8vx4svvoiMjIxWzwcAHA4Hrr766j2aR3PWr1+PrKwsfP3110hOTsbEiRPx17/+FcOGDWvR9CtWrMBXX32FK664okkDH4lE8MMPP6Bfv35ibOPRRx+NBQsW4LvvvotrzH/9y9TChQsxePBgnHfeec2msowbNw4zZszAl19+2aqEGbfbja5du4r1zz//HI8//jjy8/Nxww037Pb8m7NgwQKsXbsWJ510Uosb7aqqKhhjdvuOeWVlJZxOJ9LS0lqzqrvlww8/xPLly+Mes9t3ftXItm3b0Lt37ybT9OvX7zdfLyL6ffELhoj+x3377bcYNmwYtm3bhkceeQR/+ctfxOc+9NBDKC8vx+WXXx53h3RvePrpp2GMwfnnn9/sW2ha4q233sLUqVMxevRodTv+KOXl5TjnnHMwbNgw2Gw23H333diyZQteeumlFjflAGK58s39crRgwQJUV1fjuOOOE6cvKCgAgD1KGAGA+vp6ADvfurK3FRUV4dxzz4UxBs8+++xey0l/5JFHAOx8+01LlZaWAgDy8vJ2a1kXXnghLr300oTPq66uxi+//IJffvkF69evB7Bz+xsfS/Q2k2g0ihtuuAEHHXRQk1/Unn/+eZidnwfDzJkzkZGRgUsuuQQrVqwAsPOXvMb61q1bY38lIaL/TbxjTvQ/rmPHjujUqRMeeughXHDBBeLzNm7ciMmTJ6OgoECNlWuNbdu24cknn0RaWlqrv31x0aJFsfemH3LIIa2ax9KlS/H666/jlltuQUpKSqvmIVm+fDlGjx6Nbdu24R//+Aduu+22Vt/Rb3zbQbdu3ZrU3n77bQBQM7kbc7d3fV94a2zfvh0ej2eP/zLxa9XV1TjppJOwdetWXHHFFTj55JP3yny//vprvPPOO+jTp0/CqM9dNb6NZ3fy8J988km8/fbbOOiggxAKhdRIwnfffRfvvvtu3GO7k3e+YMECLF26FJ999lmzv3BUVlbi+uuvx/PPPw9gZ2Rnnz590K9fP7z55pu48847UVpaipEjR2L9+vVYtmxZk88nENH/Bt4xJ/ofV1BQgEWLFqlNeSgUwjnnnIOGhgY88MADe9zQ/dpll12GhoYGXHvtta36gN2qVaswZswYhEIh9O7dG/fddx/OOOMMVFdXt3gegUAAY8eOxbRp07Bjx47dXgdNWVkZTjjhBFRUVODDDz/Eww8/vEfNrN/vB4Am7y2vr6/HK6+8gk6dOuHwww8Xp2/8dssuXbq0eh2Anb+sderUqdV/4WhOUVERjjzySCxduhRjxoyJ3eHeU7W1tbG/otx33327tc6NHxZt6Vs/3nnnHVx55ZXIyclpUU74xIkTY3et16xZA2DnB24bH3vuuefU6YcNG4Y77rgDxxxzTNzjO3bswD333IOuXbti1qxZmDVrFv7xj3/gnHPOwffff4+zzz4bzz77LJYtW4bDDz8c27Ztw4cffsimnOh/GBtzon1A43tRJZdffjm+/fZb/OlPf9prX4rT6IEHHsC7776LgQMHtup9xD/++COGDRuG0tJSvPnmm1i4cCHGjx+Pd955BwMGDMDixYtbNJ/p06djzZo1ePjhh9G5c+fdXg/Nww8/jG3btuGZZ57B8ccfv8fza3y7QuPbHhq9+OKLqKqqwiWXXCK+plVVVZg1axZcLpf4WYKWWL16Naqqqlr914nmfPbZZzjssMOwfPlynHLKKZg+ffpe+QKiSCSCv/zlL1i9ejXOO++82JfvtIQxBtOnT4fdbo+lpGg++OADnHPOOfB4PHj77bf3+JeflrDb7bj99tvjHotGoxg9ejSmTJmCiy66CCtWrEAwGMTTTz+NzMxMXHjhhbjgggtQV1eHgQMHIicnB4sXL8bRRx/9m68vEf2G/oiMRiL6/Vx77bUGgDn44INNTU3NXp33P//5TwPAtG3bdrezsI0x5tlnnzVJSUnG5XKZV199Na520003xb4QaNasWQnnddlllxkAZsuWLbu9HokMGDDApKWlibnwu+ubb74xAMxVV10Ve6yiosK0bdvWZGVlmYqKCnHa888/3wAwF1544R6tw7333msAmGeffXaP5mPMzqz4iy++2NhsNgPAXH755WoO+e4IBoNm3LhxBoAZMGCAqa6u3q3pX3zxRQPAnHrqqU1qv84xf/zxx43D4TApKSlm/vz5zc7vk08+icsLP/jgg825555ramtrTW1trVmyZIkBYP773//GHnviiSdiOeahUMgAMDfffHOz8+/UqZO54IILjDHGbNmyxZSXl5vKykpz4YUXGpvNZu655x7z/vvvGwBm6tSp5qabbjJOp9OsXr16t/YLEVkTG3OifVRDQ4M555xzDADTq1cvU1JSstfmXVlZac477zwDwOTk5JjFixfv1vQbN26MfelPenq6+KUoTz75pLHZbMbhcJgXXnhBnef1119vAJjXXnttt9alJQ488ECTnJxsamtr99o8hwwZYlwul/niiy9MIBAwY8aMMQDMI4880uzzGxoazIUXXmgAmMLCwj16PUtLS012drZJTU3d7UZ3Vz///LOZNGmSSU5ONgBMXl5ei36JaqnVq1ebQYMGGQCmX79+Cb+g59c2bdpk2rZta+x2u/n++++b1Bsb89LS0ljzn5+fr34LbHONOXbjC4Z2pzHfvHmzufPOO01mZqbJzc01s2bNMsuWLTNt2rQxbrfbjB492lRUVJj8/HwzaNCgJl94RUT/e9iYE+2DvvrqK9O3b18DwBx11FGmrKxsr8y3oqLCPPTQQyYnJyd2F379+vUtnr6kpMTceOONsUZu4MCBZuXKleo0L774orHb7QaAuffee8Xn/fzzz8br9ZqkpCRzyy23mOXLl5tIJNLsc4uLi82nn35qHn30UbNgwYKE6z1p0iQDwBx99NHmp59+Svh8v99vNm/erD5n2bJlJikpyTgcDpOVlWUAmNGjRzdZ561bt5p//vOfpkOHDgaA6dy5s1m+fHnCdZAEAgFz/PHHq78ESKqrq82sWbPMjTfeGNeQpqammltvvXWv/UWmvLzcXH/99bHjZOzYsbv9S1FpaWnsHLj22mubfU6nTp2M0+k0ubm5BoAZOnSo2bp1qzrfXzfmJSUlZsuWLS36iUQiLW7MQ6GQOfjgg43X6zX/93//ZyoqKszs2bNNVlaWOf30083tt99ukpOTjTHGzJkzx9hsNnP++eeLxzwR/W9gY060D1mxYoU577zzjM1mM3a73VxzzTUmEAjs8XynT59uzjjjDJOUlGQAmJSUFHP33XebYDDY4nnU1dWZzp07GwAmOTnZ3HnnneJXtP/a888/b2w2mxk2bJh6V/DTTz817du3jzWMbrfbFBYWmh49epgOHTqYjIyMWJPf+HPnnXcmXH5NTY058cQTY9O0a9fOjBgxwowdO9b86U9/Mqeddpo57rjjzJAhQ0xBQYGx2WwmJSUlYZP0xRdfmD59+pisrCxzySWXmPr6+ljtvvvuM3369Im9PcTlcpnLL7/clJeXt2ifNaeiosKMHj3aADCnnXbabjdx27dvj/0SAcD07dvXPPLII3u0Trv6/vvvzZ///GeTkpISe4vUSy+9tNvzWb58uenYsaMBYE4//XTxbTW9e/eOHSe33357i47nxsZ89uzZxufz7fZPXV3dbr2VZceOHaaurs5cc801xm63mwkTJphAIGCmTp1qAJjKykpjjDHXXXedAWDGjRu3V855IvpjsDEn2kfU1dWZ7t27GwCmf//+5quvvtpr83777bcNAFNQUGBuueWW3X5LQaMff/zRXHrppQnvSjan8T27iQQCAfPaa6+Zc8891/Tt29dkZGQYh8Nh3G63yc7ONh07djQHH3ywOfPMM82NN964W/vpiy++MFdeeaUZNmyYKSwsNMnJycZutxun02nS0tJMfn6+6du3rznuuOPMn//8Z1NVVbXb29nop59+Mjk5Oea4444z//znP/f4rUhvv/12rFk944wzWv22h1mzZpnHH3/crFq1ao/WpzlLly41qamppm3btub2229v9f6rrKw0RxxxhPn73/+uNtubNm0yF1xwgfn5559bPO/GxnxPf1rSmNfX15snn3zSFBYWGo/HYx5++OHY8xrfO19aWmqMMSYajZoLLrjAADDXXHNNi7eHiKzFZowxLfqUKBFZ3tq1a/Htt9/i3HPP3asReMDOnPH+/fsnTIAh63nnnXdwxhlnICUlBXfddRf+8Y9//NGrJFq1ahU6d+4Mj8ezR/MJh8N7/Uu0AODTTz/Fcccdh9tuuw0HHnjgbk8fjUZx9tln4+abb8Y999zTpN65c2eMHDkSzz//PB588EFcd911GDlyJB5//HE8++yzyMzMRGZmJqZOnYqNGzeioqIidq4bY3DPPffgiiuuQGZm5p5uKhH9AfgFQ0T7kO7du+/Wl6jsjgEDBvwm86Xf3umnn453330XgwYNQvv27f/o1VH16tVrr8znt2jKdzVs2DCMHDlyt6cLh8Mtfu61116Lo48+GoMGDQKwM8v95ZdfRm1tLdq0aYNHHnkk7hdwm82GW2+9dbfXiYisg3fMiYiILKK4uBgejwdZWVl/9KoQ0R+AjTkRERERkQXwzaJERERERBbAxpyIiIiIyALYmBMRERERWQAbcyIiIiIiC2BjTkRERERkAWzMiYiIiIgsgI05EREREZEFsDEnIiIiIrIANuZERERERBbAxpyIiIiIyALYmBMRERERWQAbcyIiIiIiC2BjTkRERERkAWzMiYiIiIgsgI05EREREZEFsDEnIiIiIrIANuZERERERBbAxpyIiIiIyALYmBMRERERWQAbcyIiIiIiC2BjTkRERERkAWzMiYiIiIgsgI05EREREZEFsDEnIiIiIrIANuZERERERBbAxpyIiIiIyALYmBMRERERWQAbcyIiIiIiC2BjTkRERERkAWzMiYiIiIgsgI05EREREZEFsDEnIiIiIrIANuZERERERBbAxpyIiIiIyALYmBMRERERWQAbcyIiIiIiC2BjTkRERERkAWzMiYiIiIgsgI05EREREZEFsDEnIiIiIrIANuZERERERBbAxpyIiIiIyALYmBMRERERWQAbcyIiIiIiC2BjTkRERERkAWzMiYiIiIgsgI05EREREZEFsDEnIiIiIrIANuZERERERBbAxpyIiIiIyALYmBMRERERWQAbcyIiIiIiC2BjTkRERLQXLVy4EDfffPMfug6RSARbt279Q9eBdh8bcyIiiykuLsZXX33VbK2srAylpaW7PU9jDGbMmIHq6uo9XT0iSmDt2rV47bXXfvPlLFu2TKxt2bIFnTp1+s3XAQD8fj+2bduGZcuW4a233sITTzwBYwzC4TB27NiBlStX4oMPPsCUKVOwYMGC32Wd/lc5/+gVoD0XiUTgcDj+6NUgor3kyy+/xEMPPYRvv/0WVVVVyMzMjNXuueceVFVVYerUqU2mu//++3Haaaehd+/eTWqRSATjxo3D4sWL0b9//99u5Yn2MyNGjMDnn3/ebM1mszV5bODAgVi4cCEKCwtRVVUFp/P/tWI1NTX4+eefMXXqVDz66KPwer2xWjAYxKGHHor58+cDALZt24ahQ4fi8ssvx/3334/PP/8cxcXFsefv2LEDxhi88cYbccvv1asXDjnkEADADTfcgIcffhgejydWDwQCuP/++zFixAgceuihSE5OjtUikQgyMjLilrNx40Z07doVXbp0QU5ODgoLC9GjRw8UFxdj0qRJ+PDDD1FYWIh27dqhW7duyM7OxrBhw1qya/dLbMxbKRqNIhgMxj3mdruxcOFCVFdX47jjjlOnD4VCiEQisX/bbLa4E2PTpk04++yz8dVXXyVsuo8//nicf/75mDBhAoCdJ47P52vyPI/HA5fLFfeYMQYnnHACHnvssdjFvLS0FAMGDEBRUREAYMGCBZg2bRqef/55dT2IaM+cddZZuPbaa+MeO+6443DxxRfjggsuUKeNRqP45ZdfMHToUHz00UcYMmTIb7mqRPu9aDQKu33nGw9eeeUVnHfeeTDGIBAINPt8t9sde36j6667DgcddFDs33/5y19i/3/SSSfFrusAMHv2bKxevTr274KCAsydOxfHH388fD4f2rVrh+XLl8fq9fX1MMbg/fffj1tmOByONeYAcO6558b9on/++efH/r9jx47YuHFj7N/z58/H+PHj4+ZnjEF6ejrWrVvXZJvD4TD++c9/Jhy/6P9hY95KX3zxBY4++ui4x+bNm4f58+dj7dq1cY15bW0t3n//fZxyyilISUkBAPztb3/DtGnTYs/p1KlT3MHv8/nw3XffwRijrse7776L5cuXY/To0QiHw3A6nfjkk08wevToJs+9//77ccMNN8Q99sknn2DDhg3o0aNH7LFoNIqysrLYv6urq7F27Vo89dRTcLvdPMGIfiNerxfvvPMOBgwYAGDn2LF48eIW3V2y2+2YOnUq/vrXv2LChAkYPHgw/vOf/zR53q4X5Ebz5s3DiBEj9nj9ifYXfr8fvXr1wo8//hj3+KZNm9ClS5cmN9QikQg++eQTjBw5cq+ux6GHHop3330XL7/8Mh555BGUlpbC7/cDAIqKijBr1izcc889seenpaUhJycnbh5vv/02vvzyy9i/S0tL0a9fPwDA1q1b0b1791ituZt+paWlqKurQ25uLkKhEOrr65GZmYkTTjgBK1euxKefforrr78e1dXVSE5OhsvlQu/evfmWFgEb8z3Q+OcoAHF/Go5EIli8eDG+/vprzJ8/Hx988AFycnIQCoXw5z//Ofa8f/3rX7j88svx008/4bTTTsOOHTtiv2mXlJQA2HlS7HqCt2nTBh6PB0uWLMHnn3+OBx98EC+88AK2bt2KwYMHY8mSJQCAvn37xv3mfN5558Wte01NDRYtWoQ77rgD9913Hx599FHY7XZcf/31cDgccDqdcLvdAIBHH30UAHDQQQdh3LhxmDBhQqxGRHvPCSecgH/961+xxnz27Nno0aMHevbsGfe8adOmxf1if/fdd+OWW26BzWbDc889hxUrVqBbt2547LHHsGDBAqxYsQIXX3wxZs+ejY0bN+LSSy+Nm19GRsZvv3FE+xCv14usrCzMnDmzSS0nJyfu5hYAHHbYYc3O59FHH427ntbU1MT+/6OPPor7rElDQ0NsbNjViBEjYr9Yjx8/HmvWrEFaWhoikQii0ShGjRoFAKioqMApp5zS5K/fZ5xxhnjHvH379li7dm3s383dMZ89ezbGjRuH119/HfPnz8dVV12Fn376Cf/5z38we/ZsbN68Genp6RgxYgSuueYajBkzptl9QTuxMf8NvPHGG/j2228xYMAADBw4ENdddx0GDRqEzZs3o7q6WrwIjhs3rsn71Dp37hz378Y7Wz6fDzfddBPOOussjBo1CkOGDMH48eORnp7eonVcvXo1Tj31VBx++OEYNmwY/v73v+OHH36A3+9HKBTC7bffjrFjx2L8+PGx97gdccQRaNOmDRYsWIBjjz1293cMEamGDh2KG264IfY2t7lz58b9Mt/orLPOwmOPPRb7d1paGsrKyvDZZ5/h7LPPxsEHHwwASE1NxcyZM9GuXTvk5uZi3rx5yMzMRG5u7u+zQUT7sOOPPx6ffvppk8fLy8uRmpoa91hzd5oBYPr06bG76LNnz0abNm1wwAEHYPLkyejVqxd69eqFww8/HADwxBNPYMaMGQnX67HHHsPYsWNRXFyM7t2745dffgEATJ48Oa7JbvSf//wH7777btxjgwYNgtPpbPIZl3A4HLdtq1evxhNPPIFHH30Ud9xxBzZu3Iji4mLccccdeOmllzB48GD885//BLDzvejvvvsuG/ME2JjvAWNM7E9Gu77l5Nxzz8Wrr77a5PnPPPMMtmzZgldeeQXAzt88w+FwLM6o8QMd4XAYLpcLa9euRbdu3Zpd9mGHHYbp06fj3HPPhcfjgTEGd955Z4vXfdCgQXjppZdw66234rrrrsOll16Kbt26Yfjw4bjllltwyy234Ntvv8XLL7+Mzz//HMFgEGVlZXj99ddRWFjY4uUQUct17doVGzduxDvvvAMAePrpp5t8lgUAkpKSkJ+fH/fY3Llzcd5552HNmjW45ZZbAABVVVV4++238c033yASiWDOnDkoLi6O/RUMAN58802cddZZv91GEe2j/vSnP2H79u2YMmUKbDYbAoEA8vPzY024w+GA3W5HKBSKTeP3+5t9rzmwM8nl/vvvx4IFC1BVVYVrrrkGc+fOFZf/448/ora2FgCQl5cX+5zYxIkTceGFF8IYE3tbCbDzQ53nnntus9vR3IfJgZ1jyK5+fce8W7du+PDDD1FaWoqnn34ab7zxRuyO+6/fHpeZmYl77rmHn1dLgI35Hli0aBGSkpLiHnM6nfj555+xaNEipKamwm63w+/3Y8uWLfjggw/iflNMSkpCZmZm3J+uAMTulv36g5q/duKJJ2LChAl4/fXXsWDBAowcORLvvfceAGDFihVNPg1+//33x/37jDPOwIIFCzB79mwcddRR6NmzJ9avX49IJAKPx4NIJIL09HQYY+BwOGLvOTvqqKMwffr03dhTRJTI9u3bcfLJJwMAKisrUVJSgsGDBwMAbr31Vpx66qnq9CeffDLeeustnHnmmejduzfOPPNMPPXUU+jXrx/69u2L2bNnAwA2b96Mdu3aAdiZzpCWlvYbbhXRvqt///7IyclBUVERHA4HevXqBWMMXC4XamtrcfTRR2PUqFG44oorkJeXB2Dn21SfeuqpuLeXLlq0CIcffjiMMQgGg/B6vYhGowiHwxgyZAhSU1ObvDUGAG677TYsWbIENTU1OOWUU2I3BKdNmxZ3x7yxuZbumO9q1xSYXd1zzz245pprmjzucDhw6KGH4v3330d5eTkuvPBCcd7SXw0oHnPM98AhhxyC2tpa1NbW4sADDwQAjB49Gn6/H8OHD0fv3r3Rs2dPDBgwABMnTkR2djYmTpwYm37IkCE4//zzccopp8TNt/EOWadOnWCz2WI/v74LX1FRgdmzZ+PJJ58EsDPZ5aGHHgKw8z3moVAo9vOnP/2pyfpHo1F8//33ePDBB/Hqq6/imWeeQTgcjmWPNv73888/x8EHH4zi4mIsXLiQTTnRbyAlJQXnnXcezjvvPDgcDuTm5sb+3bVrV9TV1aGurg6hUAjhcDj27/r6+tg8TjnlFMycOROjR49GSUkJJk+ejLZt2wIApkyZguOPPx5XXXUVJk2ahI0bN6KkpATDhw//ozaZ6H9WIBDAfffdh379+qG4uDiWqvbRRx9h7dq1mDJlSuy5J510EtauXYu1a9fipJNOajKvAQMGwO/3IxAIwBiDI488Es8++yyi0Sj8fn+zTTkAfPDBBygqKsI//vGPuMevvfZa9O/fHyNHjoTP50P//v3Rv39/PP744y3aro0bN8Lv98d+xo8fj3A4nHDaxvejSz+/R677voB3zPeA3W6Pvdeq8c9SAwYMwNKlS5t9vs1ma1HeeHV1Nex2O7Zu3RrLNx05cmTcb7LDhg3DV199BZfLhbPPPhuHH344OnXqhGeeeQaDBg0CgLhs1F/fPX/qqadw+eWXIxqNYty4cWjfvj3eeustTJkyBT/99FPseaeddhouvfRS+P1+LFmyBKeffjrWr1/fgr1DRLsjPT0dV111FTZv3oxrr70WnTt3xkUXXYTk5GQsX768yZ3txovcrz9oduyxx8Lj8eBPf/pT7K9v7777LpYsWYKVK1di8eLFuOiii3D44Yfj1FNPjcsoJqKW+f777/Hcc89h5syZOP/882NvFxkxYgScTicaGhpw/PHHAwDee++92FtAKyoqftP3WL/88stoaGgAsDM84oQTTojLMd/1/eJ7268TXH6Nd8xbho15KzW+v7zxQxWNaSrTpk2LyyHd1fDhw2PvIwd2vpfc7/c3eQ9paWkpCgoK4t5D2vjnrUYzZsyAw+FATk4O0tLSYnfTTzvttBYlpowbNw6DBg1CUVERpkyZgldeeQWvvfYavv32W8yZMwf9+/fHjBkzMGPGDOTm5qK2thYffvhhwj+nE9GeeeSRR9CrVy+sXr0aQ4cOjb3fHECT+NT58+dj7NixcY9df/31KCoqwsKFC3HNNddg8eLF+Mc//oEHH3wQK1euxF133YVXXnkF+fn5uPLKK3+XbSLa1wwbNgyrVq2C3W7Htm3bUFhYGBd5DCCWiNTYKEsuuOCCuJSlSCSCefPmxb0t5MUXX0y4Tk888UTcW1VqamoQjUbx9NNPxz2vd+/e+Pvf/y7Op1evXnE38xoaGnDXXXclXP6vE1x+rbnxipriW1layefzobq6Ota87voBieHDh8MYE/fT+IHPRoFAAJMmTUJSUlKTLwJZt24dunTpEvdYTU1NLAMd2PlBjwEDBmDbtm0AgF9++QWHHXYYzjzzTNjt9th7zBt/fv0npNzcXKxYsSLWxD/wwAO4+eabm93WvLw81NbW4vXXX8df//rX3dtRRNRiP/74I6ZNm4bLL78cAwYMwOjRozF48OAW/5UqGo1i+vTpOPPMMzFv3jzk5ubCZrPhvffew9/+9jfcdtttmDNnDsaNG4fRo0fj0EMP/Y23iGjf5Xa7MXfuXHg8HnTt2hWHHXYYPv7441h9xYoVmDNnTtw03bt3x3fffRf32AsvvIBwOBz7+vrk5GScd955WLlyZezx5tKZfq1NmzYoLCyM/bRr1w42my3uscLCwoSpTKtWrUJVVVXsp7m3wjanqKioybJ2/Rk3blyL5rO/4x3zVqqsrESvXr1i6Qe7Rhh9/vnncW8jAXbe6dr1S0Lq6+vx3HPP4cILL4zlmDf6/vvvcdRRR8VNX1VVFReFWFNTg6KiIrRp0wbAzt+Ak5KS8MUXXwDY+R7zXd+Ssut72xtdffXVsd/Cly9fjm+++QbAzg+Vut1uNDQ04JhjjoHD4UBeXh5ycnJi76Unor2ruroaZ511Fq644orYhXPy5Mno1KlTk9hUyfz581FdXR37MrOPPvoIAGIf3M7MzMTf//533HzzzXFxi0TUOi+99BLGjBkDh8OBcDiMaDSqPr/x81vNKS8vx/jx43HKKadgyJAhOPLIIzFkyBDcdtttsbeo/lpdXR3mzp2LDRs2YMqUKWjfvn2stnHjRkyZMqXZD21qfj3ehEIh3Hvvveo0Y8aMQUlJCR5++GHMnTsXb731Vmxdrr76asydOxdjx45lAlQL8I55K23YsCHWFP/a8OHDY7/lNv7s+mcqYOdvlllZWU2mNcbg7bffjvvmzoaGBtTX18c9/5dffkHbtm3j3rYydOjQuBx0p9MZ+/n1e8z9fj98Pl8sU33gwIGxLzJo/BKSf//73wCAH374AevXr8fJJ5+M2tpaPPjggy3aR0TUcjU1NejRowduvfXWuMcvueSSZqPVmvPAAw/EfcPwr/3nP//B/fffj1tvvRU33XQTbrrpprgoNyJque+++w5vvvlm3FtORo8eHftL9SWXXIKZM2fG/fV606ZNTeZTWVmJiy++GL169ULv3r0xbdo0XHrppVizZg369euHd955BwsXLsTy5cvjxoKVK1fi8MMPR0pKCu644w4MGDAADzzwAL7++uvY22oefvhhlJWVYfv27diwYQOWLl2Kr7/+Ou5D47vKy8vDli1b4j78GYlEcOqpp2Lx4sX46aef4j4r19DQgDfffBPnnHMOOnfujE8//RSTJ0+O+wXh3nvvxY033ogFCxagV69eGDFiRNxfFige75i30tdff42BAwe2atpwOIzVq1eja9euTWqvvfYaUlNTY18oAOyMNwMQ957zhQsXom/fvnHTjho1qkl8o+T777+PZZ4CwJFHHhnLUz/hhBPgcrkQjUZx6KGHYuzYsTj77LPx4osv4thjj8V//vMfXHvttS3fYCJKqEOHDvjwww+b/BK9q+ZqjV+v/f333+Ojjz5q8mdyYOfnVm644Qa8/fbbmDVrFkaMGIGTTjoJp556KmbOnImXXnqJb2sh2k3hcBhjx46N+8K9Dz74IPahz+Y09+HIrKwsDBs2DNddd11cX5Ceno777rsPc+bMwcSJExEIBHD99dfH6tdeey1ycnLw3//+F6mpqRgwYAAeffRRTJs2DUVFRbEo5quuuio2jcPhQEFBAdatW9fs+hUXFzf7+Lx58/Dggw8iFArFvU88Go3i0UcfRdeuXfHuu+/imGOOaTJOeb1enHXWWTjrrLOwaNEiPPHEE+jRo4e4j/Z3bMxboaysDHPnzsVll12GJUuWIBAIoKKiInYwfv75581eQBtjyb777jtEIhH07t0bgUAAgUAANpsNq1evxhVXXIGXX34ZxhjMmTMH+fn5eOGFF1BYWBiXnrB27dpYxnGjiRMnoq6uDp9++imi0Sjq6upitXA4jGAwiLq6OqSkpCA5ORkTJkyI1Ru/MGD79u247LLL0KVLF6xbtw6jR4/G+eefjzvuuAOHHXYYzj33XBx00EF7b2cSUYzUlLtcLuTl5cU+bN7o+++/x6RJkwDsvIj/+c9/brbBvuOOO1BUVIQlS5agU6dOAHbGtS5fvhyTJk0S77ATkeyII47A0KFDY/9euHBhwml2/YBoUVFR7P+b++KfRqNGjcKoUaOaPP7MM88gJycnFgxxyCGHxP11PhqNIhAIIBqNwm63w+12N0mGmzx5csJ1BoBLL70Ul156aZPHU1NTY2+DbYkBAwa06IOs+zVDu62iosIcdNBBpr6+3jz44IPG4/GYnJwcU1RUZF566SVz1FFHmVAoFPczbdo0M3z4cGOMMbfffrsZM2aMMcaY9u3bG5fLZYYOHWqOOeYYc9VVV8WW07VrV+PxeExmZqZ57rnn4tYhGo2akpISY4wxKSkpZsOGDSYUChkACX82bNgQm8+8efPMkCFDmmxjKBQyBx54oHn66adjj61bt8707t3bzJo1a2/tSiL6HQSDwT96FYiIqAVsxgifQiBVbW1ts9+YV1tbi5qamrj3VwE7P6BRXV2N9u3bIxAIYMOGDTjggANQVFSE8vJydO7cGYFAALm5uS1+P+lvLRAIxL40gYiIiIh+W2zMiYiIiIgswBq3ZomIiIiI9nNszImIiIiILICNORERERGRBbQ4LnHdkiKxpr1LXU7kBWx2rar/1hBVFhpV1kebp4E8oVG2xGZr/dv0jfYlYUqesVE2Utut6uKU2h7Nt5Xb4XAo06lrq78e2vGq1bTt73pwobrMfdHtR94g1jLbyV/53LmvnF+b0z5HXaYnyS3WfFV1Ys2eJH+I2eaQRwWbcmD7yqvF2tLvlssTAijo0U6sHXTsIWItGpYP0LAvLNZq15WItVBxhVhLd8v7GwBC/qBYq1ROGE+nPLHmTJFfK6Ocg8nKdLVbm89mbvTxW5+ItZ49uoi1/oPl6NhBl45Ql7mv2vjmj3JRGe9DTvk89Af0L8GKRuQT1WNTrvrKue9SrltaI9EQjIg1m1dvubSzzame+/J5aFP2qyPNK9aCAXk88Yf1bzeN1PnFWpZb3gc+ZTwxytjvCMn7PKr1Ay799QhpvZ1yTLqUY67v348Sa414x5yIiIiIyALYmBMRERERWQAbcyIiIiIiC2BjTkRERERkAWzMiYiIiIgsgI05EREREZEFtDguMdrKeD4tfk7N2AMQaWUinhr79xvMU91G6HGBWl6iNhmUGJ+osrZ2ZaaJ4hJNog2V5qtuh1wK++U4orryGrEWCCeI1lK2IxSSp40qEVH7Y1zikacNF2u15XJ0YeV6OXrVX16lLrPzwJ5iLTk3XazZlOhNX11ArNnt8r0Lb3qKWDvs2MFiDQACDfIyty2V948zRY43KzyovVjLaCfvm9IVW8Va+Up5XQAg1SXvnwwl3s23VY5v9B7YUV6gyyWWnG55XXJ6yPsGAE67eJxYswfl8z7YoI81+6Na5dj2uOXXz6YcLx4lYg8AfPXyMuGWz/1gUI4EDCrXJpdH3g670hBpNQAI+eTjya5EO4aU5sWrjGFaP+BMluMJPQniK4NGji5tiMjRhnUh+fVwpcrzdDvl48OuNBlatDYARMLyunpTk8Ra6wO0d+IdcyIiIiIiC2BjTkRERERkAWzMiYiIiIgsgI05EREREZEFsDEnIiIiIrIANuZERERERBbQ4rhEJXFHz4ZpZcQeADVnT4vgs2kZfErMoE35PWVPNlGNWlTik/Rkx9blPmoJlQ4111Dfr9qUWuxh9fYKsWaPKGvrkQ9dl1eOVQIAp8sh1oyyIXa7PN3+yJ4mvwaFnbqItbYdc8Ra+So9ns+/rVIuKnFanuw0sWZXIv/qiqvEWumm7XJtuxwHCAA9+8mxj6l58v7x++Q4sdIt8rnUfWAnsdbtmAPEmu+QDmINACp/lvdB1Sq55vUFxVpg+RaxFspJFmvOdDm+LLdrgVgDAHu2fG5rcbf+ap863/2ROydVrEVq/WLNrkSahqPycQ8APuXKFvbLx5ozRYngUyIRw1qsoVMeT8IN8roAgEu5khqXEgkYUeKsldhDo1xjo6HW925u5fpss8v7NSNJXlco121bQD4+oso2RhJc0u1eeX0CyljsSpK3sSV4x5yIiIiIyALYmBMRERERWQAbcyIiIiIiC2BjTkRERERkAWzMiYiIiIgsgI05EREREZEFtDguUcsLjEblOBotYi9RGI+SbAhbq3+lkNcnquUe6kmCreZQ4hK1hdqiSuyjGnuoBj8qNSAUkKOeKorLxVpEiZbKaJst1pIyU8SaTdlvCeMrW/tatnrCfdP6b9aItXBYPlbadsmXa330eD7UyvP96eufxFper0Kx1qVfd7GWqayrvVaOyrMrxzwAlG3YJtY8SixnvbLMiqJSsVZdUiPW2nVpK9aSM+UIQgDIOUh+vTy56WKtbPEmebqaBrGWXitfa8o3bxVr20qrxBoA5PbpLNZcKfI+cChRdPsr3w75WIso0aRJGV6xFqjVz6eKojKxlpWhxDeGI2LNnStHrDqU6D4tns+uTAcA3mz5mheoDYi1kBI/GnXI+9ylJUsH5TjARNthlLjAsBKZWbddjsPNaJMh1pxp8rETVl6PYJ0edxp1yy1ysks+96NKfGNL8I45EREREZEFsDEnIiIiIrIANuZERERERBbAxpyIiIiIyALYmBMRERERWQAbcyIiIiIiC2BjTkRERERkAS3PMVdpYZhyza5meOu54lE5fhR25dcNLeJaW56Wx54o/9to2elKHrm2HYn2nSQSljM9q0qq1Gkb6urEWlqunDGa2lHOg7UrG6ntGz1YPsHroZS1/WqUzNv9Ud+j+rdquqJlG8Ta6vnL1WkzM+VM4k55eWKtcoucs/9zWZVY69G1s1hLV/Ktve3lbHAA2Li5RKzVKlm+P/+8Sqy1addGrGUq52fxWjn/PJIoj1fNpVb2T448Jvj9cl6zRzk/O3doJ9bKS+V9CgDbP10i1vL6dRRrGV3lnPv9lcsltxUmVa6VbywWa7k5meoy/dVy9n0wIh8zGUrWvj2iXJu13kTJ+PbXyRneAGCU6zMcSh/hVnLFjTzPzauLxFokIJ+HHToWyMsD4KpTruvKdTRNeT2MV85GN8q44FCmS1YyzgGgqrJWrAVsymupvR4twDvmREREREQWwMaciIiIiMgC2JgTEREREVkAG3MiIiIiIgtgY05EREREZAFszImIiIiILKDFcYk2JfbQ6Wxdf58w8E9ZplGj9Fq3TC2BUFuaXVlPANBWVduvGm0yLQ4w2hAUa0luOVYIALJ6dJDXR4sZbN1Lpb9YSs2h5UwCCAbkfVCjxCO5Q9qGdFKXuS+qrq0Xa7ntc8Va4cAeYi1Yr8eJOUJyvJW/Wl6frLxssZaUJB/3NiW+TDuvHQlGt04d5GhDV5JHrDk98pBdVLRdrG1e+rNYO/iYQ+XltcsSawAQ8MnnUliJRatTIhHt2XJk2tatO8RavjLQZLeTX38ASPPJMZw7ft4m1sqXbhZrPUb1U5e5r0oKyudMaFuNWHNW+8RaOEN+fQAgtbM83oT9cuRnSlaKWLMH5ePXpsThRZV+yJ0mR4gCQNgfEmshZX0cSkSjxyGvT/tOcsRsXZUcj2xLk8coAAjXyOd3arq8zyMeeTsiSrShv0oe+6PKdC67HmuYnZIsz1cZ4qPK69gSvGNORERERGQBbMyJiIiIiCyAjTkRERERkQWwMSciIiIisgA25kREREREFsDGnIiIiIjIAlocl6hH3slFLQ7QqCGEicjTKuk4arRha6MLE9FiGLXMtahSMw55ptpvW/Vl1WLN4dQPh+rN5WItqkSjJSkRUUY7sLQ4oog8XSgoR7gBQI0S8+dJ84o1b5IcnbQ/CjXIMWQl60vEmjvZLdZcXrkGAJ5sOTbNninHcFVskY/diHIspSoRog67PJ3PpgxCACK1ShRbdYNYy2uTKdbad5Sjz8K1cgxlaEuFWDOp8rkCAOnt5TjF5M5yJCScckxZVbkcWRptkybWyn4pEmu+0kp5XQB0UmLj0nLkZZZsLlXnuz8yYXlcyEiRrwUpSkxoQ7Ueo9ouWT73w175XIwoF+f6MjkuULvIunLl48Xt0OP5olpPpF3z6+RrnitTvm45tPFUaaSMcv4CANxyL1FbI48prmT5GLArvYLWYgWVGM7KerkG6PHJ2Xny2JfWRo58bQneMSciIiIisgA25kREREREFsDGnIiIiIjIAtiYExERERFZABtzIiIiIiILYGNORERERGQBLY9LVKMNlUhEJfJPzbgBYFOWaZSIM21djV3+XcSh1LQYo4hfjocCgGhAqYeUmrLv/L6AWKuvlWOekrPkGJ8kJToJAOxK1JO/So54c3nkyDm7Rz4Eg4GQWDNROcopKV2O5AKA9A5yjJvTKR8DEb+8Pvsjb4ocLenzyfFdpVvl6LpkZZ4AEA7Kr7sWd+pUjsGqWjkyqyEi15wBeRurwvL5AADZ7XLFmls+teHzy8WkVDlqLKqMtU4lFs2mRI0BQGVZjVgrX7VdrLmz5Ne5bd8OYi25IFOep0seSypWyFGKAFBUtEOs5baVl5ndQX4d91fa1dDVIJ8zToc89mal6lG1EeUYDgTkmtsuj+kOJZpTS/mNKNGOUa/ectUH5fWxKf2Aduxr0dLOkBzb6vXI0bWJopVNsjzgbNmujP/18viWmZ8p1ux2uTdxK9GFyfkZYg0A0nzZYi0a1nK5E8RJJsA75kREREREFsDGnIiIiIjIAtiYExERERFZABtzIiIiIiILYGNORERERGQBbMyJiIiIiCygxXGJShKZmnqoRRcqiXcAAIdDnrPdmSBrsRXCSsRbXWmVWHO45Cg2AHArUW3RoBwuFW6Qo4McSfI82/YsFGt2tzydFk8J6K+XNztFXqYSg6UdPF4lWspoEZ1allUCESWSyq5GUu1/UjLkCLPkTLnmSZJjuKpKq9VlVhRXiLWMVDkm014jxxfaKuXIP7sSiZWeKceppXjliC4AqN4iR4ZFvHKUYHq2vExXWD4nQhnydBEletWpxLcBQGqyvK52u7w+wSp5rN322c9izeGRY8hcaa0cgwAEjTwu1igRfw5GqDZhU+JqI0o8sCskn2tRJcYWAEyaHBXqrJKncyjXEWdAjhI0SsxgVLlOFG0pl1cGgL9BjlrMUmI7K2rkWNNQWF7XdK88Foe0mGenHgcYUJYZ8Mnb6FSajJBScypjf7hSia5N0ISmaImImfJ4E9Ua5hbgHXMiIiIiIgtgY05EREREZAFszImIiIiILICNORERERGRBbAxJyIiIiKyADbmREREREQW0OL8N7tN7uH1eDol8jDBrwVaOaLEFfmr5HgcX1WdWAtH5XmmtckQa24lqgcAoMRtRSNyHk9SvrxMe4ocD9XaKMFEIYNRZVqnEmdljLw+WkSjdlgpSWwwexBVpG1jVIlS3B/VlNeKtYw8OS4wS6mlKFFrAFC2XY5LLNlRJdbcPjlOLLleiSV1yjF6QSVK0O7W48RS2+WINbcSRWaUaNb6mnqx5lJOGLcSoep265cIuzKIh5TTMKCcZ3aHvP12JRIyXCO/jrYEEaoRpR5W1sehRH/uryrr5OtvSr4SI1qnnE8efT8HQsq0bVPFmq1SHhc8Ss9jS5aPCe0a0rZdllgDgKASv5mkRBLbauXtcCvThevl3kTrFYLaBRhAwK3UXfrYKK6PVlP2m/IywpMg6joclHtCLWrRrkRktwTvmBMRERERWQAbcyIiIiIiC2BjTkRERERkAWzMiYiIiIgsgI05EREREZEFsDEnIiIiIrKAFsclRhqUKCot91CJ1QkrETeAHjsU8inxOEpST0Zhtlize+VIJpsyU5MgRs+WLEcbOrR4OGXfqUtUIxG16EKdGlGorJD2OrZ2nhFlD9gSpBoq6Y26BBFR+xu3EpEZVWKmwoGwWPOmedVlFvYsEGvBgBz9BbsSh1dWI9bqVm4Ta6F6OaJsR1GxvC4A2hbmiTV7VppccynxXco47KuVoxRTlCi6aJI8dgFAOCS/llBiH0PK2L+jVI7EzEiWx8usXDle1qFMBwDBiLwdQb987UscMrv/yVDu95mwHDEXVmL03EqsMADYlUHd1PrFmk2Zry1Jbo+MQ97GSEDpTRL0Ci6vHLNnwvK5n50mH9/hkBLrl6KMt3Z5PK1Rxj4ASG0rj2E52XJkphYVq103fPXya+xRxrCQEnkI6K+lU+kHXAm7KR3vmBMRERERWQAbcyIiIiIiC2BjTkRERERkAWzMiYiIiIgsgI05EREREZEFsDEnIiIiIrIANuZERERERBbQ4hxzE1HyN/1y3mV9lZyf60jVM3LdqXI2pztJzt61K/nKenC2XLK1NlMcANxyNqcWuq7lo2spmVrmuhqjmiB6U89yb91stfVxKPtcy0ZvdU45AKPMV9v+/ZFNyfLV9qN2zGvfTwAAUeWAiSoHoZYPnFLYVqw5PfIYVL+pRK6VyDUAqCopF2uukJJV7pG3w5aWLNZ+KZLXp6BdG7GWlyDHPBqUXy+jZAunKeN3Vtf2Ys2pZeeLFcA49HPX5ZDHaOOQ11U9zvdTdmVcCFUrmeLKsa1l9AOAI1nJ/26Q+xO7kputXZtsSg6/QzlGHcnysQQAdmW+IeU7A6LK9juU4cSmZIND2Y4kpz4uBErr5PVJl7PTnS65JQ1Xy9npKco4FVXGU5eSnQ8AEbe8Ph7l+2lCezgu8I45EREREZEFsDEnIiIiIrIANuZERERERBbAxpyIiIiIyALYmBMRERERWQAbcyIiIiIiC2hxXKI9RYmMCstxNKlpcjSOTYs1TERLPVRys+xKjE04Kk/osMmxOolS9GxKck5EWaYa36guUysqMYMJEn5sNu310ubbughCbSu01MtEsYZa5B7Dz1ouOV2OErQrr15DrRx7FU4Qi5aWmyrWXEoUWUCJTIuG5HPQ2zZdrGV1yhVrmT0KxBoA1BVXi7X64gqxlpadItb8lXI0bY8O+WLN7paj1uqVyEMAqCmrEWsOZWxzK69zkhJt6VEi9VxK9J1RagBgU/ZBNKyM0W49/m5/pEUQejxyyxHVogS11wCAU4n9CyuReJGwEnuo9RhalK88GZCmxwxGnMpClTEsXC/XHErkn0OJtgwrjYtJ8HoYJfbWprweUeUQsCvxhC6lB7Up1yKbvhmIKrGuvvqAWKuvadBnnADvmBMRERERWQAbcyIiIiIiC2BjTkRERERkAWzMiYiIiIgsgI05EREREZEFsDEnIiIiIrKAFsclqhF0WhSVFk0X0bNq7EqcorY+ESWwSFkdNVbHaBOqwX6A0UL41IzC1kUy2ZV9oy1OiyDU1waJMyPFZSrrquw3LfIwwcuRqEwt5K+WI6G8XjlmKhqQo7T8fjn2DAAiQbmelJUsT6gcZy4lgi+iRCnW++vEWvHm7fK6AMjKzxFrHqccw1hTWinWvC55OK+rkNfVpkS0pWRniDUA8GTI8Y1OZXxPcsjXDG0c8isxbYGwfGykpumXOpsS7Rj2y1F0/jo5+nN/5VT6gWhQjrUL+uW4T1uqHLsMADYl9k8bb7TxRLsyu5XeJKJdt1L1eM20TllirWpliVhzKvvVprweIeVccyQr+7xcHk8AwK3EukZ88j53KldnLc46pAwaarRjjX7+hpVb1/YMOS7YEZSPuZbgHXMiIiIiIgtgY05EREREZAFszImIiIiILICNORERERGRBbAxJyIiIiKyADbmREREREQW0OK4RC1uzK7F+tnlqBo1ghFANCLHDtmUSCJtvloEkl2L3FHWJZro15vWRjQqs9T2uVEyEbV4wlYmHiakbYdOWSFtXdUISv34cGg74bfaQf+jPMo5UV8lR585leiz9KxUdZmBhoBYiyrRZ+GQXPPX+sWadqz4y6rF2rIFi8UaALTr0kGs5XftKNa8OZliLaJEfzmUODmj7JuQsr8BwCinRFCLNqyXozYDAXk7UlPl48OTLMdlVjbIkYcAoKTfweNSoh2DeuTv/sgoO9PhlVsOt1+JmDP6fg4q8bmhiBIl6JHXZ/XSdWItv0ueWGuTkS7W7AniYOsb5LEoqVO2WAsVVYm1iHJ+B5Ux0+6TXw+7Mp4Aei9lS5bjaR2tPtfkmkmSl+f0yjUAsNnl7Qwo45urIFOdbyK8Y05EREREZAFszImIiIiILICNORERERGRBbAxJyIiIiKyADbmREREREQWwMaciIiIiMgCWhyXqAbFKUW7rfW9fySsRPkkiOuRaPFe2obYHFpcZCJaJKIWCanMUVuomhYoFxPFGqrRjkpEoVZLcGSJtNhH/TUGjBKtZVfikVq5qvusqHIOOkLyzgorMYsu5TwDgPS2chSZwy0PZ5GAHP1VU1Yr1qJBOWrNmybH8w0aebhYA4Csjjliraq0RqxVF5WLtWCdHLXmUJLGUjxyfKVdyxEE4FTi7xzKIFVZIccXVpfKUZullXJEZVYbOcIuo60cNQcARjmWo1F559lt8vGxvzJK5F1IuRbYlJorweAbqJbjNxv88nnhdbvFWpJScytXfZsSiWjfUSfWAKCiXB6L1DFDGd88yq5zKpdmLX7WmSTvGwBwheTzIqIdA+nyWBRWIk+NXz5HPVrMsXLNAIBoRJ6vQ4la3FpUKtZ6qUvciXfMiYiIiIgsgI05EREREZEFsDEnIiIiIrIANuZERERERBbAxpyIiIiIyALYmBMRERERWUCL4xJ1rYzDSxRrp8w2pMSYOZSIL5u9lZGISuROVI0D1OO2oMxXS/kxSoyPSlteghckqr3OWgRSWIkUa23spbbPExxXrdxzifMk9zPacR9VXnPjC4i1ugTHdap6UijHoHJuZ+RnijW/EkHoVOKyklI8Yg0AXErMoFuJYbQrEW6OsLzvAmVyzGDlxmKxllqnn5/e3EyxFk2V90G3YQeJtZoKOTLOX1Ip1iqLq+RalTxPAEhNktc1I1ne585kOd5tfxUNyTF7VbU+sWYPycdvhnJsA0CK8vqtXrVBrAUCcgTfwUMOFGtuJfLVptTsCdI1MwLKObypQqxVbJLj+aqViNEefTqJNY9NHqMCEX1D/AG5rsUXRkuVOEktttQjR3Rqkd1adDIA2JV6WGkItm8qUeebCO+YExERERFZABtzIiIiIiILYGNORERERGQBbMyJiIiIiCyAjTkRERERkQWwMSciIiIisoC9EpdoU+JvtFg7kyC3zumRVy8alCOZ7E45OscokTtaAp+xKduRIEbPpsT1qBl8WiKgMk8tUU7b5XuSBmiUqY2acKfETinLUyMqtQUiweusxCNp+3V/FFHOwYASiehQdqQ7V48Z1A7SuqoGeb5KtKE2XoQD8jaGQgmyzxQOZZl2JUI0p32WWDNKhFm9Eie2Y4cStbatSqwBQNQhj9H+Sjn6LFAnv1a5PQvEWkqP9mItvWOeWKupU2LYAJSu3irW7D552sIO7dT57peU8yIrI0WsBZSYRdMmVV1kXbn8GqW3kc+ZgE+OQ3UoMZmmUo59VC+kbvk8BACXSxkXlBmnK3GRnnCaPM+wEu2YLI9DLo88ngJAeakc0ejNzRBranqhFvWs9UpKVC6UWMed6yPP2K0ss3OHfHW+ifCOORERERGRBbAxJyIiIiKyADbmREREREQWwMaciIiIiMgC2JgTEREREVkAG3MiIiIiIgtgY05EREREZAF7Jcdcy4XWcim1DGtAzzu2KTnAWpCoza5krqtro6xLgim1jG8tGtto269sh0bL+4wmCpZX1tZoW+LQXqvWvR57kimuTqoUW7vP91WlpVVizetRMoCVvO1oRY26TJtyKHnTvMp08oQhJT/Z7pTvXQQagmKtJkE+biQiH91ZbeXc4eQMeRsD9fL6rFy4Sqz98P1PYs0ekucJAJ2ra8VaZlq6WAtXyznI/h3yPO1KHrsjNUmsRZWcZwBIz84Ua24l67msTM5r3l+Fg/Kx720jH9uO7VViLVpery7TpZzf7fNyxZqWnR5WvqfB5ZLHhWidfM5EElxDouny+a3lw3tS5enSM5LFmls5L4J18ndRIKT3Cplp8jK1vk87duqVcy1FGfvtGfIxJ7/C/z/lOyUQlveBZw/vefOOORERERGRBbAxJyIiIiKyADbmREREREQWwMaciIiIiMgC2JgTEREREVkAG3MiIiIiIgvYO3GJrY0D1FMGEdUC85S8PG191IhGdWVbVWpcqLxIJY7H1spMwITrI02XeEPEirauWkKUeuxo89RiFhNsh039dVSZr5b9uR/Kbt9GrEWVSESnEnVqgiF1mYFan1hzK5Fh2knhcMnDoHYGutzydMF6vzIl4IActVVb2SBP1yDvu+RUOfpswMj+Yq1j7/ZiLRLQ4xI3L98g1rYsWSPWuh3QXazldGgn1hqq5Nc/WifXGkorxRoA2JRBIZQsxzB6vHIs6H5LiRH1F8txqDanfD6FapXoPgDeNPk10mKHfUrkXahGPofdOaliTd0On74d9lp5mVG3fO5HkpX4USWa1a1cnJOU8S3RRTasjfHKdF5lwHW3keNXXXYlvlKZZ0SJwwWAqNKfaVGbdQH9OpYI75gTEREREVkAG3MiIiIiIgtgY05EREREZAFszImIiIiILICNORERERGRBbAxJyIiIiKygL0Sl2hrbc5gAnoijxazJ0+lrau2vHBEjlWya3mASBCJqEyrrY8W+6itj7wVrY9n/P9XSJuzWEmw6+TFqauiH3OtrSaa7/7Go0V0NchHmitdjjazR+XIPwDwFcuxd7U75Cg2uxKJmJwtR59FlOPTqcSJpSbL0V4A4E6S951DiRqLROX96q+XY+qiSiycN1nefiTLJQAYcnpHsdb/2MFizSjxhNq6hjaUibXakmqx1jY/W6wBQM3WUrEWqJSPK0+6/jrvj1we+dgO+uUYOa9Hjp40fjmaDgCiSXI8q3HIJ3FqQaZYi+yoE2tuZRvDYSWeUNlGAAgrEXzbN2yX55skjxm5HeX4UY8S7WhzyLVogljbqBYR3VYeb8KlyhheLsfIRrTIW2U8jQb14yoQlo8rnxIJnJG7Z+MC75gTEREREVkAG3MiIiIiIgtgY05EREREZAFszImIiIiILICNORERERGRBbAxJyIiIiKygL0Sl6jZk4A5NUnvN0hoVNMCtQjGPUgZ1GjzNWrsY+t2gB57CRijhS3KtIg3NdpSKarRjgm2P6qUtajJPYqT3AdpkViejBSxFlQiwcJKnBoAONPk/L6QT57WROSardYn1jLyMsSaQ4lLDPnl6EIAqC6To9g8SUpkpHIIBuoD8jyVODG7U74/E1IiwQBgy0o5ws2rRELalZg2X4O8HVC2w9tGjiirC+ivR3KX9mIt6pKjFGtKKtT57o9syvHk0I7toHysJbryBEPy+a2dw2m924q10qVbxdr29cViLSc/S6zBrd8LrayoF2slW0rEWscuBWLNoVzwjDKeOLR1jcoRhABgS5LP08yuuWKtfL28jWarfK5FOuaINUe6V6w1lMjxjADgq5OvDcF6v1hLzVOOgRbgHXMiIiIiIgtgY05EREREZAFszImIiIiILICNORERERGRBbAxJyIiIiKyADbmREREREQW0OK4xEhYjjiLKHE80bAcgeRyy3FaABJl6YklLdZPiwTUUvbsDvl3mEQhelp4n7ZMo0ypTafF2Nlt8nbYE/yaFo0q+07dSi2CUJuqldMlekGUJ9i14yrBbPc3Sz76VqwV9O0i1vK7FYo1W0Tfyz4l2tDm0F5X+eB2e+VxqKFOicTKkqMbs9tnijVAH098tfIyG6rkmrYdNuXkDijRjibB6+F0yLFpdeUNYs2T4hZrbo+8HQ5ledrrYVNiUAHAq6xPekGmWCtT4iL3V9plxKO8tloar9ulx/O57XI9UifHbwYb5GM/s1sbsdawXY7ZsyljTSSox8FmZKaKtT4De4s1b448nT0g925Giac1ypgRdenNgvHJ+7VsmRxDuSMsr48rO0msRaqqxVq6Ft/p1I+rrdvk87uwgxxR6UwwbibCO+ZERERERBbAxpyIiIiIyALYmBMRERERWQAbcyIiIiIiC2BjTkRERERkAWzMiYiIiIgsoMVxicXri8VaOCLH8SSlyBE3OQW56jLtSvyZFsEXVaMUlTkqRZsWa5ggn8+WML9PmK+yPlElWkpbXOtCDXeyK5FjanyjFqepLM+pvP5aPGOiaDSbslB1vq18HfdVg0cOEGu/fPeLWNNCUjv26qAuM9ogxwVGlFgwKMdSUIkMS0qXx6+6snqxFg3JMbEA0LarPPZlF2Yq6yNHv9XsqBVrWuyh2y5HBTbUyPsb0M81V5I8XzUmVhkvMnLlWDi3Enm4fW2pvEAAGW3l+dZVyK9zMKDvn/2RXYug084L5aBwuvVWxaHUg355XKhauEVenVT5eApH5e0wEbnmSHANcWgRwco2RpT9alciIbXXKprqlWspetS1rVyJPayWx7Acu0esfbXiZ7HWJjVFrKVnZ4i1OiUOFwDWb9gg1nr26yZPGNmze968Y05EREREZAFszImIiIiILICNORERERGRBbAxJyIiIiKyADbmREREREQWwMaciIiIiMgCWhyXmN+tQKy1OkRuD+LntChBLdYuGpKjk4Lbq8RaOChPl9xJj310eORoIXU7lD1rV36l0uapBSaqkyFRXKBSU7Im7do8tcVF5WLEaCGMehyblifpdCkxYPshW7UcNdX/oJ5iLazs5EBljbrM/O7yueavlyO6qjbtEGu+7eVirXidT6wVdJbHxB0V1WINAPw18nzb9coXa2o8oTLOhJRISJsymHhT5PgyAIiE5XMtFGoQa06PfOkxYTn6LarUIj45Fi4lWY6+A4DqYvm4S0mTY+PadstT57s/2lJZIdZSPfK+zExJFmv2JD2eLxqRj0Oncs44nEokYlAepzIz08Sado46vQm2QzmGQwH52A82yOdaXZUc9+lxydufrGyHTQ1eBrRLrPZ6pGXIx8ChIwaKNYcSF5mcLEfeukL6+Hbc0cPEmterTJsgLjcR3jEnIiIiIrIANuZERERERBbAxpyIiIiIyALYmBMRERERWQAbcyIiIiIiC2BjTkRERERkAWzMiYiIiIgsoMU55lo2tpYt61SydSNa2CUALY5ay/jWMjbD5XVizaktzyvnfdoc+u836r5Td4FSbG2m+J5oZXZ8a7PK1S1UdrnNJHg9lFx1PQE+QdD7fiZUJufjhp1yxrknST6XnAkO3cryTWLNkZ0i1nK65Ii18qIysbbk+yVirVrJaz5gcD+xBgANdbVi7ZPXl4q1Pv37ijW3ktfrcClDvbLPI8rYDgCBBjl3OeiXv/uhoVY+PlxeeV2r1pXKK6NkWaeky/sGAJzK+B5VdlC7/h3V+e6PVq/bINZyIeeYJ/fqJtYcWXK+NaB/N0JUOUZdynlhc8vfW6F9U4ZTuUxEEuRb293y+njS5NzsmiL5vKjyyRnnndpli7WKLfL4lpUt57gDQFKGfL5p35ei9XxJSfL2V1TJ2x+OKN9Bk5FgO4JK/xqQ5+t07tl3nvCOORERERGRBbAxJyIiIiKyADbmREREREQWwMaciIiIiMgC2JgTEREREVkAG3MiIiIiIgtocVxiqDYgz0SJt9KoMXqAnpen5Az6y+UoMqc/JNYiSlSRO1eJ1UmQomeUdbXbtbxAuaZFDmnz1KIbtcjDndMqWUZaRJQyTy1USI1V2oPt0KiTMi0xTlB58YKV8jkY8snRdN4UOU4NAFKUus0vv0DBzdVizWOX708MO/4wsRauk8fEFCUqEACc+RlibVORvGM3L1sj1jr37SXWoknyvnEosXAmrAXDAQ6XPK3LI9eCPnnfZWTL+8aTKr/+ZVsrxVqxUgMAo8RCJinLDAfl60mPoV3VZe6rBnTvKReV46l0h/waeUNy5CEABJVIvEwlEq+hQT4OXVE5Ks945F5BO5Zsdj1Gz+ZSYodD8r5r271ArHU4qLNYq99cJdYqlDHcq8WvAvCmyNGGIWX/aPsuqlyb1/wkj4t25f7zwYcdKM8UQFKSEvsYkNfVvoe9Au+YExERERFZABtzIiIiIiILYGNORERERGQBbMyJiIiIiCyAjTkRERERkQWwMSciIiIisoAW5xzaHXIP71CigzQ2LSoQgFaOKhGEASUu0Z0qx9/Y28qxSg6PHJ0UUSJ+AD3aT8uEtJlWZvcpy1MjCPcgD1B9JbX1USdTqmqU5B5sh7rI1scw7ovKtpSJtezMdLHmTZbPwVBQjxn0N1SJNZdXjmFMykwVa3aXHIfn0yLDlAGqessOsQYAprhCrA08sJ9YC9nkyLRtG8rFWocDCuV5BuV5hgJyHCCgxyVmZsv7PDVN3uepbeRxOLtAjlLMypena9shW6wBwI7N8utRWVwl1mpKatT57o/cbvk8rPPVibW1G7aKtYNH9FeXWbp0nVjbvmG7WGvXMV+sZbjknsfulccwf70cweiBHj9qN3Iv5Yoq56lSc2YmizUtvNHlkKvRiL4dYZ8cb1lT0yDWPG65z3JkyPu864E9xFrDDjkqN1AtrwugXzecSk+MBD1hIrxjTkRERERkAWzMiYiIiIgsgI05EREREZEFsDEnIiIiIrIANuZERERERBbAxpyIiIiIyAJsZk+y5YiIiIiIaK/gHXMiIiIiIgtgY05EREREZAFszImIiIiILICNORERERGRBbAxJyIiIiKyADbmREREREQWwMaciIiIiMgC2JgTEREREVkAG3MiIiIiIgtgY05EREREZAFszImIiIiILICNORERERGRBbAxJyIiIiKyADbmRLRPiUajzT4eDodRW1v7O68NUF5ejlAo9Lsvd2/bsGEDLr744j96NYjIYvx+P7788ss/ejX2GWzM9wNPPPEEZsyY0eTxL7/8EnffffcfsEZEv40vv/wSQ4cObbY5nz9/PgYPHtzsdA0NDairq4v9NKqqqmr2Z9fnJHL88cfjkUceSfi8QCAQtw6RSAQAUFdXJ67H7poxY0arz/mSkhI888wzrZrWGIPi4mKEw+FWTd9oyZIlOO200/ZoHkTUvPfffx+DBg3CSSedhLKyshZPV1RUhGHDhgEAbrrpJnz99de/1SruF9iY/4bef/992Gy23f5pvHgtW7YMRxxxBFauXBmb53nnnQe73Q6n0xn3Y7PZMHXq1CbrUFtbi5tvvhk1NTUIh8Oxn2g0in//+99YunRp3OONP0T/i/7973/j0EMPhd2+e0Nbnz59kJaWFvvx+/0Ih8PIyspC37590a9fv9hPt27dMGbMmCbzaO48CofD2LBhA7p3795sbddfIP7+97/HrcMnn3wCABg1ahS6d+8etw59+vRBVlZWk3UYP348jjzyyNjPhx9+GFd//fXXW7Rvzj///Cbj0tChQwGg2TErNzdXnV91dTXatWuHn376KeGyNZWVlZg/f/4ezYPoj/L444/j1ltvbfL4888/H3ft/6NMnjwZCxYswBVXXIF//vOfAIAdO3Zg/PjxTX4kAwcOxOmnn47169f/Xqu9z2Fj/hs65phjsGbNmiY/jz76KLKyspqtrVmzBk6nEwDQpUsXtG/fHkOHDsUXX3wRm+9tt93W5AJ/xBFHNLsOTz/9NGpqanDBBRfA5XLFfi699FLMnDkTM2bMiHu88cfv9/8u+4hobyguLsaqVaswZ84c3HLLLXjmmWewdetWBAIB+P1++P1+hEIhGGNi//b7/QgEArF5rFy5EsaYJvNesWIFioqKYj/N3TUuLi5u9jxyuVyorKzEmWee2WztrrvuipvPU089BWMMhg8fHvf4888/H7cOS5cubXY/LFy4EBMmTMAtt9wCp9OJkpISvPrqq3A4HEhNTcVHH32E+++/H6mpqUhNTYXNZkNRUVFs+l23/+6774YxBsYY+Hy+Zn/C4TCMMbt1d62lGhoaUFZWFvdTXV0dW96uP+Xl5Xt9+UR72/fff4958+bt9nTLli3DkCFDsGDBgt9grf4fYwwCgQDq6+vhdrsBAElJSRg1ahQOOOAAfPjhhxg1ahRGjRolzuPMM8/E1Vdf/Yf/kvG/jI35byg5ORmFhYV45JFH0NDQgO7du6N79+7Iy8uD3W6P/bt79+6477778MILL6B79+6x6VNTU/HGG2/gzDPPhM/nazL/wsJCZGdnIzc3F999912T+uLFi3HbbbfhjTfegDEGBx10EF588UUYY+D1etGzZ0+EQiHcf//9OP7442MX4cY60f+KsWPH4oQTTsDdd9+NoqIiXHnlldi2bRvy8vKQlJSEpKQknHjiiVi9enXs30lJSejVq1fCeXfq1AmZmZmxn4kTJzZ5Tn5+ftz50/jzySefIDc3F9FotNn6HXfc0aLtO/fcc+PWoVu3buJzjzjiCIwaNQr5+fmxx4YNGxb3Nplfv2Wn0aBBg7B48eImjyclJcWa+caf5ORkvPTSSy1a/9Z44IEH0KZNm7if0047DTU1NU0ez83Nxb/+9a/fbF2I/kgNDQ2orKzEUUcdhUmTJqlNbzAYVOe1YcOGJo99+umnKCsrw3333YeTTz4Zb775JiZNmgRgZx9y/vnnY8yYMUhKSor9/65/MevRoweA//fXtOuvvx69evXC5MmT92Cr919szH9jXq8XP/74I6ZMmSI+JxAIYMaMGXEXUmMMPvjgA9hsNrzwwgs44YQTmp32+++/R1lZGYYMGdKkduedd+L+++/H2WefDQD4xz/+gcWLF8Pv9+Obb77B66+/DmMMzj33XHi9Xv7pif5nvfHGG3A4HCgsLMTFF1+MO++8E4MHD0ZVVVVck9yrV6+4xnjjxo2xecyfPx/vv/9+k3lv2rQp7n3d06ZNS7g+F1xwAZxOJ0444QSUlZXB5XLFvfXsyCOPbHa6ZcuW4f33329yB/i1116LW4d169aJy45EIrE72Y3Wrl2Lq666qsnPr3Xo0AFvv/12s/OtqqqK+yvdWWedlXA/7KmTTjop7vWaN28eMjIy4h5btWoVPB4PBg0a9JuvD9EfYciQIViyZAkuvvhiPProo/jTn/7U7POWLVuGww47DDU1Nc3Ww+EwBg0ahJtuuin2WHV1Nc466yxcdtllGD58OBYsWIDp06cjIyNDXJ+cnBxs37499vPNN98AAGbNmhX3+BVXXLEHW73/YmP+O7jmmmswffp0lJSUNFufNWsW/H5/3MlWXFyMK6+8EieddBJqa2sxbdo0pKam4s0338R9992H1NRUbNu2DQcffDBSU1PxzTff4OKLL0Zqaio2b94MAHj33Xcxb968WDNwwQUX4N///jfee+89fPfddzjggAPw448/YujQoZg5cya6du36u+wPor2tsLAQH374If7yl7+gffv26NmzJ1atWoVoNNrk/dy7Npe7Nq/vvPNO3Oc0bDYbPB4P8vPz4fV6Yz/nnXde7M+8u3r11Vdjb02JRCK44YYbEIlEYIyJW+bUqVPFO17ffPMNpk6diuLi4thjHo8H48ePj1uHwsJCeDyeZufRv39/uFwuTJ8+PfZYeno6DjvssCY/v3b88cfj008/bXa++fn5cXfM33rrrWafpxk8eHCT96enpqbu9nx2dckll+CEE06IvQeeaF+UlJSEp556Ck888USzv1QDwC233AKfzyeeU/Pnz0dFRQVGjhwZeywjIwNTpkzB9OnTxV/Kf63x3P3222+Rn58f+4zJmjVrkJ+fj61bt+Kjjz5CSkrK7m0kAQCcf/QK7A9OOeUUpKam4uWXX8a1117bpP74449j/PjxaNOmTeyxdu3a4dtvv8WwYcPwz3/+E+3bt8egQYMSfvDJZrPFfaAsEong+eefx/nnnw8AGDFihBgnR/S/bPXq1ejUqRNeeOEFzJgxA//973+RnZ2NJ598Mu55Lpcr9v+vv/567INMjz32GA444ADYbDYAgMPh2K3PWqxdu3aPP9x40UUX4eKLL8aIESNij3322We7NY9ly5ahX79+cR/Qatu2bbMf2DrnnHPi/j1mzBhkZmbi448/hs1mQzAYRDQajb2VrvGD57v+udzv98PlcsHhcCRctxkzZqBv375xj2kfRo1EInFvuWlcj10fu/XWW3lTgfYJjWOP5rLLLmv28bfffhvvvfcepk+fLp5T06ZNQ15eXpPPsFx44YWYOnUqrrzyShx//PFNGvvMzExUV1fH1vHss8/G+PHjcdZZZ2HGjBno06cPAOC9997D//3f/+GSSy5BXl5es2/7o8TYmP8O3G43LrzwwtiHOndVVVWFoqKiJs0DAOTm5uKzzz5DTk4O5s6di4EDB+Lyyy9vNvoQ2NlYHHvssUhKSlLXp6GhAYWFhQB2vh+trKws7kR87LHHcMEFF+zOJhL9YdavX49BgwahpqYGkUgEubm5sNvtcLlc2LJlC5544ondnufUqVNxySWXJHzeSy+9FPul99ceeOABPProo00eD4fD6N+/f8J5b9y4sUUX6okTJzabyNTI4XDg22+/hdfrhTEGwWAwdrfd4/HELaNjx44YNWoUnnjiCRxwwAE4/vjjsXr1aiQnJ8Pn86FDhw6YPHkyRo0aFRtDysvLcfXVV+OWW25JuK6dOnXCAQcckPB5jebMmYO0tLQmj+/62Kmnnop33323xfMkshqfzwe3292iX26bs379evz973/HySefjHHjxjX7nO3bt+PNN9/EDTfc0GQ5NpsNTz/9NAYMGIC77roLDzzwQFx969atWLRoEcaOHYt169bB5XLB4/Hgvvvuw4QJE/DGG28A2Dlm/e1vf0N9fT1eeeWVVm0L8a0sv5lwOIy1a9fGfhpPmrVr16KkpATRaBRr165FWVkZ5syZg6SkpNhzG9+KAgAFBQWw2WwYPXo0Hn74YVRVVeHyyy/Hxo0b43769OkDn8+HTz/9FHl5eXHr0vgWl9TUVCxYsAButxv33HMP7rnnHpx55pno0aMHXn31Vbz66qvo2LHjbkfNEf2RunbtiksuuQT33Xcfzj77bJx55pmor6/HySefjEWLFsXec61FFRpj8OWXX2LOnDkAdsYFGmMwc+ZMDB06tNkPbhpjxKYcAK677rpmP3D5/PPPN/t8Ywx+/vlnzJkzBxUVFejcuXMs/zsnJweVlZXNrsOvm/IvvvgC77//PrZt2wYAGDduHOrq6lBfX49Vq1YBAF588UX4/X5MmzYt9ot+NBrFM888g549e2LFihWx5v3FF1/E2rVr8dprr8WW0b9//7ix7bdQV1eHcePGIRQK4aCDDsJjjz0Wt92lpaXIyMjAn//8599k+US/l5KSErRt27ZV027fvh0nnXQS0tLS8MILL4jPmzx5Mmw2m3jD4cADD8Tf/vY3PPbYY00+b5aSkoKkpKTYW88ax4ZrrrkGV1xxBXJycgAAf/vb3zBr1ix8/PHHyMzMbNX2EO+Y/2aKiopin1SWSPVevXrhl19+AQCEQiEMHToUb7/9Njp16gQAmDJlSpO7gJWVleIF6umnn457K4vT6cR5550HYOd72Q866KDYl3bcd999TGSh/zl/+9vfUFxcDLvdjuXLl+O1117D9ddfj0GDBqGwsBBbt25tMs2ud5r9fj9eeeWVuL821dXV4eqrr8aWLVvgcDjgcDhiTaHdbkfPnj2xfPlycZ1294653+/HokWLsHr16rgIwyuvvDKWRGKMgcPhQCgUgtvtRiAQwI4dO2Lv8fT7/fjkk0+QkZGBLVu2xGIcG912223wer14+OGHUVFRgcmTJ2PmzJkAdt7tuuuuu/D000/jiSeeiF1YJ0yYAI/Hg0AgEEuDWbx4ceyOeU1NDa677jpxP7RWVVUVcnNz4XQ68dxzz+HYY4/FqFGj0LNnTwDAjTfeiP79++OMM87Y68sm+i28+uqrzT6+cOFCHHjggbs9v+XLl+OUU06Bz+fD3Llz494Ou6uff/4Z//73v3H55ZejXbt24vxuuukmvPjii7jhhhviPqPya6WlpbFfJAoKCmJ3xy+77DI8+uijWLZsGdq3b7/b20M78dbob6TxbldzP6+//jpycnLEemNTDgAvv/wyNm7cGPuNFABuvvlmFBcXx/00fuvWrwWDwSZ3zHe1dOlS9O7dO/bvmpoafmCD/ue8++67eOyxx2L/XrNmDV588cXYv7/55pu4c+zX335ZW1uLN954I3bHvKqqCmPGjMHo0aOxadMm5ObmoqioCCeccAKef/55vPHGG2pqAbD7d8xra2tx0003Yc6cOTjooIMQjUZx9dVXY+PGjaiqqsLAgQPxzjvv4N5778Wf//zn2B3x9PT02Dzq6urw2GOPYerUqRgyZEgsxjEYDKKgoAAHH3wwHA4HzjjjDNx8881YsGABBg4cCGDnXx42bNiAsWPHYvPmzSgsLMT8+fOxY8cOFBUVYceOHXjxxRfx1VdfIRAIxDLVa2pqWvQ2lt31yy+/oGPHjgCAQw89FFdffTVOPPFEbNu2DS+//DLefPNNPPvss3t9uUS/p+LiYsyfPx8nnnhii6fx+/144IEHMHjwYLjdbixYsCDuOr6rYDCIiRMnIiMjA7fddps638LCQlx44YWYMWMGFi1a1OxyL7jgAgwaNCgW93rXXXfhuOOOA7Dzg6T33nsvJkyY0GwsI7UMG3MLC4VCmDJlCi677LK494Dff//9KCwsjPv58ssvm51HaWkpZs2aFWsKdm3g6+vrMXv27LgBoaysLGHDQWQ1q1atQufOnWP/HjVqVOxOcCLl5eXw+/1x36R5yy23YMCAAZg8eTLGjx+PK664AsXFxfj6669x+umno6KiAtnZ2ep8n3322bhv62z8ufHGG5t9/pYtW+J+AX/rrbfw008/4YMPPsDdd9+NtLQ0HH300XjyyScxceJEVFRUIDU1NZYQU11djerq6ma/hfPf//43DjzwQAwYMADAzj9BFxQUNIl+dLvdWL16NTZu3IgDDzwQZ599dlzzu3nzZvz3v/+Nm+bYY49V7661RjAYxE8//YTBgwfHHrvjjjtw7LHHYtiwYbj44osxderU2N1zov9VV199NZKSkjBhwoSEz924cSPuvfdedO/eHTfddBP+8pe/4Mcff4z7/pNf+9e//oWFCxfimWeeafbbgn/txhtvRHZ2duyvjH6/H//9739x/fXXw+fzwePx4P3338fo0aPxwgsvYN68ebEPfwI7/3o5bNgwHHnkkVixYkUL9gD9Gt/KYmFTpkzB9u3b8X//939xj99444244YYb4h7bNf6oUV1dHVasWBF30uzq3nvvRdeuXWMZ6A0NDSgvL2/1e92I/iifffYZ7rvvvlgu+cCBA5GXl4fKysqE0y5ZsgTt27ePewvXv/71L6xbtw7Dhw9Hv379cOmll+Loo4/G7bffjoyMDGzevLnJZzl2pX0Yszl+vx+rV6+O++KgcePGYcCAAbjqqqvw888/48MPP8Tll1+O/v374+ijj8bcuXPj1mHp0qVo3749kpOT4+b9zTff4N577437a5nH48Fbb72FI444AjU1NXjwwQdjHwh76aWXYnfbI5FIwhSnXZ/T0NAQ9xmZRrW1tQB2ZsJLUW4dOnSI/bXunXfegc1mi4t09Pl8yMnJwebNm+FyubBkyRKMHDmSNxLof1IwGMSVV16J//znP3jllVfU4/ibb77BhAkTYh+8bPyLV0ve/vJ///d/6NixI84888wWrVf79u2xYcOG2AesV69ejcsuuwx//etf8corr6CgoADPP/88qqqq8MMPPyA/Px9r166Nm8e0adNw7LHH4uWXX1a/w4Wax8bcotasWYN77rkHd9xxR7N3wBotXrwYWVlZKC0tbZLg8NZbb6FXr15o3749fvrpJ+Tn56OqqgpOpxOvv/46Hn74YXz99df45JNPkJOTg2+//RYOhwNdunT5rTePaK+JRCIYM2YMjj766Ng3UaalpcX9Kba5jOvGKK+PPvoIQ4cOxfr162PfehmJRDBp0iRMnDgRxxxzDI455hgccsghOPbYY7Fu3Tp88MEHzaYfzJw5s0VJKrsKhUKYN28eMjMz0bZtW7z33nvYtm0bbDYbHnzwQaSlpeGdd97BlVdeiZUrV+Kll17C+vXrMX369Lg7ZR9++CGGDh2KmpoalJeXo6ysDF9//TWuuuoqPP/884hGo1i/fn1s/fr06YOPPvoIY8aMwYknnoiRI0di06ZNePTRR+M+w3LJJZc0+cDYr7fx4osvBrDzC8+OPvpocVvHjh0r1mbNmoUxY8YgHA5jypQpOOecc5CSkoK6ujq88847uP3225GZmYlFixZh/fr1uOKKK/DEE09gwoQJOPnkkzF8+PC499MTWdWnn36Kq666CqtWrcITTzwR+8yXZMiQITj55JPRt29fnHLKKbt188zlcolJLZJdU48OOuggbNu2LS5V7oILLsCECRNQX1+Pjz/+GBUVFXFjQlpaGj7++ONmE5WoBQz97l5//XWTk5OjPqesrMycdNJJxu/3xz1+7rnnmvvvvz/27+HDhxu73W4yMjLMjz/+GPfck08+2TzxxBPGGGOOO+4443a7Tdu2bc3cuXNNRkaGmTlzpjHGmAkTJpi8vDyTk5Njbr755r2xiUR/iAcffNBMnDgx7rH27dubzz//3Ph8vtjP7bffHnvepEmTzJtvvmnmzZtnDj74YHPiiSfGpg0EAqZ///7mzjvvNOFw2Jx88snG5XKZXr16mc2bN8ct5/bbbzdjxowxtbW1u/VjjDFvvfWWueKKK0x1dbUZMGCAGThwoNm4cWNs3hdeeKE566yzTHl5uXnggQeMx+Mxubm5Zvbs2bHnnHTSSeY///mPWbRokXE4HMbj8ZhPPvnEfPzxx8YYY0aMGGHsdrs58sgj49a7oqIi9v+rVq0yxx57rIlEIsYYY84880zz5JNPmlAoJP4cddRR5vXXX2/lK9bUl19+aVJTU83GjRvNpEmTTFJSkunQoYN54IEHTDAYjD3P5/OZf/3rX6Znz54GgLn++uv32joQ/ZY+//xzc9hhh5mvv/76j16VPVJaWmratGljXC6XGTNmzB+9OvsMmzG7fPUdWd727dvh8XgSvr8V2PlWFrfb3ey3FFZWVrbo/WZE/+sikUiT3F6zS7qK9JxG0Wj0d4kQ3dN1qKmpQVJSElwuV5Pt2x2/1/Zq1q5di+7du2PRokXYtGkTTjnlFDXj+YcffkDfvn2bvI2HiOh/DRtzIiIiIiILYCoLEREREZEFsDEnIiIiIrIANuZERERERBbAxpyIiIiIyALYmBMRERERWUCLv2Bo1eQ5ctEfEkuRaESsuVOT1GU63fLqhX1BeTqnHKtlonIITTgsf8NddW2DWPOmesUaALiU9QlCXp+sbvI3C/or6+UFKl9w4ggp3+JX55NrAKIReVqnV/5ij3BQPgbCIfnYqa2R1yctq/lvDwQAe4KcoWgoLNaMS36t7ElNYycb9ZzU9JtX93XfPyyPCU4jH4MRJQiquLhCXaavRj4PtYCpqHLeh0LyWFJeUSPWPMny+BWF/iVDDmVMcLnkcc+uRAa2zUwRa15lLHW65XkmZ8vnGQBUFVeJNb9y3vvq5XO73ifX6ur8Ys0oQ5snWR+jU9Llfdc2Q45g3LR5u1i76I1J6jL3Ve9c+x+56JWPw6x8+Zsv0xN8UU3pplKxlt9Vvo5uWrtNrK1ZvFqs9R58gFir2SGPGSUbtoo1AMhsK0cYa8d+ske+/to88nWrz7DmvxUcADYtb/oNvrF1KZe3EQAcyjXYnSafTw7lHG7bUf6ixTJln29du0msHTGsv7xAAEnt5FjqOp/cg21bWyzWTr97vLpMgHfMiYiIiIgsgY05EREREZEFsDEnIiIiIrIANuZERERERBbAxpyIiIiIyALYmBMRERERWUCL4xKddiX+S4nqcSfJ0TharB8ARBoCYs2uxJ9FQ3JMV0iJyqtUYh99AbnmSfWINQAIa3FkXeQIoGC9HOMWUuKB7A759y2bXa45ErweNiXirbpWjjiricivR4PyWnmV7UhXovEiET0v0SjbYdP2gXI87o8iQflc0hjlGMxpJ8eFAYAvRT7XKsqqxVplaZU8T58cQ+ZKkpeXki5HCfr98rkLAC4lltOhRCKGlHNJS2hUThc4bMr9GZ887gFAjXJOuJPlfRcNydeMHaVyZGanTvlizaWc95VVSrwsALdyDdPicNNSlevbfkobQtu0l89vm10+7jcuWa8u0+2Sj+FyJUoxrPQD+R3lmMWgTz7uTUSeZ15BG7EGAAUd24q1JYvXiDWHQz5GvW45LnHrcjlKMFuJbsxU4kUBoGTLDrFW2EHextKiMrG2ZbMcQehVrimDDj9IrGV30l+P4vIqsfbNRwvl9XHI+7wleMeciIiIiMgC2JgTEREREVkAG3MiIiIiIgtgY05EREREZAFszImIiIiILICNORERERGRBbQ4LjGqRNchGlVqcoSVPahHcdki8ny12D+jZIM5lSiyrFS5lqlEIpr0JLEGAM4sOVIrVCLHHjorGsSaV8lGiyq1UFCOW/OF9NejVomAK62qFWteJXIuM0mOeUpyyYdndY28b0J6WiJSlBi3JCU2zaGsz/7I5ZT3h69GjqdzpsrniztTj+FqUGJbjRLt50yRj9383EyxlpYmn7v1yvngcOvHil3JlAsE5PnWVteJNaOcvx0L5Ygy45WP+VrlPAOA2jo5JjVNeZ0blKjNzJxMsda2k7wd21ZvE2sOp34PKqK8lg11cpym08Ex4dcKO8gRwKnpaWJt7Tr59QsE9ajawgO6ibX1SzaItZ4Duoq1bxYsEWthJVq5TU66WKsqk6/3ALB04Sqx5vLKEXx2pWZsch+V7JWvhTs2y5GH5dvlGgCkpslRsptXF4m1WqWPcCTL25jTRY5RbddFjr3cUVwl1gBgzaLVYq1NRoZYSxSXmwjvmBMRERERWQAbcyIiIiIiC2BjTkRERERkAWzMiYiIiIgsgI05EREREZEFsDEnIiIiIrKAlmc9KRGENo8cM2i3Kb2/Mk8AcDjl+UaUuDGbEu1oU+IbjRLfGFaiG531CaJxauS4La8S+xhR9k9dgxwfFVamcyn71KFESQJARoocf5adLcdguZTouPoqOY5te3m1WKv1yfs01SNHQAFAkkveTn9IjnFzapGh+yG/EusXCsmxnGWllWIt06Xv48wc+TirqpCjBOGRo7Y8SqxfkhL16VYiynxKjCAA+APyWGNX4k4zMuXt1yIabUotHJCPeU+aHgXbWdkH3rxMeX2U2MusTDlqraJUjpvbur1MrDmUcRYACgraiDWvEpkZUsaL/VWdcg6v/Gq5WKtWxvtU5RwFgPKt8mvfb3hfsRbUrs1OOUY0XTlG66vlcz81Wd8OT7IcH+yzyWNq567txVp1qbxf163YKNYcyrmdniWPQwDQs0dHsVZUXCHW2naTow0Lu7QTa7U75HFh2xo5hjO9jRx5CAB2l3wMaJHdWo/REuw0iIiIiIgsgI05EREREZEFsDEnIiIiIrIANuZERERERBbAxpyIiIiIyALYmBMRERERWQAbcyIiIiIiC2h5jrmS/21T8ozDSja4S8kWBoBIVM4OjyTJ+ZKlNXKe8fbNJfICo3K2buf8HLGW7ZHXBQCMkh0elheJsJKTmazkutqVjGCbkv+eKFfeKPMNBeU832BDvVgL++VjJ9Ut71encmzUBuSMdwDwO+QM1uQsOZ/WyIvcLwWV1zyk5P5XKrm6O3bINQDo0kfOx/Uox0u6moMsH9cNSt640yFPFw7LmcMAkKJkBLud8nlfViOfSy7l3C4plrPj3cp2RBJk93fqnC/WkpRxsfchXcVafbk8fhslH7/7AZ3EWtCnf9dEUlqKXFSypZ3K+LW/sinjwua1W8SalvGd1k7OmQeAykr5mMkNyq/R1q07xJpNOYfra+Ss8o49CuR5BvVxob5Bnm9Gurx/lny7Qqw5jXx+d+3ZQazldZYzxeur5e8fAYDtW+RceVeqktXeII+3RWvlPPLt67aLNQO5r8mu07ejUx95/2ivpJaP3xK8Y05EREREZAFszImIiIiILICNORERERGRBbAxJyIiIiKyADbmREREREQWwMaciIiIiMgCWh6XqMVCReQ4GrsSmRVO8GtBSInOKS6V47+2lVWJtYwUOXKoYzs5EjEzXY7T0uJ4/v8niMJaHFuSHKlmwnIcXVSJP7Mr6xLxyesCABElMlPLErQne8RaRte2Ys2rzDNYWivWshoSRKN1zBZrpkGOWjTb9Ci//c22Cjm6DyH5NUhRzsFARI8T27K2WKy5lQjCJLc81NVWy9vhVSJd7R55nu5MJX4PgMMlj4uBihqx5lQiEcPKsRsIyRF2Vco+96bI0WYAYFPiFKtLqsRaqjIoupVYuDQlstWhjDMNCeLdfEqcYn2tPG1WXoY63/3R6p/WibXcXHnsze0ox/N1OlCOSQWAtYvlZbqUHiS7fa5Ys9fLx0SdcjytWi6vS7I7QUS0Em3YJSddrOUo+zU7U44A7j6wm1grLyoXa1vWbBVrAJDXQY63NMqYWlksL3PVms1izaOMp0kpyWKtukQeawFg/YoNYi2jTZZYc7nkiOyW4B1zIiIiIiILYGNORERERGQBbMyJiIiIiCyAjTkRERERkQWwMSciIiIisgA25kREREREFtDiuES7kmJmlAgvoyTshYJyhBcAOFPk+KscJTarME+JsXHKMTYmIsfzRRr8Ys2mzDMRm7JM1MnL1HasTYmGCyvRlv6wHlUXTZNfj7AS32iiSm1HlVjzKvP0NMjHji1RVJESe+gOyfvAvgev874oqkTelSpxpilJ8nGUlS1HggFAQ1WdWKupliM089vKcWI1NXJcos8ux6JpEYQG8vgEALlt5TEqTYmTzM+Xt8OvxCWWlsuxYMlKzGRuGz0OMBCQ98HG9XK0pbdEPj669esk1vx+OdJ166Yd8nT1ylgKwKOMmR6nfP/K6eCY8GtpqXJUaN4BcoxekhJ3+ctXK9VlOpXo0nULV4u1zr3kGEZ7BzlKcd3qH8WawyYfL1GPfo0t7CDHB7frKsdJ5nSRa7Xr5fOiukweFxZ/s1yswa4f93Wbt4u1zCx5TFn3ixxP2K6tHGcdUMYFu11+PXwBecwEAKdDPq5sSuumJUu3BO+YExERERFZABtzIiIiIiILYGNORERERGQBbMyJiIiIiCyAjTkRERERkQWwMSciIiIisoAWxyUatxyP41SipqJKDS799wKbLyjW0jwusabF42i1iBL9FVViBo1bj0bTIiOrfHJcT0WDT6wlpXjlmhLB6PbI0WhhJYISAJR0PHiS5fWxK3GJ6gHokBcY8cjRWjZleQDgUCIRQ0E5dsnhavHpsl9wKQeEVznOtCQp7fwEgJw8OS7QKHNOUyIB6+vkSEQt3CzZKx+DUeU4AoCQct6XKvGNyelyFF1ujhw12VUZLwJKbG1JaZVYA4AaJUY2rAx89TXy2LZq8XqxlpqlbH9bOYatokwf2yJKVKwWk1pXIx87+6uwEiMaisq1SLl8TkR8+vmUmSu/9ut/2STWgg3yfEPKdTQzM02s+ZVz4sBBPcUaAFRWynGwlZVyHGyacj3cUVwm1qLb5HO0Xef2Ys1Xrx/3+Z3l2MeMDHnfpbnlvq5ie7lYc3jl60ZIuaQ4lWhLAMjqXiDW3EpPaFPiO1uCd8yJiIiIiCyAjTkRERERkQWwMSciIiIisgA25kREREREFsDGnIiIiIjIAtiYExERERFZQIszXWxK7KEWKWbq5OggE5ajkwAgmixHAEW8cqyOKyivUVSJytO20YSVCD4t/w1ASFnmDiUCqS4sRzlpkYApShyRW/lVzOOSY8EAAEo8EALyurqUqLqQcgyElf3mq5fj1tzZ8vYDQFIbOVYu0iBHdlWWVou17uoS9025bTPFmtstv+ZRJbkuNUHMVHGZ/BqElcg7W2qSWPMkyeNMOCLP06VEO2Yq0YUAUFElRyL6lbg5OORztHSHvG9MRJ5nTm6mWMvOSpXXBUC1EvvoVcbv4qJSsZYelZdZWSGPl5mZ8nT5BTliDdCHcLdTfp2j0QSD/37I5pRPcIdyGd2wbptYC4T0uESnch3Nzs4Sa7VK3GWaV44Y3VFeJdby2rcRa1nKmAkADQ1yRHRQuR5+8uEXYq2tEjF78KF9xFpyO3m/LZq3TKwBwC/frxJrBYXy/knOkmMvvUny65GeLY+3UYd8PFaWVIg1AHAqY3zIyAdz7Y4qdb6J8I45EREREZEFsDEnIiIiIrIANuZERERERBbAxpyIiIiIyALYmBMRERERWQAbcyIiIiIiC9grcYkmKEdx2ZQIPpcSpwUAYSNHUdmU6CB7RI6xiSo1Y5NjdWx2uRZV1hMAlLQedFOiyhytXB+jxHsFtV/FUuQ4IgBwKq9XqFSOqwpW1Ym1iHJ8BDPl9dm8dYdYq9i8XawBAFLkKL+DB/QUazYlonN/FNTOMyVKCkY+doNKPCEApKcli7Xt2+RjYnutEouWKs8zWYn69LiVyFanPrRqUYLaeRjwy/GEZcVl8vKUdXUnK9uvxEwCQJZLnm9NnbzP2+bJ8YVepzwmBBxynFxUiWytVmI2AcCTosRpZqSINbvRj9f9UedeHcSaPaxc093y6+6I6vs5O0uOy6tVonXbtssUa2FlXbXk4BTlXFu1dL08IYCQEguZ1T1PrLmUaMfufbuKtVQlEjGkjMWd+8qvMQDUVctxsJu2yVGpB3WWt7FLOzmUuHiDPM/6GnldktL08S1NiYtNz5ejHb97/zt1vonwjjkRERERkQWwMSciIiIisgA25kREREREFsDGnIiIiIjIAtiYExERERFZABtzIiIiIiILaHFcYiRVjuOJ1ssRXjYlwsr45OirnQuVI9ccHjmSKKLE/NhC8jztSsygluLmcCX4/UaLU3S07nejqF2Zrk2aWMoY1FmsOZUYQQCoLaoUa46AHJkZ3FEj1rSgyaQsOaYskiRHa0Wr9fjKtplytBYC8jHpUo7H/VFVlRyHFw3LJ4xfiS+rV+IAAaAgX47Z69oxX6xVVcqRnVrsoV05P7WY1EQxXO4UOS6xuFw+X/Lay9tfWJAt1mzKPRi/Mg5rYykABELyea+9ztntcsWaJyrv13Rlv0XsSvSqPiTAroynNmUf+Or9+oz3R0o8cKV27iv7Middjq0DgLyeBWLNWVIl1/xyf+JKktujTOXcr6+Uo4MbKuTtB4BwWB7/QsqY2r6bvP3ZypixacUmsbZtXbFYy28vn78AkNtGjhLMViIIK5Wxr6BXO7GWrMTorl68WqwlSLpG6aYSsZaZLfdZXXp00mecAO+YExERERFZABtzIiIiIiILYGNORERERGQBbMyJiIiIiCyAjTkRERERkQWwMSciIiIisgA25kREREREFtDiHHNTI+dvGp+SY65kP9sThUgG5YxcE5TzRx1KLrHNpeRfKwGsTq+8qyI2JbgVwI6aKrm2Q64VFrYRa9pvVFEls9leIdccNfrvaQ1LNsvzLauX55ssZw+HlF1XreTBhsJy9nL/Ib3lmQJwKNnL7oCSVZ6iZ1Pvb+xROVc3EpX3Y7qS8e3U8vkB+BrkrGMTkKfNVnJuw8o45PTK2f5u5XsPkpL17wQoL5OP7dxsOWc/M1fOAA4G5dejvkbOnK+qkc9dvz/Bd00oY1+Dsj7OSnmZHrc81uYq39EQqJa3MaKc8wBgXNr4Lk+XmiJ/v8f+qqyoXKwV9ioUa4FqucdIydFzzKM2+RzWzlOjZINrmdqZBVliLaSMUXqnAES88vHUUCKvT5s2mWKtUpluzeJ1Yi1Nud6VlVaJNQAwyrgQCcn7/IDDe4q1uh1y77Lp+zVibcUKuZbdTs54B4Akl9y7LPt2pVjrcUh3db6J8I45EREREZEFsDEnIiIiIrIANuZERERERBbAxpyIiIiIyALYmBMRERERWQAbcyIiIiIiC2hxXKLdJ8dmOZRYLIdTjic0SsQRAESVabXfKSJK/FlUiW80SsyiFqsUsuvbUQ85qmtLUI5WSorIy2yXKcdHRZRoy9rvN4g1hxJjBADusLzvtEgxkypHxzk8yj4vrhZr3brJsVs5hdnyygAIlchRdcYhb4hRtn9/lKVExUWi8jmxubRCrLkd+pCUokRvBrRIQCV6NSlV3o40pda+r3wMur3ycQ0AkZ+3irXKKjlKcNP6UrHmV2Jr05T9lmSXj3ktShIAIsp4mp4kL9OlnGdVZfLxUVmyQ6xpEW0FebliDQAcyrQByNuYnJ2iznd/lKfE/O7YLL9+lSWVYs3u1noBYPU3q8RaWLmO2iEf3w6nfA4HlWtsTa18/kaMfg3pO6SfWFv3oxxt2C5PvuY5suReyZUsj29B5dRPFBzsccivV0abDLFWvK5YrHmT5HUNK2NYepYcP5vdXj5Wd67PNrGW361ArIWUMaMleMeciIiIiMgC2JgTEREREVkAG3MiIiIiIgtgY05EREREZAFszImIiIiILICNORERERGRBbQ4LhGBkFxToqZCUTk2xqZE3ABAVJmvzSuvelSJaoso0UlOrxzxo8UsOpW4SADIkJPacGgHOXLH5ZZjBrUwNleaHCsUrZPjGROkVwJKfKXDKy/TqcQjadGO6anJYs3lkqPYTLU8TwDwReVjeWuxHNXWvXdndb77mxrlWEpLkV8fr1M+d/0B5WQBEGiQIwHdyvEZVWLRsju3FWudhnQTa2mdcsTa5u/kaDMAaKiXt6NBiaYtLi4Ta9r5m6TEktqUmNjMVD0Yra5cjh71+eVtzMxJE2uV5fJY67DL95K0yLi0TD3WMKWNHKnmyZPHL3+lHNG5v2pokI9fpxKhWaDEz/mq69RlRsPyuOGvk1+jvkceKNZ2bJajSd2p8nYceswhYs2Vox+H9iT5mr9jU7lY265ETWaF5H3T88AuYs3vl6+TPy+W4ykBIDVdPr/9Afn4qKyWx5MBR8lRkp169xdrK75eKdZcyusIADXJ8vjnV3qXsg3ysdMSvGNORERERGQBbMyJiIiIiCyAjTkRERERkQWwMSciIiIisgA25kREREREFsDGnIiIiIjIAlocl2hTIs7CSuqhcSsRe4ni+exKDKFbXh+3EjnkU6K4wkpUm02JS7QpsY4AkJwsx/5p8UhGmW04qMTKKdPZkuV4IJsSNwcAEWWZNiWiMlhaLdZS8jLl6bLk/ebIkGOMAsprBQBbS+XIueXL1ou1wt6d1Pnud5SIsuRkOWIuWzlWtm/doS4y2SPHjTmUSMBuQ7qLtfyBncWaUzk/t/yyWaxt/mGtWAOAlCT5+M3KThVrVTvkWDTtxK+srhdrESXS1p1gbPPVy5Fh3jT5/A0r69q+YzuxVlcjR9+5lWgz49IvdZ2O7CXW7EnycbXkv9+p890fdegix48GlWtIXakcledMMKavXyefi4ccfbBYSy/IFGsrvvlZrCUrsY9wyMd22Ojb4dsoX5syUuTzqbpcvsZu3bhdrG1YI++3foMOEGvdEkQHR5TerkaJJE5Jlcf3RXOXiLU+Q3qLtWplzEhNML7VK/GNOflyPGtdhTxdS/COORERERGRBbAxJyIiIiKyADbmREREREQWwMaciIiIiMgC2JgTEREREVkAG3MiIiIiIgtocVyiLxySZ+KV46QiWjxQgrjESJ1fXmZIjlKsrqgTa+W1cnROZqYcU5aapkRxKXFaAFBaViXWtqyV44ry2+WKtfYd5UgquxJ7aMLyfov65dcYAOCSf48zYSVO0q0cHylyzZ2fLtZCQXldg5vKxRoApAflA2/EoQeJNVdNQJ3vfsfI+9Hvk8/dDOVcqkn3qotMyU0Taz2HyZFZmQfkizWbXY7MWrdwjVhbMXOhWHNE9ehRtJHPpdQ28jikRRBGlLjX+np53Av65ePa5ZXjIgHA7lDGBKWmxebZII8lXuVaE1ViH71eJd4OQPlWOcItr5cc35icos93f1RWUiXWtq7ZJtb8DXL0pnZ9AYCkVPm8SEqRx5vMbHk8ye2UJ9bKNpaINY8S5bxsuRzHCwAH95djO9t0aiPWGmrkOFRXgTxdaYkcv+qwy+dvVYI4QK8Sy5yqxcGW1ihzldenrkG+3nQ7VI7KLVXiKQEgqkT7ZrXNFGvVpVqsbWK8Y05EREREZAFszImIiIiILICNORERERGRBbAxJyIiIiKyADbmREREREQWwMaciIiIiMgCWhyX6FAixZxaklFAieDz6It3KtFoWiRgRIlr0oINo0rMUVCJGYRdjxSrCQbl+drkOB5tt0Yj8vpElH3uyU4Ra34t2hKAt4Mc32hT4qw8mXKUVW2ZHI9U8+lyseavluPfclLk5QFApvJaltVUibX1i+TovD5/OUJd5r4op322WOt4YCexpiRQIX9IV3WZKXkZcq2dXDPKQnes3i7WFr71nVjzQh6DHMl6hGpIi5qMKOehSxmjlLhTv0+ORCzfIceL2nNz5HUBkJIjR5o6lDG6vlaOtO3Sq4NYK9sqr2tEiWB0J4i0LVqySayVri8WaxWlejTr/mj9so1iTYvmrK6UrwXZhfK1BwDyusiRgLXKtWLHhlKxlqXELG5WxrDlP6wSaz0OPUCeEEC3o+S6egQr17RgnbzPU1LlbayvliMYS4vk/QYASUov1eNAeYzPy5fHG3uGHKVbslEew3Py5DGqtEiOSQUAhxL17FLG4n6HydG9LcE75kREREREFsDGnIiIiIjIAtiYExERERFZABtzIiIiIiILYGNORERERGQBbMyJiIiIiCygxXGJzoIsseZ2yZEyNiXWz6R51GVGlPivULkc5ZOeLsflOZX4G7sSU+ZV5unXohQB9OrVUax169hOrDlS5BjGBiXKaNM2OcrIXyRvY0HnPLEGAO21qLZqOeqqfqO8PuHSWrEWqpdjJu3KcRV1KBGdALYrsVxBJcauXbYcu7Q/Ss+R40zTurcVaw2VPrHmypIjsQDAnSrXfVXyObH1hw1irfTnrWIt0CBH8BmPfF8jLU2OIQOAsBIvWrlOPl9SlHEoxSOPbaZejowLZsrHdVrbTLEGAF6vPIZvL5ajyDr0KhRrO+r8Yi2kxNRlpsjHRiiojwmpynY0KOvT5YACdb77I48SwVdTI8dkdjhAvk6266nv5yrl3C9at02s1ZbK14Jkuxz3mZ6uxQ7L16bUBOPCpu9Wi7U0l9wPeJRjP9krT1dfLV9/7Up8cpLS8wHAps3yPocSpTj4iAPF2vfzfhJrVSWV8vKUqNzuB3WXpwNQkJ8p1nYofU1gD+958445EREREZEFsDEnIiIiIrIANuZERERERBbAxpyIiIiIyALYmBMRERERWQAbcyIiIiIiC2BjTkRERERkAS3OMbcVyjnm/lo559XhVXI7bfoyAzvkjE2HQ/6dwii5nR4l6xd2eYVCDrlWUylnqAJAWVGxskh5OxqicoZy+y5y/rlxyfP0bZazhVPycsUaAISUfGVnlXwMuKJyHmpUyWb1pso5sg4lCzWiZB0DQBunvH+SMuWcaLuSFbs/CtfJOfPFP24Ua3a7vP/DG+VjBQDqlUzpcG1AqcnTpXrl17VbZ/k8CwTk7U9xyMcuANQF5HPbpmRu11TIY2Kb3AyxlpEpZ87Xh+R9nqLMEwCqS6vEWlj5fgePcv4G6uXXqkGpRZSM97rtVWINABxe+bqQ3UkeF7d9u0Ke6cSj1WXuqzxKJnySkv/d8cDOYm3TEvl7CAAgt1O+WCtoI79+DfXydyr8slZepl25p3nE2CPFWn2JfP4CwLbl8jL7Du4lzzcijyepSXIPFvTL02nZ6Dkd5O+pAIADjj5IrDWUyftg0xr5OyXcLvm4ylC+i0H7zoisdvr45lPGW3+DPP4bpZdsCd4xJyIiIiKyADbmREREREQWwMaciIiIiMgC2JgTEREREVkAG3MiIiIiIgtgY05EREREZAEtjksMrVei8pRYnYhPjv6KluvRQR5lWo1R4tjsKXLkji1Ljtty56aKtawCPXKnYdkmsZaRLcf8hJRouGy3vB1tO7UXa7a2bcSa06EfDlGfHA+kRU1GlEhIo+xzZ5q8jZFqOebKWSPH5gGA1yMvM2jkrEVPoHXH476qVIm9ygrJUXkamxJLCgBhJQuzVonSc9nk+QaUyDS/EmsYVdalqlaeJwDUNcjHaKYSKZedI49DLuUcTEqTI9NSahrEWuXWcrEGAFq4pdcpR0aWrJcjZPM65Ym1iHKtcSfJ40U0qr8eCMmv87a1JWKtwS6PJfur9Ez5GG3TRX5tPR45Otdfo79+tnr5GA4G5HMtO1uOEc3pPlisJSnHWsd+HcVaSVDuBQAgmJ8j1jat2y7Wan3y/jl0RH+xltdejpIsr6wSa70HdRVrAGBTYql/WrpRrKXlKZGvyvFRFZGvN5XbKsXaN+9+JdYAoEDppVJT5DG1ZLM8ZrQE75gTEREREVkAG3MiIiIiIgtgY05EREREZAFszImIiIiILICNORERERGRBbAxJyIiIiKygBbHJdask+MSU5XoILcSmZXot4LagBzPV1ZdJ69PhhzXlKbEYnmDci1QWiPWHBlybA4AdFQiibQIwuTeBWLN1UaOWYQSVedfI7+OkYp6eZ4AomF5vk4lhtKbLu+fkBK45t8iR7U5ffJr5XTLUU0AgKC8TFtQjkS0uxLMdz/jcSgxmMqxYnPIY4JNmScABMLy615ZK0emuaJytGFQidpyKK+5UcYSd7Ic6wcAUOIbQyH5GExNk+drlHOwslaOkszKVeJeldcKANLzM8VaxTb5/HVD2X5lTGzbRl5eTvsssVaySj42ACCgxFfaPUrErnJ9218lJcltxeYNW8Vabq4cXZjVQY75BYBqJXq5rEK+dicrMaKB8mqxltIlX6yVbywTa1Vb5BoANCgxq1ElBjril69pFcoyq5VI5qKtcqRpwKUf9z9+9qNYy82TIyF79+kn1r6b/YNYc0TlsbhTlw5ibeWKDWINAAIN8rWhzyE9xFppkf46J8I75kREREREFsDGnIiIiIjIAtiYExERERFZABtzIiIiIiILYGNORERERGQBbMyJiIiIiCygxXGJ2hPtETmqx6fEpgWVCDMAME45UiuvUI4g9NiUGDclErF2e4VYC/1/7d15lN11ff/x113nzp01kwlDFrIBAUNCWAIBRAMUJIoesC5wjhsIlIpa/SEuVFsoyLGliLQSSwsc3ApqAaHVEgoFsRSCImsQIpBtsmcms965+/3+/sjJbYbk/b7DjSHfJM/HOTknM+/5fr+f7/6+37nzuk5MW9PhdqyhJLXNnWzWep963ayVXrUjvFK9drRhvseOh1LGjmJrqBEzGEnaEUmBF//mRE3K2R9NzuvGoCFp1krO8ShJQWAfd5EGext4cVUHomLBjrVrSNjbKpa0ryZ9A3ZcmCQVZO+7jBM1FnGiDQvOud3SakevKmof8/kRfz3iTixkIWefo2vfsCPMps62Y8EiznYbdpZXLttxapKUcY6BzFY7wu6grg6z5t1PChX7fqIG+7jKDPv7o1iyl9nqxO+m0naU4oFq9cr1Zq3iRE+ueLnbrE2aPtFd5rp1m8za5On2/XfjGjs+ONZk32OOOPlIs9a7bK1Z27TOjhCVpMC+pKizy44DbXMiTzPO9S3txC4f2m5HSfZttqMkJWnmUYeatcOOnWnWSiP29aQ5ZY+nFNjrGInY176j5h5m1iRp/GQ72rGx0e4VJjjXt7Gg0wAAAABCgMYcAAAACAEacwAAACAEaMwBAACAEKAxBwAAAEKAxhwAAAAIgTHHJbZ2tJi1vBN5F0vYEXtNNeLnBrJ2VNemrXZcT2dT2qylp9jxN92/t+cZcSK8jkqnzJokFdbb802n7Pio8ogdY9b3mh2b5sXGtTljjTgxgpKUaLaPAU+02V7HSME+BMsDI2YtcI6rESf6TJJiTfZ4ss4xN7S536y9w13i/mmgxz6uE070Zi6wY0CHBuwYUEmKORFVibgTQWifSmrvsCPDnJRBFbwoWCeeUZJaxtvLjDvRo7m8vSLrnSjFtvF2nFrWiUvM9DpRp5KanbG2t9jX4UYnTjMSt68JCWeHDHXbUXTlgn9NyFfsevcmO0Z31twZ7nwPRP0Z+xw+5axjzdoz//6kWRvc0ucuc8Q5F195+Q17Qify9NR3vtOsxZ175biJdqxhz8Z+eyySMkP2tmvvss/h5hkHmbWXn37VrM10rguvLFtt1vq39ps1SVpwzgKztvbFNWZt69oes/aO+YebtZGsfU95Yenvzdr4GrGGXdPt7dq/0b7/xZxr2FjwxBwAAAAIARpzAAAAIARozAEAAIAQoDEHAAAAQoDGHAAAAAgBGnMAAAAgBMac6VJx4l+SDXYt2mDHm2X7ht1l2oF40tYt/WatNbAjkBpH7FidyS3N9nQx+zVMaZM9FkmKyB5PpWLHLg1n7NjDohPvlXZi44KKHSv1ByfWUZJSfXZ02iGTO+3pOu2YxWimYNYCJ3Ku7Kxj6uB2syZJKzZuMWurX1lr1ia32sfHgainf8isZZ3zvqXTjugqj9jHvCSl2uwIviYnnq+zrcmudbWbtS2b7HPCOwf7asQMDmbsWM7xTjTtkHPd6+m1x5pz4sT6tjr70RmnJDU2tJu15rS9P1LNdmxrp3P+di9dZtbyjfb5OTxiR69KUt+wfdzFvXtYr73tDlTHnGFHIg5ttc+LzRvtuMuJnfb9RZI6nGuzd3wf+e45Zm3cTDsq76l77WjH2SfONmsDw/5xWCzY98Omg53rphMjmirZ/ccrv/uDWctl7bGknF5JkgY32PGWyajT2dlDVZ9zLc4N29epiZPs/djY1GgvUNJQj92jdky1o7ebupwI3jHgiTkAAAAQAjTmAAAAQAjQmAMAAAAhQGMOAAAAhACNOQAAABACNOYAAABACIw5LnGkZEfnyE6108hGOzanJdXgLrPFiak6ZmKXWWtotOcbGbRjw5oakmYtFrcjfkpRJ+NHUsSpB06M2bgZdsxPkLJ3XWH1VrPW29Nv1nqyfpTTUc54Eo32tlPBjmgsDtkxZVEnojLmxK0NOTGLklRxjoHjDjnYrLU5x8eBKOu8rB/XZO+fYsbe/nEvSktSJF80axOcmMHiiB2nlXfGE3NiOZta7eWl0v61LenUi15MWdK+JpbK9nmmiH0NGslkzNq4Nj/2K5m0z4nWg+x4t3Fpez22vPiGs0D7uEo5x9zAuk32PCV1TXfuJw32fCOBHVN3oNr82jqzlojbx/30mVPNWn7Qvzf199mRiO9YcKRZO3z+4Wbt2YefNWtNTjxjrmxfo+adebRZk6TykH0tSk+wz6fXn1thj8fp3RJt9noUS3a0Ze9mu8eQpJGDx5m1pNOfZfPOWJ1r8bjpdm/yyiurzNr0ow4za5LUOdWe77hD7HUc2bJ7Mao8MQcAAABCgMYcAAAACAEacwAAACAEaMwBAACAEKAxBwAAAEKAxhwAAAAIARpzAAAAIATGnGOemt5p1gq9w2atYcTOpWxo8rN+Y0l7eMlOO1/Xyxwv99uZvYGTSVt2codLUT/LtmGcnRUqJ+8ycPKVi6t7zJqXql5wskBnn2rnvUpSOmFnFhc329u1HNgZ0vEGex8HUXusQxk71zZ+sJ33KkmHTZpg1qKDdq46RutwMnCbnazaZidvOp7wL0mlnJ0RXJR9Hm7otc+zlJNPH4nb48kP28d1LOHnsU+daedmF/rsc6nJ2a5FJwO47FyiIhPta3ubk9csSekO+zrc7pyHfW+sN2ub3+g2a5NnzTBrQ05W+8QZU8yaJKW77LEmA3u+3U5m94Gqd02vWUs453fUyegfzNnnmiTF0/Y1paWl0az1bxowa9n1/WZtk3P/+d3Sl8zaQRM6zJokTXBys4+e0m7WXnpqmVk7/sTZZq3BOb+X99vXzAbnGi5JJedc9D5vYtZR9vnds6XfrGWcz7eYcdR0s3boKXaOvSRl++39vPm1DWYtEbeP5bHgiTkAAAAQAjTmAAAAQAjQmAMAAAAhQGMOAAAAhACNOQAAABACNOYAAABACIw5LjE2YEf3aciO6UqPs6Nx4p12TZKSk+wIq0rMfk0RdG81a9kBO4osNsGO/qqoYtYSCT8ap5Swx9p41CSzll1pRyJmu+1a0ol2bGxLm7U3fveGWZOkcRH7cJl5yMFmLeZEZJVKJbO2YqUdRdZ5+ESz5kX1SVIsbe/LoOzUnKi+A9GM2dPM2pb19jnY7pxnIzXiKjets4/7fNa+RjU028d9rlQ2ax3t9nRDzliLRXuekrRxnR3hNnGSHamWd2LBUin7OuTFTLZOsSPass7yJCnqRFQOru8za5kBO4asYZy9/pWkfW73brVje8vO/UKS+pbb8Y0NTvRldph41TfLObGdkw+zYyvXr9po1g6dPd1d5opXV5u1wNl/Jefcb59ix+pW+gfNWsqJEhw/zo/ynXCYfV/bvHqzWZv7zqPNWpC317Gt2T6fZh0/y6y9/uoasyZJk2ba/cCgE1G5odtex2PPOsasZbP2dartEPt68tjdvzZrkjS4aotZK+Xs3uXkD7/TnW8tPDEHAAAAQoDGHAAAAAgBGnMAAAAgBGjMAQAAgBCgMQcAAABCgMYcAAAACIExxyUmnWjDwqAdfRWJ2PNMTxnnLvMPm+zonHyfHVM1o8GOIovF7OikiBMpFi/bsWClmB0PJUmFmL0RXn15pVnLrbNjhSY3N5m1eCpp1tYst2OlJrc1mzVJmji+3axVinZ0UMmJVFux1o7I2jw4ZNYm5eyIt/JmezpJKjpRbVF7qIrH7WPnQNSz2T4+M04s6VMP/86sRaL+s4LGlB3v1dFpR5F1dNoRjV5037hW+zyLlOyDZfNmOypQkvrX2nGSzY32+TvpHZOdefbaC3TiAmPONm9ot9dfkpJJO6Kx6EQtFgbta3R+yL62550400jUvs4mk/6trrHZHs9Qvx3DOL7LjmI7UE2cZF+b+zfZx31bix1N2t5k7x9JeseJs81apxNBuPT+J83ajEPtaMeRjH3N2OQcLxO77AhGSXr5f142a+OndZm1gyfb8129zL7nb+y2778L3n+iWZs4x74OSVJu2O6JRnrs+3OkYl9TUyn7HG6f3mnWep347OIme19J0qTxdo862GdPW9jNGFWemAMAAAAhQGMOAAAAhACNOQAAABACNOYAAABACNCYAwAAACFAYw4AAACEQCQIAicgDgAAAMDbgSfmAAAAQAjQmAMAAAAhQGMOAAAAhACNOQAAABACNOYAAABACNCYAwAAACFAYw4AAACEAI05AAAAEAI05gAAAEAI0JgDAAAAIUBjDgAAAIQAjTkAAAAQAjTmAAAAQAjQmIfcpk2b9IEPfOAtT9fd3a0bb7xxD4wIAACESaVSMWuFQkGZTGaPj2FgYECPPPKIWVu3bl1d8920aZPy+bwkKZPJaOvWrXWPcV8QCYIg2NuDOFDdd999Ox1gEydO1DnnnFP9etWqVZozZ46Gh4fN+Xzuc59TLpfT7bffXv3er371K51++uli9wL7tlwup/POO08/+MEP1NXVpUWLFumf//mfNW3aNHOap59+Wl/60pfGNP+f/vSnmjx58h9ruMAB55VXXlE8Htfhhx8uSbrgggu0cOFCfeYzn5Ek/exnP9N73vMetbe315zXkiVLNHfu3Ld8Tv74xz/W/fffr3vuuWen2u2336577rlHS5Ys2eW0O/YX0WhU6XRalUpFg4ODu/z5ZDKpdDq90/efeeYZvf/979fGjRs1MDCgtra2au3mm2/W/fffr1/96le7nOdLL72kgYGB6tfz589XKpWSJM2ZM0e33HKLTjvtNN1666165JFHdrme+4v43h7AgezBBx/UypUr9eKLL6q9vV1Tp07VvHnzdPrpp2tkZESS1NfXJ0nq6empTtfR0aFo9P9+2ZHL5ZTL5d7ewQN4W5RKJT300EPKZrOSpIceekhDQ0OStl0ffvCDH2jOnDk688wzq9P09fVpxYoV+s///E9zvoVCQQsWLKjOFzjQBEGg5cuXv6VpmpubNWXKlFHfu/766/Xaa6/pySefVCwWqzamkvTcc8/p/PPP19NPP60TTzzRnXcul9O5556r++677y035t/73vf0oQ996C1Ns11LS0v1/11dXdq4caNWrFihWbNmadKkSaN+dnh4WOedd56+//3vV7935ZVX6pRTTtHUqVOr37vkkkt05JFH6rrrrhvTGD7/+c9r7dq1am5u1u9//3v927/9m1577TVJUm9vr37605/qmWee0dKlS/WHP/yh+o6AD3zgAzriiCPqWu+wojHfi2677TZJ0pFHHqkbb7xR73//+yVJ11xzjf7mb/5m1M9OmDCh+v8vf/nLuuGGG96+gQJ429x444368pe/7P7MHXfcoZdffllPPfWUTjjhBDU1Ne30M5s3b9Z5551nzoPfpuFAl8lk9I53vOMtTXPOOefoF7/4hSSpXC4rCALdcMMNOvroo/XEE0/one98p4IgUKVSUalU0pe//GWdf/75Ou6441QqlRSNRkc9WNvRM888o3K5rHe9611jHs/GjRu1bNkyrV+/Xp/97Gd100036dOf/nT1abO07cV9pVIZ9QAvGo0qmUxWv85ms9q4caNOOumk6vdaW1u1du3aUcu78cYbtWzZslHfS6fTuueee3TFFVdUv/frX/9al1566ZjXQ5JuvfVWnXnmmZo+fboKhYL6+/slbdvOw8PD6u/v18jIiIrFYrVWKBTe0jL2BTTme9mGDRu0YsUKnXrqqaO+/4UvfEE333zzTm9lmTdvnubMmbPTfEZGRkadQFu2bJGknU6q9vZ2NTc3/7FXA8AfyZVXXqkrr7yy+vXw8PCoJ1qSFIvFdNVVV+nEE09UU1OTfvjDH+qhhx7S2WefXf2ZKVOmaNWqVeZycrmcGhsb/+jjB/YVzc3Nu3yBuv2ce+qpp0Y1qm926KGHavXq1dWvTzvttOr/f/3rX+vrX/969euf/vSnkqRzzz1X999/f/X725vcUqmkf/3Xf9WECRO0ePFivfTSS1q4cKEuu+wy/e///q9uvfVWLV68WK2trdVpi8WiTjnlFJVKJf3TP/2TlixZomuvvVZnnHGGjj322J3Gu+P5vnDhQvNtJdsNDAzs9PabfD6v888/f9T3zj77bN12223Vxvzpp59WPp/XwoULR/3c448/rkgkUv364osvHvUW3B0de+yx+shHPiJpW+P/kY98RIcffrieeOIJLV++XBdffLE79n0Zjfle9pOf/GTM7z17+OGHNTQ0pAsuuGCn2s9//nP9/Oc/3+n7hxxyyKivv/Od7+iLX/xivcMFsIc999xzevnll7Vs2TL97ne/08svv7zTz3z0ox8d9WvxH//4x1qwYMGoxnzNmjW8CAf2sP/4j//QOeeco3K57P5cLBbT9ddfr2eeeWbU9z/xiU/ohRdeUDKZVD6f1/Tp0/Xkk0/qiCOOqDbXbW1teuKJJ3TCCSfowQcf1MyZMyVJiURCDzzwgN7znveopaVFn/jEJ3TrrbfqmGOOGfWCo9Z7zKVtb63d/tbZ7dra2qpPprfb1RPz448/XuVyWZs2bZIkPfroozr//PPV0NAw6udOPvlk3XfffdWv3/w+9QcffFCrVq2qPoi86667dPnll0vSTu8S+NKXvqR7771Xf/Inf2Ku076KxnwvqlQquu2229TR0aHPfe5zklR9UlYsFjU8PFx9r/nw8LBOPvlk/eY3vxn166ftPvaxj+nHP/7x2zd4AHvENddco1gsphNOOEF/9Vd/pTlz5mj8+PHV+tlnn62zzjpLU6dOVSQSUU9Pj4aGhnTttddWf2bRokU7pTRccMEFOvLII3XNNde8XasC7BNef/113XLLLbrxxhsVj7/1tuiOO+6o+baNxx57bJfff+655yRJL774oo499lg9+uijmjFjxqifmTNnjn7zm9/ove99r04//XQ9/vjjmj59uiRp7ty5uvfee3X66afrsssu08jIiHp6etTR0VG9BlQqFQVBoFKpVJ3nm9fzRz/60U5vdcnlcqPeErPdJz7xiVFfp1IpbdiwobouV1111S7/diWZTOrggw/e5XaQpD/84Q/KZDLVBJZCoaBTTz21+tahHc2fP1/FYtGc176Mxnwvuuuuu/TKK6/owgsv1PTp03XDDTfo4x//uKRtf8jxve99r/qz23+VffTRR+uFF17YaV5vPul25L2nDUC4PPDAA6O+rlQqeumll6p/DLZkyRJt2rRJg4ODCoJA6XRaEydOVCwW0w9/+EN9+tOf3uV8t9+kv/nNb+6y/t///d87/eoZOBCsXbtW//AP/6C//du/HdWwbt26VRs3bqx+HYvFRv291z333KNDDz1U9957rxYsWKClS5fucv6dnZ2SpE996lPmH2guXrxYixYt2qkp/8pXvqK+vj7ddttt+q//+i+9973v1dq1a6uNuSQ9//zzWrhwob71rW/p+uuv10033aQXX3xRv/zlL0fNK5FIVP//5rfp3HXXXaPeYz5z5swxh0rMnz9f0ra31Pb29la/vvjii6vJNGPxhS98QWeeeeaoJ/uPPPLILpv53t7eMc93X0Njvpds3rxZV1xxhVpaWnTuuefqvPPOq77XqlAo6Morr9TKlSt1zjnn6KKLLpIkfeMb39DmzZt3mlcul9Ndd92lu+66a5fL8t7HBSCcTjrpJP32t78d9Z7MN6tUKvrrv/7r6lPwT37yk/rkJz+pcrms7u7umsuIRCJu7CJwINsxulj6v8SS7bY3oNK291VbT9u3v83lzW8t3W7dunX6wQ9+MOptHttVKpVqwktHR4eWLl1avSY8/vjjOu+88zQwMKAgCNTc3KxYLKaOjg6tW7duVCP+VuwqgGJXHnvsMZ122mnVB4o/+clP1NjYWP163rx51belFAqF6h9xbjeWt9qdeeaZ5hPz/RWN+V6yZs0anXfeeVqxYsVOtWw2q+bmZn3hC1/QFVdcoYsuukiFQkF33HGH/v3f/32nn+/v79ef/dmf6frrr9+p9vnPf36PjB/AnnfHHXfowgsvNOsf/vCHd/n9DRs2aMaMGZo7d67527JsNqs33njD/E0bcKB79NFHdcIJJ1S/ts6lSy65RJdcckn169NOO02LFi3S1772tTEt57rrrtO0adO0aNGinWqNjY2j3hay4wv1+fPn69JLL1VXV5eWLFmiOXPm6IYbbtDChQu1fPlyHXnkkbtc3o6/Rd/+XvSHH364+rkq11xzja655hr94z/+o5YuXWo+9Nvui1/8ojKZjK677jpFIhF9/OMfV2dn5y7/cH3Hr4eGhqrNeRAEuvrqq3XLLbdUH0A2Nzfr1Vdf1Zw5c1QqlbR+/fpqJGM2m91v/3idxnwvmT9/vo4//nidddZZO9XWr1+vk08+We9617s0efJk3Xnnnert7dWCBQtGXSS2e+2113TGGWdUf122ozf/8QWAfccVV1yhb3zjG2Z969atu0xp2u7JJ580n0o9//zz+/VTJ2B3NTY21nyquz0ycUc7xiXuKBKJKBaLjfres88+q9tuu03333//Lhv/lpYW84N+mpqa9KlPfUqVSkUrV65UW1ubvv3tb+vuu+/W5MmTzSfmV199dfW3bNvfrrJ48eJRLwDWrVuna6+9VsPDw4pGo0okEiqXy9UXBmefffaoJ9m33367Jk6cqOXLl+uEE07Qvffeq1mzZkmSVq5cOeqtN6tWrdrpLTvZbFaXXXaZ5s2bp9/85jeStj142P7wYXtC3Zv/8HR/RGO+F1m/on722Wf1sY99TJJ055136p3vfKf6+/v19NNP7/SzfX19eu2113bZsAPYt9100011PTHfbseb4ZvVSpEAUNu0adN2+VHzb45LlLall+yYypLL5fTJT35SZ555pj7wgQ9I2tYQf/GLX9SSJUuUTCaVSCSUSqW0YsWKahrLjm677bZRT46ff/55RaPRanThhg0bRr1He8cn+9K2p9bJZFJLlizRqlWrdNJJJ2ndunU6++yz9Zd/+Zc666yzdM455+i1117T/Pnz9d3vflfPPvusfve731XnsWbNGl133XX69re/ra9+9av6yle+olNPPXWnv5fxDA0Naf78+Zo3b56SyaRuv/123XzzzdV6EAQqFAo7/THqVVddpauvvnrMy9kX8BeBIfDBD35QkUhEy5cv1xtvvKHVq1dX/wjrF7/4hbq7u1Uul7V48eKdXoH/7Gc/U2trqxYsWLA3hg5gD/rzP/9zNTc3m/92zEPe0ZQpU1SpVHTzzTdrxYoV6unp0YUXXqhTTz1VPT09euihh/TVr351pyg0AG/dgw8+qCAIqv+2/yHmjt+78847d5oukUjo3HPPHdWAfvjDH1ZLS4t+//vf65lnntGf/umfas2aNZo1a5Y++tGP7vTZBMuXLx/1AnzRokXmdWFXuru7R6U+SdLll1+uiy66SBdddJEuuOACXX/99XrkkUeUyWT07ne/W1u3blVHR4ekbQly559/vs455xwdddRRkqTPfOYzWrx4sebNmzfmcaxbt27UOC655JLqp5rncjl99rOf1axZs3TSSScpk8lUv7+/NeUSjXko3H333RoaGtLQ0JAeeeQRve9971MymdRVV12lyy+/XA888ICeeuop/fznP9e8efOqr0KLxaJuvvlmXXjhhbuMUASwb7v11ls1PDxs/rM+2XPNmjV6z3veo6uuuqr6sdY7Ghoa0t13360ZM2bou9/97k7RisCBYGBgQD09PdU/ruzt7VVPT0818WN7/c3/tsf5jUVfX5+y2Wz1/ds72p5tvuOnjz733HP6i7/4Cx1yyCGaMWOGpk6dqmOOOUb33nuvli5dqqOOOkq//e1vJW37g8onnnhCxx9/fHX6E088UYlEYszn9AsvvKBDDz101Pfuv//+6ltnP/ShD1WjGL/zne8oGo1qzZo16urqkrTtqX86ndZ3v/vdUfO46KKLdsopt6xatUqFQkGTJk3aZX3p0qX6/ve/r4cffli5XE4f+tCHNDQ0NKZ574tozPeySy65RMcff7yam5vV1NSkZcuW6e///u91wQUX6Be/+IUef/xxnXHGGZo5c6aeeeYZLVy4sJqHunz5cg0NDemqq67ay2sB4O3ywgsv6Nlnn9Xq1avV3d096n2pW7du1aWXXqrDDz9cra2tev7555VIJLRixQqtW7eu+va50047Tc8995y++c1v6hvf+IZOOumkUWkJwIHg3HPP1YQJE6ovcKdMmaIJEyZUn0AvWrRIEyZM2Onf3XffPeZlfO1rX1M6ndaXvvQlzZ49u+bPv/vd79a3vvUtvfjii3ruuef0ox/9SPPmzdO5556rF198UZdddpmOOeYYSdua4k996lM67rjjqtPPnj1bjz/+ePW6MHHiREUikeq/O+64Y9TyHnroIZ188sl66aWXtGTJEkUiEQ0ODuorX/mKrr/+er33ve/VqaeeqosvvljTp0/X8uXL9eijj+qwww6TtO098I888sioTyR9sxkzZowaw5vfX/6rX/1Kxx13nIaGhqpZ5pFIRPl8Xrfccove9773afHixZo6dap++ctfasOGDTriiCO0ePHi6ouq/UqAUMnn80EQBMH69euDQqFQ8+eHh4f39JAA7AULFiwI7rzzzp2+f9NNNwXt7e1BIpEIWltbg4cffrhaK5VKweWXXx48+uij1e99+MMfDuLxeJBOp4N/+Zd/2Wl+3d3dwX333bdH1gEIs/7+/mDLli1v+V8ul6vO44EHHgg2bNgwar4LFy4MvvWtbwVBEATZbDbo7u4OVqxYEVQqlZpj2rhxY/Cxj30s6OzsDFpaWoJFixYF3d3dNaf77Gc/G1x99dXVr4vFYiApWLVqVZDNZqv/LrzwwlE/98EPfjB4+umng7vuuiuYO3ducOmll1Zrvb29waGHHhrceuutQRAEwbx584J4PB6ceOKJwcDAwE5j+O1vfxt0dXVVv85kMkFXV1ewbNmyoK+vr/pv2bJlQVdXV5DJZIIgCIKvf/3rwbXXXhs8/fTTQSqVCg466KDgscceC5qamoJjjjkm+J//+Z9RyykWi8Hf/d3fBZMmTRp1/dtfRILgTX9ODADY61asWKGOjg61t7fv7aEA2AeVSqWdstW3v8Vl+xP1crm8U1LMm39+T39AYRAEGhoa2ump+yuvvDLqbT57Y2x7A405AAAAEAL730sNAAAAYB9EYw4AAACEAI05AAAAEAI05gAAAEAIxGv/yDb/+/8eMmuJZMKslaO7/th5SQrKJbMmSSU5f5easpeZGxgxa42NKbNWKdofUZ0t5OyhxP0P90l6fzXs/e1t3Nk9zocH9A/awfut6QZ7cfaukiTly/YyG1ubzNpItmDWXu8bNGtR5yPDG2L2tpmctsciSemUPW0hb4+1VLLHc9o/nesuc3+0dUVvfRNGnAMtbH+L7o3Hubap1qfdR+qcr1fzNl3ZKdqBDP48Jcm7ZgR1jrXmQq3JnOlqHVfeMenVnLF2zOz0l7mf+uGFN5m1WXNmmbUXX1lp1o4+bJq7zN88vcysTT1654+x366jodGsdb+xzqzFnfvojCOmm7VEwjvZpLZ2ezzZin1RiW2276MNBfsY7R+xPyhpU4+dDx5t9Hue1EHtdq3R3nYx75LhFFtnHmTP0+kVg6J/XSg5fVa0yV6PqHNdOOyUQ83a/00PAAAAYK+jMQcAAABCgMYcAAAACAEacwAAACAEaMwBAACAEKAxBwAAAEJgzHGJxYodbRjk7EiZRIMdqxM48XOSFI3brxuKeWdaJ94qk7fjgcpOpFipaK9jPLAj9iSpYVyzXXQiGuNOJGAxkzFryagdydSfsWMfD+poM2uSFHX2V7zBHmtjxd6uR4xrNWvlQtGsjTQ4EZ1eNJykgrPNY842xx9J6CIR652uzsjDWsv05ltyam7kn7e83YivtC+LfiSkG0HojcdZYGw3njM5sWi+Gvv5ADT3hDlmbSRn9xHjU2mztn7lBneZTR0tZi3ZYs/39Ze7zdqEFjt2N5G0+5qYc4y2jvfvsdkhO+q56ZAOszaycotZi6acWL+SvT/yPVvNWsuMg82aJDU327HUSSdeO9Zu76tIu70/goR97me22r1SxemHJKl5+gSzVvSuGV5E9hjwxBwAAAAIARpzAAAAIARozAEAAIAQoDEHAAAAQoDGHAAAAAgBGnMAAAAgBMacDRdJOD9asmNjyk50UNSJ9ZOkaMyuB0U75qfRGWuyxY7xKYzY8XxB3I74qciPfWydYsccbe22I4kieTuGMe7ENSWclLLhnD3WUo31iFbsemHQjnlKtDtxkTl7HctOJGLCiTXMx/zos5wTEZV0ohaD3YxA2t8ETlRcxMkDDJz4zIgbo6ca6XRetF+dUYIp+xj0rntuTfLjAr3ViDnTecv04hud/VErDbDiRI96aWLxtLNd3ejCOrebF7NYa1oSEd+S5avXm7XC5iG75twLhrN2zLEk9QwOmLXx0w4ya3OPn2XWkhH7ep9zjtGOOVPMWnnrsFmTJLXY9/Vyn32PjZbt8Xix1H2b++zlOT1Gus2Op5SkmLPMoMnuweTELMZb7NhH7/qWyNv3+951PfY8JTXHuuxiwVnHujN4t6HTAAAAAEKAxhwAAAAIARpzAAAAIARozAEAAIAQoDEHAAAAQoDGHAAAAAiBMcclxgKnh086sYZOFFfRia2TpEhgx9HEnNcUsbg9nlLZiQR0oshGsjmzlkrZsUKSlHNijoKcHdEYdxK+ikU7WirZ0mTWxjtRkoG3bSRF4s42d6ItSxk76qoc2MfHQN7eNulGeyzN49JmTZKGhrNmrVKwx1PO+ZFdB5piZtCsxZN2tFU04Z8v9dsDuXbO8bnHeKtR93i8SMQ6oyQlOYlybkLjHlkPjzdQSYo483XTG8lSfLPiJjsSMepEzqba7fvWnBPe4S5z+aq1Zu33L/7BrCVOmmvWWgP7Xjns3AuS63vNWn6zHesoSVPeNdusZZdvMGtR5zgMnHOt2YkyHs7YPca65fb2lqTxU+yIyuZGJxKxzutCLGYfV2Un2rK/1z5WJWnciL2fG9L2Pa7sxFeOBU/MAQAAgBCgMQcAAABCgMYcAAAACAEacwAAACAEaMwBAACAEKAxBwAAAEKAxhwAAAAIgTHnmEedfMlKyc6/jji9f63IykjJzoKMOyG5xYKdfx2t2HnbXmZvMpWwx1Ij/7syaGegx7yFOqXAyeX1tmtQsLNJ5WSBSlLFyUqtuHnG9nQ5pxZP2XmnFScfv5Kx978kKWPvr1JgH3PRWlnIB5hkk52Bu0cyxfeGvH9uv+28k9v5HAYF3nWm/qx277MNIu4FzFumV/Pm6UxVYx3rr+4nx/kfUeeMiWYtmrCPl+zGfrNW7LE/M0GSGpP2ZyMcc9LRZq3HyRzvzdr3ygmHTzJrzz72rFk75f2nmDVJSh/cZtaKPXbmdnHtVnumcbvNS6TsLO6g197mAxs328uT1NHVbg/HOZ/Kg/ZnjES8ns/JKt+6fL1ZSzvZ+dsWai+zkLc/hyfmXBfHgk4DAAAACAEacwAAACAEaMwBAACAEKAxBwAAAEKAxhwAAAAIARpzAAAAIATGHJfohULFGu3InZITOZRwomgkKeosteLE2sW82CwvSjBpRyn2l0bMWnIwb89UUmvMnm/c2QYlJx4o76WmlewYnyBijyXmbNNtP+DUvDhNJ4Yx4mybtLOO3ivKIGPHU0pSpehtH3uh7eNb3fkeePZAVFyt5L56F5lyLnVOLKtb89S4tsmJ+6x7HZ15Rpxz24terWmPpAV60Y7umW9XyjX2o7ce7r6sP2pyf1Vw4pOHNtjxhLERu1fYkvcjcCudabPWv7LHrE2b0mnWVq2xY/Zizj1/ylQ7LnL83EPMmiRFEk6vMM6O9is5PVhlxB7r4OY+sxZzzpmOSV1mTZIUtdcj6Vyn4jl7P5fW2WPNOvf8xmb72GibPM6sSVJDk71dG1vsOOdCzomlHgOemAMAAAAhQGMOAAAAhACNOQAAABACNOYAAABACNCYAwAAACFAYw4AAACEwJjjEmNOdF+xbMfPxdJJs1YZ9iNlolEn/iqwX1P4rzacWL+iHQ/U0GCvx2Dcj3KK5Owon9aovQti6YRZSzTYtagT71WWHWVVqhGXGEvZ8UCBE4tZyds1L4ks4kQwFst2LRn1j4Bka6OzUCeis+DvZ/wR7Fb8nhNd58UT7onEO+fYleRfpOqNUnSO3aBWfGPd9tR8DbUiXS3O/Qt/XEM9g2at0mgf+NMmTTJr6RpRtYODGbP2xvp19oROrN/8ObPMWiFr39PHn3SYWUs68XuSFDgXo0Sj3YOMOPF8sYp9zozrbDNrLU7Ps3q9HXspSRXn/pwfypq1QtnuT4KE3StFmp17unPuR5woZ8m/jEfi9nzjyTG31rvEE3MAAAAgBGjMAQAAgBCgMQcAAABCgMYcAAAACAEacwAAACAEaMwBAACAEBhzpku+YEcixgM7cqgQc+L5snZsjiSlnHiceIMdO5StOLF2XuRO3olLLNjROBMa0/byJDXE7WhDORF8QckeTzThxCU68UjOblQ0ae9HScqN2BFRUScuMd1kb59ozFmmU4o6+zioEeEWKdkbYXPBPiY7Is5+xNg5x2dNTryZm3tYsK9De4WXpugevvXG/u2JTEhpj8Ql1hvtWCuisl7ecPbUZt2HlQp5szbSa8caDjr39M3Dw+4y176x3qzNPuNYszZxSpc9ntftebbNmWLW0oeMN2u1TpeKc88vOffYiBMJ6EWlbuy1oy0jKXt/tLf6PU9Tu1Of0GKWGiaNM2vFvHMNd7Zb4MUcR/xn03lnm8cz9r2oUt6Ne5x4Yg4AAACEAo05AAAAEAI05gAAAEAI0JgDAAAAIUBjDgAAAIQAjTkAAAAQAmOOSyw7kTPJuD2bhoodKZMY1+YvNGvHLnnRWPGYHQ+UmGRH9WTXOtFBWTuqJxrUiudzpnXi38qBE5foLK9YtOMAg8Aeixd7KUnxqBMX6JRiUXu0eWfTjYzY+7/RiYeKxf39USrb22Dj8IBZa++c5M4XY7UHIvb26Hzfbs56OOeSGxe4v8T67S/rsR/rWb/JrB0z/yiztjE3YtYC554mSbNPmG3Win32fDORrWZt/IJDzVrrzIPMWsSJ4CsV/Ri9/nV9Zi0+YvdgMSe+sLTVjppMJO0bd6qtyZ4uUaNXGG/3WU2HH2zWoil7PPlNQ2Yt5ownyNmRh2UnAlqSoi2NZq1Sti9GkXojX7cvd7emBgAAAPBHQWMOAAAAhACNOQAAABACNOYAAABACNCYAwAAACFAYw4AAACEwJjjEuNOPF3ZiTKqOK1/uWLH1klSMuEMr2LHDsWdhUb77cihzhldZm3LKxvs5TU4WYGSMk4kT8KJRnMjgEr2eqTiTkSlE20ZifhRToETQxmU7OignsGMWdswkjVr09pandE4Y6n4mWqJtB2BdOKEw81aecheD4ydFyQV1HxUUGck4P6SpOjx4hIj3sapM4Kxpjp3iHf+OlGogRNLG4nVuNV5yySi8S057sS5Zq1jziFmrfL6RrPW0mRfsyWpOGRH6xb77LjApiPt8TRP6jBrEacfijgHTOD0LZLU2JIya/keez08zlB18KTx9nTeWJyoQEnq6bPvlZUtduyhF9HonaPeaBLOegyss+MyJWl8px372NBo932Vgh/vWQtPzAEAAIAQoDEHAAAAQoDGHAAAAAgBGnMAAAAgBGjMAQAAgBCgMQcAAABCYMxxibGY3cNXynZMVVC2o/ui0RqvC5xorOywHY/U0GhHKxUH7elKI71mLRaxxxJN+pvx9e4tZq09ZccDTRpXX1xguWxHMnn7Me/EXkpSXHY8UDSZNGsjwyP2eLyxOjlPyYQ9llKxYNYkKZKx4ysjTjxcIjLm0wWOwMvvitrXi20Te3FjB0CunRe35sYlOtt8tyIR9wAv2tKN2HUmrBHv5h07bvydt10PUJ3HzTRrQxvseLrhdX1mbfwMO/JOkooFOz544pypZq3j+OlmrVS0j7WYH/pqVuI1eoVoiz3fnHOvrIzYfU3UOUYrTgSyd6UdHLLv6ZLUu97elyXnGtZ+SKdZy+XsfdzobNeoEzsddY4byY9e9o6P3b0q8MQcAAAACAEacwAAACAEaMwBAACAEKAxBwAAAEKAxhwAAAAIARpzAAAAIARozAEAAIAQGHMwcylv5z3GYnZOZLFkT5eM+K8LCmV72mjCHnrZyeP28iXjTkRuxcmz1Iifmz1rQpe9TCfTuezkfW4eypi1yc12NnrMySx2Ik0lSREnD7VUssfa1dxs19rtY8BLGPUyziNx/7Aulezjo1Swa0VnHfEmdQa5lmtk0Hs5/LufHvt28nK161wPN6u8vlnW5K5Gnevh5aoHznnvXcBqxpjb0wb71HG1940/fKJZW/HsG2atpavdrBVzdk63JI2bPN6sdcybbtZiKfvzMPIZe5mBkykejdv9UK3DMOLlZg9l7elqzNecp5PFndvcb9aSafuzYiSptdnOnW9tsmstU8bZyyw429zJcddq+3Nkok5/KknJBntfJhvsPiPifVbHGPDEHAAAAAgBGnMAAAAgBGjMAQAAgBCgMQcAAABCgMYcAAAACAEacwAAACAExhyXGHGiDYtOilwhZ8fRxJ3IQ0lKJuyomoozbcSJMgrKdjxQxXmd4kUFyotSlNQcs8eaiznxhUl7upGRQbNWGt9g1hoqzmuxYScvUlLFOQYisrd5JOpM50RbehlQZWc/eseqJMWdeM+cE8uVjdjTHZDcODwv8s4u1bog1Z3650Tweee2k6LnD6bWQKN1rok7oDp50V5OZGtN9cY3utM5+7HkXdtrbDcnhjOyp6Im91Orf/2qWatsGjJrGzN2BPDBp851l9k+e4pZS7anzVrgHE9eJKLbD+xGNmkhZ8fFpibZUYKFXnvbberpM2vNTfa2aWu2IxEHvaZPfgx03DnXSkP2/TfqTFdx9lXCmS5WqXFPd1azWLCvN7uZlsgTcwAAACAMaMwBAACAEKAxBwAAAEKAxhwAAAAIARpzAAAAIARozAEAAIAQGHNcYtmJACo5MVXphqQ90xqZMl7EVdmJKIxH7QichBOzWHZSjgoFO9avwYnjkfxopVRjwqx5KYyHT+o0ax3vOMisDS7vMWuRlLOvJJWGs2at6Ay2IW7PNxa390fSi7Fzjp1aYVVByYlW8iIha2wf7MDdCU5EmRNlWZMTQRjEnGtJxo50jTXY56e7km6cmuRmgXqRiG7MYI1F1qNGFKwKTr3B2Zf1jtXZj4ETfVYZsWPoJCnqRPO667FHNvq+LegfMWtNTgTwtKnTzVpyUru7zHLciXN2IptjDc79J+1c7+tPT3ZFnf4kGrcXWhi0t/maLXZc4oSi3bvNGNdi1mIFO9ZQklLeddPZV3HnPPR6sJgTyRwd76yHs90kqVyyl1nO2dPFnXUcC56YAwAAACFAYw4AAACEAI05AAAAEAI05gAAAEAI0JgDAAAAIUBjDgAAAITAmOMSFbF/NBLY0TmRih1NF3UibiSpEji5Q4E937wTq1Ny4vkisueZdGKMVLYjhySp4uUnjThRTl48X9Ze5vBrW81aYfOwWWuc1GwvT9KEyZPM2qYtA2YtN2jHLDYmGsxatOzsYydSruIcG5JUdPZXbsSOTxpfIxbzgOOdn+50TtbY7qTPOedZxIvTStnnduBcEyIV53iI1Ng23jbw7Il0Puc8qxn76KVJutPWeey4WXT28rxYPElSycvKdbaPE994oCoPZMxaS3OjWeucN92sNR5l33skKZGyD8TA7SPsUtyJS4zUiHq2+dPF4nZcYHnEiWxuS5u1Y46cadaSTnxl0YlSbPSiJCWViva1MdXZataaDhlnL9PJsy45Y5XTDwbr7ShJSRrpGTRrSSdOMr8bkZkST8wBAACAUKAxBwAAAEKAxhwAAAAIARpzAAAAIARozAEAAIAQoDEHAAAAQmDMcYlRJx6o5MRiFSp2jE3cTrHZtsyYnTkTjdqxQjknojHmRCeVnRi9StSeLlHy4/liCWfbFe1pK5WcWYs7sY+FwYI9z4S93SpJuyZJvZkhs5Z2ohZ78/Z6bNloxxUd3NJk1pJJOx6r4EWfScrm7YjKSIMd3xiL8zp27/P2rbN/nGMi4sb61RvtuIdi9OpcfT+dcDfiK924wDozw+pdD28/OnG/2+r2zciLxgvqjs3bf/XJvo/OmmtH9zXNtiMRY41eLqfc48LrXfwjtL59G3GmC2os0UtRDZwI1sDpQew7mhR14hmjTgRlyYkulPyI5EifHacZ22jHE8abnDVx1qPi7I9og39cldfa0dNe9HbjhDZ3vrXQaQAAAAAhQGMOAAAAhACNOQAAABACNOYAAABACNCYAwAAACFAYw4AAACEwJjjErMFO2IuKNtRUxUnpiyWTLrL9CIRK05aT8KJ/St68Y1OdE6sbMcRRWJ+zGDFiTKKODE/gRP7WHZqcmL9mg5qNWuFobw9T0nl3hGzVhl2oiZzdm3ihHZ7uqI9Xc45HlvTKbMmSQ0VJ6fT2Zc1UhgxVruVMLcH4un2yH7dnZnWGV9Y7yLDlvjnXdy9Z0lulGKNjROz5+tOaV+iDlhdsyabtY7jppm1WNqJrqt1bDsxyMGeOJ+8WMPdOPdjTpyx22d4q+j0CiXnPjqcs2OOR5yYZ0nKORGVEw47yB6PM9/h1b1mrexEIDem7DY3VWNXpZ3+rPe5lWatp8XuQQ490T4HtuOJOQAAABACNOYAAABACNCYAwAAACFAYw4AAACEAI05AAAAEAI05gAAAEAIjDkuMe7ESW3st2N1Ohrt2BhvnpIUcTKA3AjCRjt2KeEsspS14wKDqBOnFfc3Y8yJcqrIWQ9nniUnAilwkowqhYI9z0zWWaLU0NRg1rJOJGKkbK9/oxORVXTioUaGhs1apkaGWbwlbdZyg/Y2aGv3YxiB/xO2DEJPnblwuzmpybnWusvzImRridQ52H1pN79Njlg0z6xFnBg95zapWnmJFSdmL+Lc9CPefnePifoiEb2eRpLKBTvKN9Jox0vHD+k0a5V1W+15OtHKxSH7Xtg34PcKSScuMO6sRylixx7GnOm2rthk1qKtjWatodmuSVKlZPcSnePbzNqW7i3ufGvhiTkAAAAQAjTmAAAAQAjQmAMAAAAhQGMOAAAAhACNOQAAABACNOYAAABACNCYAwAAACEw5hzzkpN/7cZ9OrmlIzl7npKUTtm5lWUvszbmZZM6kzlFL381V7TzzyUpVXJe/0TsPNSIk/Pe2GhniueLdhZoxYlfbT6kwy5KquScrNh+O9e0EjjbbmTErLWMs3NCvczispPNKknJpH3YRwv2MVchs3j/VF8kMRnW0p7ZBt5FyrUbg6l7kTzb2omXVV7vPGts52hDffnoeyKG35vpyFb78zckuY9KSxn782JijfY9reLcKyMR+7NC8s5nxQQlO29dktqT9ueTVPoyZi3pZI7HWp17s9NLZvJONnreXw/nUFbU2c/tbU3ufGvhqgIAAACEAI05AAAAEAI05gAAAEAI0JgDAAAAIUBjDgAAAIQAjTkAAAAQAmOOS+zP2LF2ESfKKO/E6iRqhCcFTqxdxYkEjDfYUT2loh0B1NRsR/UUy/Z6RCr+6xsnLVCxUtmezokNKwd2dFLUyfjJj9jRjm1HTTBrkpTrtSMRU3l7PSpOJFE0Ye+rVzesN2uJkr2OE9udmEVJhax97Hg7KxEb8+lyYHAjzJxzu87JJO1GlJ7DzUXbC9F9Xv6sNxwvFy7uZtrapbITS1trPN4i3+7NWmt5eyIb7wDlHYZutLKbclyjV6gvIdmfp3PsFzP2fTSesu9p2Z5Bd5nevbLBOU9LGwfsmRade7MTazg8ZMcaJtJ2XLMklaL2WDc/v9qsBS12DzYybPcfmc39Zm3Y6bEq5XFmTZI6Z002axHnQC/01ojFrIEn5gAAAEAI0JgDAAAAIUBjDgAAAIQAjTkAAAAQAjTmAAAAQAjQmAMAAAAhMOb8t4aIHR004sQKNafs+JuSE6UoSRkn5ifihCBFnem81KVc1o5AKhXsscZj/uubsjdWd0InrsmpxRvs3Rp3oh2H1vpRTomcE9+Ys7ddMuHFv9nrkXciGJPOoZuIxezlSerpt6OMGp34qIidHnWAqi8qLih50/nxfF40a5jy+YIajzwi7nC8bVBfLlxQsefpb9Ma263eaMc9EU/oxMTW5MVw7onDaj/mRiK6Ezq1mtu5vvPCi7zzjqfCoB1XXHHieNun+5HExQF7vrGYPZ7BlZvtmTrRhVFnZyWcTRNrTtlFSUNOLHPfoH3/nTFroj3TXItZSqTs+MbB1+3Y5fHzptnLk9Q4qcMezrC9jrlMwZ1vLTwxBwAAAEKAxhwAAAAIARpzAAAAIARozAEAAIAQoDEHAAAAQoDGHAAAAAiBMccllpwefk2vHdUzrin91ka0g8a4vcyCF/9VZ+5S2YlE9F7BRL3IJUkRJ3YpcMYacaKMgrI91krRjuqJRe04QA36ET/DA1mz1pC244q8VMzGZnu6OZOmm7Xs6j6zls/ZkVOSVHGiHeNO7FLZOeYOSDWOe4sbFVgj6tKLNwtVcl2tzDjvWHLW0b+yOdM5kWn17seadiO9sL7l7cY6vt1jPUC5e8FLrKw7g7HmjM1SOWvfD6MJ+zoVTyfNWnHIvzfJ6UG88cSc9ag4659w1qO9yY5E7M/46xFvtKdNp+1aKW+vf9vBbWat3GLftxvy9nZLdzSbNck/XpNN9jLj4+rveyWemAMAAAChQGMOAAAAhACNOQAAABACNOYAAABACNCYAwAAACFAYw4AAACEwJjjEstRO1Zncsd4szYwYkfTNSSd6D5JibgdKVZwYoW2DtvxOPGE/VokmbQ3R1PSjkCSF0UmqVQpm7VUgx25U8jZ8YTlvL1dI3JifKL2WIq9w2Zt23ydaEcvErLJ3j75gr2vihuddfTiswI/WquppcWe1AlIilbIVBvN2wdeRJkzyxqxdn7Vi7N8e/ddpFwrWtMZjxeT6s7SuQ5V6o1EDNsx7x1zuzFb77iLedt1N5a5v6ovnXA3IxGd+XpFZ6xRZ783ddn3kErBvscOvLrBG40SObuvGe7eYtZy67eatfZxrWZt06qNZm35691mbcq0KWZNkqLN9lafdOxMs5bpz5i1QsbuB/KDdq/U12/3NekN/WZNktKH2L2td03tX2fvq7HgiTkAAAAQAjTmAAAAQAjQmAMAAAAhQGMOAAAAhACNOQAAABACNOYAAABACIw5LrG1OW3WkpVGs5bL2jE2xRrxSN39g2YtM2LPtzFhxwVOSNnRQa1N9nqoZMcYJaJ+TlcibsdC5gN7vpWWlFkr5p1ISCfGJ+ZEqtUMeEvZ2zWStg+ldIc9XSJqb5vBNQNmra/frjU1+DGcyUY7+jIzbMc1NYYuOi7E9sqmqnOhbrRjvSsSsmPFu0S5GXa15uvMuO7ITKfm7g9vLM5kNefrTVffZPuzwNnvpUzRmdC+AyWce6FUK2qxvmjSaIPTHjk3y/xW+x7SMtWJ35M09Owqs9ZQsrdrQ0uTWfOijLeu7zVrI/1D9vIOs+OzJSnSYveLUSeWOuHUep5fadYKTix30omuzTsxk5LUv8aOPYzk7B6sMDDizrcWnpgDAAAAIUBjDgAAAIQAjTkAAAAQAjTmAAAAQAjQmAMAAAAhQGMOAAAAhMCY4xKDUtmsFSt2HE3RS7AqOdFJkprTdszeZifKpyXlxDc6y0w7kYippB2xFzhxPJIfbTiQsWN1Im32ejQ541HMjjIaKdljjdU4GkpFe/sMr7PXY2SFvc07J9jxlUHEXo941H5NWXZqklQp2+vhRbwN5/3j9cBTK4PubVZvPJ8TIVr3PPeGute/3uzCGt72CMLdiH107mFujmzIToEw8KILCwN2lGB2ox2B2zFnirvM/JB9/0mNazFr0bh97nunU8W5F5aG7ei+eMqP8s0O2+uRLts9WDnt9ANJe5nj2u1ts25zj1nbsMGuSdL4tB1vmXd6yZizH0sDw2atodFe3vCgfcyVa1wXGpxtVyrYx8D4SZ3+jGvgiTkAAAAQAjTmAAAAQAjQmAMAAAAhQGMOAAAAhACNOQAAABACNOYAAABACNCYAwAAACEw5hxzL5u0WLEDP1tSdr5mtmDnWUrS+HTarOXH28t0oknV1mhngyejdm52xctqr5FjnnBytZNOrb/Pzt9Mpe31SESdfeVkiEZkr78kxQI7tzPh5KOPi9pZoNFBO+M9ErcPzw4nq71cI1s47sw36eSvZvNO/jn2Pi832zkn5Fy/XG5ueK3gbG+Zb3NAeuBdh/dCWHvd2eDedXgPrUfYsuxDwDv0k85nc1ScXOiRHvtzSyRpYO0Ws3bw8U1mLRLU92yyOJQza7m1W81adNCeTpISI/Y2iDTYvVT38pVmrW3SBHs8Dfa9efq8w81aQ7RG6+jkvFc29Zu1ZMw++WMFu1eY2NVh1oZj9j7uGfb3x6ZN9nE1YYK9zPYWu48YC56YAwAAACFAYw4AAACEAI05AAAAEAI05gAAAEAI0JgDAAAAIUBjDgAAAITAmOMSvbjAxqQduZN0spNKNZaezdtRNh1NduzScKFo1qJxO2Yvk7ena07bUUWRpP/6pjg0YtZanHi+uBMnWSo60WBOXmQ61WBPV/HjK71UyIakvX28WLm4E3sYdaIk5UxXcfa/JAVOnmLCiaSKko02SlBxor1izslddxzebqg3EnFP7XM3LfHtjlL05lljuwXOtPUOdY9s8t056EIUbbmPSzQ597tG+95Udu7NktQ8sd2sRZy4PP+wsIuBczMMnPt2raMl4sW6Ju1r6iHTJpm1aMRe/0hzsz2ds82jNc6nBieGMTOUNWurNm4wa0fOnGbW0k4sd9zZ/+lGp2+RlEnZ6xFvsHuQvg2b3fnWwhNzAAAAIARozAEAAIAQoDEHAAAAQoDGHAAAAAgBGnMAAAAgBGjMAQAAgBAYc1xiQ8qOOUp6MXtOVF7KiS6UpFJgRxKVSnZtwIknLOTzZm3IqR3cbscKTejqNGuSlHdi/+JOOlKzEx+VL9pRdYrY+yNI2GMpjviRVG6UU9GeNpawD7OSE12Ydg6PvHNcuTGLktyjzjnmUs32OXAgCpzzJdI45kvL3ldnZNpuJeU5MbL7lLojEeuc0Dnvd0u949lf9uMflb1NIl4UqHN/SaSdmF9J3nnqLdJJAFbEWY/UuCazVh5v9wrZFX6MXs/GHrN28KypZi1wYn7l9EpB2e4Vmp3baHbYjjyUpKFNvWYtNq7FrM2ebq9jo9dHeHGaTtR3S7Mduy1JhYo93+Q4ez83ZwrufGvhiTkAAAAQAjTmAAAAQAjQmAMAAAAhQGMOAAAAhACNOQAAABACNOYAAABACESCYE/lTwEAAAAYK56YAwAAACFAYw4AAACEAI05AAAAEAI05gAAAEAI0JgDAAAAIUBjDgAAAIQAjTkAAAAQAjTmAAAAQAjQmAMAAAAh8P8BtTgxSCrUBQ8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x800 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3×3のグリッドで画像を表示\n",
    "fig, axes = plt.subplots(3,3,figsize=(8,8))\n",
    "fig.suptitle(\"クラスごとのサンプル画像\", fontsize=16)\n",
    "\n",
    "# 各クラスの画像をプロット\n",
    "# axes.rabel()で 9 個のプロット領域を平坦化し、ループで各画像を埋め込む\n",
    "for ax, (class_id, image) in zip(axes.ravel(), class_images.items()):\n",
    "    # 画像をプロット\n",
    "    ax.imshow(image.permute(1,2,0).numpy()) # matplotlib の imshow が期待する形式に変換（permuteで次元を並び替え）\n",
    "    ax.set_title(class_labels[class_id])\n",
    "    ax.axis(\"off\")\n",
    "plt.tight_layout(rect=[0,0,1,0.95])\n",
    "# tight_layout(): プロット内の要素が重ならないように自動調整するもの。\n",
    "# rect = [左端の割合, 下端の割合, 右端の割合, 上端の割合]で、プロット全体の表示領域を図の左端・下端から右端・上端まで調整し、上端に余白（5%分）を確保する設定\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bd72ea-b73c-4d4d-acb3-02bbbec4f3c0",
   "metadata": {},
   "source": [
    "## 2blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "931c0533-865f-4109-b919-6735c6407587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5aae9c43-f59a-4509-941c-34ec54b23339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU が利用可能であるか: True\n"
     ]
    }
   ],
   "source": [
    "# 使用するデバイスを決定\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# GPUの利用について出力\n",
    "print(f\"GPU が利用可能であるか: {str(torch.cuda.is_available())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "57f29743-e7b1-4fb2-b50d-7ddd4b9e29bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2.4.1+cu121\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62a24dfa-c1c0-4454-b9b1-b103c4440819",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "adIKdfmGcw3M",
    "outputId": "a18d55eb-f0ae-48c3-d041-cab4acd6636c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataloader: 704 batches of 128\n",
      "Length of test dataloader: 57 batches of 128\n",
      "Length of val dataloader: 79 batches of 128\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# データローダーに変換\n",
    "BATCH_SIZE = 128\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "# 形状の確認\n",
    "print(f\"Length of train dataloader: {len(train_loader)} batches of {BATCH_SIZE}\")\n",
    "print(f\"Length of test dataloader: {len(test_loader)} batches of {BATCH_SIZE}\")\n",
    "print(f\"Length of val dataloader: {len(val_loader)} batches of {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4cb2efc-e8e6-4994-a844-ee0d2ffe92ed",
   "metadata": {
    "id": "7G4PwDd7hrd8"
   },
   "outputs": [],
   "source": [
    "# 乱数シード設定\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "class SmallVGG_2blocks(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, conv_dropout_rate, linear_dropout_rate, batch_norm=True):\n",
    "        super(SmallVGG_2blocks, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # ブロック1\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32) if batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(conv_dropout_rate) if conv_dropout_rate else nn.Identity(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32) if batch_norm else nn.Identity(),            \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 28x28 -> 14x14\n",
    "            nn.Dropout(conv_dropout_rate) if conv_dropout_rate else nn.Identity(),            \n",
    "\n",
    "            # ブロック2\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64) if batch_norm else nn.Identity(),            \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(conv_dropout_rate) if conv_dropout_rate else nn.Identity(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64) if batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 14x14 -> 7x7            \n",
    "            nn.Dropout(conv_dropout_rate) if conv_dropout_rate else nn.Identity()\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64 * 7 * 7, 256),  # Flattenした特徴量を全結合層に入力\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(linear_dropout_rate),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # フラット化\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81ec53d5-afb0-43b6-96ed-4b1c42aa1247",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_url = \"postgresql+psycopg2://optuna_user:optuna_password@portfolio3-postgres:5432/optuna_db\"\n",
    "study_name = \"250129_PathMNIST_2blocks\" # 変更要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "562ce44e-262e-4436-b380-acce1fae8964",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "VHCSdyFucw0a",
    "outputId": "70fe6df8-d4bf-4641-8198-d7a5c5b05f71"
   },
   "outputs": [],
   "source": [
    "# ログを出力しない\n",
    "# optuna.logging.disable_default_handler()\n",
    "\n",
    "def objective(trial):\n",
    "    set_seed(42)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # GPUが利用可能なら使用\n",
    "\n",
    "    # ハイパーパラメータのサンプリング\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2)\n",
    "    conv_dropout_rate = trial.suggest_float('conv_dropout_rate', 0.0, 0.3)\n",
    "    linear_dropout_rate = trial.suggest_float('linear_dropout_rate', 0.0, 0.5)\n",
    "    batch_norm = trial.suggest_categorical('batch_norm', [True, False])\n",
    "    # モデル定義\n",
    "    model = SmallVGG_2blocks(\n",
    "        in_channels=3,\n",
    "        num_classes=9,\n",
    "        conv_dropout_rate=conv_dropout_rate,\n",
    "        linear_dropout_rate=linear_dropout_rate,\n",
    "        batch_norm=batch_norm,\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate,  momentum=0.9)\n",
    "    scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "    # モデル保存用ディレクトリを作成（ローカル環境でも保存）\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_epoch = 0\n",
    "    best_acc = 0.0  # 最良の検証精度\n",
    "    #best_model_state = None  # モデルの状態を保存するための変数\n",
    "\n",
    "    for epoch in range(50):\n",
    "        # 訓練モード\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "            labels = labels.squeeze().long()  # ラベルを1Dに変換し、整数型にキャスト\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_acc = train_correct / len(train_loader.dataset)\n",
    "\n",
    "        # 検証モード\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in val_loader:\n",
    "                imgs, labels = imgs.to(device), labels.to(device)\n",
    "                labels = labels.squeeze().long()  # ラベルを1Dに変換\n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                val_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc = val_correct / len(val_loader.dataset)\n",
    "\n",
    "        # ベストモデルを記録\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch\n",
    "            best_acc = val_acc\n",
    "            \n",
    "            # トライアル番号を使ったモデル保存\n",
    "            model_path = f\"models/trial_{study_name}_{trial.number}.pth\"\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "            # モデルパスや関連情報をトライアルに記録\n",
    "            trial.set_user_attr(\"model_path\", model_path)\n",
    "            trial.set_user_attr(\"best_val_loss\", best_val_loss)\n",
    "            trial.set_user_attr(\"best_epoch\", best_epoch)\n",
    "\n",
    "        # 学習率スケジューラを更新\n",
    "        scheduler.step()\n",
    "\n",
    "        # 進捗報告（トライアルの進行状況を追跡し、プルーニングするかどうかを判断）\n",
    "        trial.report(val_loss, epoch) # スカラー値のみ\n",
    "\n",
    "        # プルーニング判定（現在のトライアルが有望でないと判断した場合、トライアルを途中で終了）\n",
    "        if trial.should_prune():\n",
    "          raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        print(f\"Epoch {epoch} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5cd2038c-bb1c-4203-8737-a954e97f53db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 00:58:51,092] Using an existing study with name '250129_PathMNIST_2blocks' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Train Loss: 0.0110, Train Acc: 0.4551, Val Loss: 0.0084, Val Acc: 0.5828\n",
      "Epoch 1 - Train Loss: 0.0068, Train Acc: 0.6656, Val Loss: 0.0059, Val Acc: 0.7069\n",
      "Epoch 2 - Train Loss: 0.0052, Train Acc: 0.7546, Val Loss: 0.0055, Val Acc: 0.7487\n",
      "Epoch 3 - Train Loss: 0.0042, Train Acc: 0.8038, Val Loss: 0.0036, Val Acc: 0.8321\n",
      "Epoch 4 - Train Loss: 0.0036, Train Acc: 0.8301, Val Loss: 0.0035, Val Acc: 0.8358\n",
      "Epoch 5 - Train Loss: 0.0029, Train Acc: 0.8599, Val Loss: 0.0027, Val Acc: 0.8700\n",
      "Epoch 6 - Train Loss: 0.0027, Train Acc: 0.8718, Val Loss: 0.0027, Val Acc: 0.8732\n",
      "Epoch 7 - Train Loss: 0.0025, Train Acc: 0.8792, Val Loss: 0.0027, Val Acc: 0.8782\n",
      "Epoch 8 - Train Loss: 0.0024, Train Acc: 0.8856, Val Loss: 0.0023, Val Acc: 0.8958\n",
      "Epoch 9 - Train Loss: 0.0023, Train Acc: 0.8941, Val Loss: 0.0023, Val Acc: 0.8896\n",
      "Epoch 10 - Train Loss: 0.0020, Train Acc: 0.9084, Val Loss: 0.0020, Val Acc: 0.9139\n",
      "Epoch 11 - Train Loss: 0.0019, Train Acc: 0.9111, Val Loss: 0.0019, Val Acc: 0.9192\n",
      "Epoch 12 - Train Loss: 0.0018, Train Acc: 0.9146, Val Loss: 0.0017, Val Acc: 0.9242\n",
      "Epoch 13 - Train Loss: 0.0018, Train Acc: 0.9172, Val Loss: 0.0018, Val Acc: 0.9179\n",
      "Epoch 14 - Train Loss: 0.0017, Train Acc: 0.9201, Val Loss: 0.0017, Val Acc: 0.9247\n",
      "Epoch 15 - Train Loss: 0.0016, Train Acc: 0.9270, Val Loss: 0.0017, Val Acc: 0.9261\n",
      "Epoch 16 - Train Loss: 0.0016, Train Acc: 0.9273, Val Loss: 0.0016, Val Acc: 0.9316\n",
      "Epoch 17 - Train Loss: 0.0015, Train Acc: 0.9290, Val Loss: 0.0016, Val Acc: 0.9307\n",
      "Epoch 18 - Train Loss: 0.0015, Train Acc: 0.9305, Val Loss: 0.0016, Val Acc: 0.9330\n",
      "Epoch 19 - Train Loss: 0.0015, Train Acc: 0.9332, Val Loss: 0.0014, Val Acc: 0.9376\n",
      "Epoch 20 - Train Loss: 0.0014, Train Acc: 0.9354, Val Loss: 0.0015, Val Acc: 0.9340\n",
      "Epoch 21 - Train Loss: 0.0014, Train Acc: 0.9350, Val Loss: 0.0015, Val Acc: 0.9356\n",
      "Epoch 22 - Train Loss: 0.0014, Train Acc: 0.9358, Val Loss: 0.0015, Val Acc: 0.9372\n",
      "Epoch 23 - Train Loss: 0.0014, Train Acc: 0.9363, Val Loss: 0.0015, Val Acc: 0.9315\n",
      "Epoch 24 - Train Loss: 0.0014, Train Acc: 0.9374, Val Loss: 0.0014, Val Acc: 0.9401\n",
      "Epoch 25 - Train Loss: 0.0013, Train Acc: 0.9389, Val Loss: 0.0014, Val Acc: 0.9381\n",
      "Epoch 26 - Train Loss: 0.0013, Train Acc: 0.9395, Val Loss: 0.0015, Val Acc: 0.9364\n",
      "Epoch 27 - Train Loss: 0.0013, Train Acc: 0.9392, Val Loss: 0.0014, Val Acc: 0.9387\n",
      "Epoch 28 - Train Loss: 0.0013, Train Acc: 0.9406, Val Loss: 0.0014, Val Acc: 0.9393\n",
      "Epoch 29 - Train Loss: 0.0013, Train Acc: 0.9399, Val Loss: 0.0015, Val Acc: 0.9364\n",
      "Epoch 30 - Train Loss: 0.0013, Train Acc: 0.9405, Val Loss: 0.0014, Val Acc: 0.9392\n",
      "Epoch 31 - Train Loss: 0.0013, Train Acc: 0.9408, Val Loss: 0.0014, Val Acc: 0.9384\n",
      "Epoch 32 - Train Loss: 0.0013, Train Acc: 0.9404, Val Loss: 0.0014, Val Acc: 0.9394\n",
      "Epoch 33 - Train Loss: 0.0013, Train Acc: 0.9417, Val Loss: 0.0014, Val Acc: 0.9398\n",
      "Epoch 34 - Train Loss: 0.0013, Train Acc: 0.9405, Val Loss: 0.0014, Val Acc: 0.9383\n",
      "Epoch 35 - Train Loss: 0.0013, Train Acc: 0.9418, Val Loss: 0.0014, Val Acc: 0.9392\n",
      "Epoch 36 - Train Loss: 0.0013, Train Acc: 0.9413, Val Loss: 0.0014, Val Acc: 0.9394\n",
      "Epoch 37 - Train Loss: 0.0013, Train Acc: 0.9415, Val Loss: 0.0014, Val Acc: 0.9390\n",
      "Epoch 38 - Train Loss: 0.0013, Train Acc: 0.9422, Val Loss: 0.0014, Val Acc: 0.9385\n",
      "Epoch 39 - Train Loss: 0.0013, Train Acc: 0.9405, Val Loss: 0.0014, Val Acc: 0.9394\n",
      "Epoch 40 - Train Loss: 0.0013, Train Acc: 0.9411, Val Loss: 0.0014, Val Acc: 0.9389\n",
      "Epoch 41 - Train Loss: 0.0013, Train Acc: 0.9419, Val Loss: 0.0014, Val Acc: 0.9400\n",
      "Epoch 42 - Train Loss: 0.0013, Train Acc: 0.9425, Val Loss: 0.0014, Val Acc: 0.9392\n",
      "Epoch 43 - Train Loss: 0.0013, Train Acc: 0.9427, Val Loss: 0.0014, Val Acc: 0.9398\n",
      "Epoch 44 - Train Loss: 0.0013, Train Acc: 0.9421, Val Loss: 0.0014, Val Acc: 0.9396\n",
      "Epoch 45 - Train Loss: 0.0013, Train Acc: 0.9419, Val Loss: 0.0014, Val Acc: 0.9390\n",
      "Epoch 46 - Train Loss: 0.0013, Train Acc: 0.9415, Val Loss: 0.0014, Val Acc: 0.9385\n",
      "Epoch 47 - Train Loss: 0.0013, Train Acc: 0.9425, Val Loss: 0.0014, Val Acc: 0.9396\n",
      "Epoch 48 - Train Loss: 0.0013, Train Acc: 0.9422, Val Loss: 0.0014, Val Acc: 0.9399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 01:06:50,116] Trial 1 finished with value: 0.001391120246801077 and parameters: {'learning_rate': 0.00800303742246812, 'conv_dropout_rate': 0.19435851275733987, 'linear_dropout_rate': 0.12898559210060273, 'batch_norm': False}. Best is trial 1 with value: 0.001391120246801077.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Train Loss: 0.0013, Train Acc: 0.9420, Val Loss: 0.0014, Val Acc: 0.9395\n",
      "Epoch 0 - Train Loss: 0.0106, Train Acc: 0.4766, Val Loss: 0.0092, Val Acc: 0.5582\n",
      "Epoch 1 - Train Loss: 0.0063, Train Acc: 0.6901, Val Loss: 0.0053, Val Acc: 0.7642\n",
      "Epoch 2 - Train Loss: 0.0046, Train Acc: 0.7834, Val Loss: 0.0048, Val Acc: 0.7789\n",
      "Epoch 3 - Train Loss: 0.0036, Train Acc: 0.8293, Val Loss: 0.0040, Val Acc: 0.8180\n",
      "Epoch 4 - Train Loss: 0.0031, Train Acc: 0.8566, Val Loss: 0.0029, Val Acc: 0.8646\n",
      "Epoch 5 - Train Loss: 0.0022, Train Acc: 0.8966, Val Loss: 0.0020, Val Acc: 0.9075\n",
      "Epoch 6 - Train Loss: 0.0020, Train Acc: 0.9089, Val Loss: 0.0022, Val Acc: 0.9007\n",
      "Epoch 7 - Train Loss: 0.0018, Train Acc: 0.9162, Val Loss: 0.0020, Val Acc: 0.9096\n",
      "Epoch 8 - Train Loss: 0.0017, Train Acc: 0.9236, Val Loss: 0.0017, Val Acc: 0.9228\n",
      "Epoch 9 - Train Loss: 0.0015, Train Acc: 0.9311, Val Loss: 0.0017, Val Acc: 0.9213\n",
      "Epoch 10 - Train Loss: 0.0012, Train Acc: 0.9442, Val Loss: 0.0014, Val Acc: 0.9369\n",
      "Epoch 11 - Train Loss: 0.0012, Train Acc: 0.9457, Val Loss: 0.0014, Val Acc: 0.9384\n",
      "Epoch 12 - Train Loss: 0.0011, Train Acc: 0.9477, Val Loss: 0.0014, Val Acc: 0.9369\n",
      "Epoch 13 - Train Loss: 0.0011, Train Acc: 0.9511, Val Loss: 0.0013, Val Acc: 0.9448\n",
      "Epoch 14 - Train Loss: 0.0010, Train Acc: 0.9520, Val Loss: 0.0014, Val Acc: 0.9412\n",
      "Epoch 15 - Train Loss: 0.0009, Train Acc: 0.9602, Val Loss: 0.0012, Val Acc: 0.9448\n",
      "Epoch 16 - Train Loss: 0.0009, Train Acc: 0.9610, Val Loss: 0.0012, Val Acc: 0.9504\n",
      "Epoch 17 - Train Loss: 0.0008, Train Acc: 0.9621, Val Loss: 0.0012, Val Acc: 0.9457\n",
      "Epoch 18 - Train Loss: 0.0008, Train Acc: 0.9632, Val Loss: 0.0013, Val Acc: 0.9429\n",
      "Epoch 19 - Train Loss: 0.0008, Train Acc: 0.9644, Val Loss: 0.0012, Val Acc: 0.9508\n",
      "Epoch 20 - Train Loss: 0.0007, Train Acc: 0.9679, Val Loss: 0.0012, Val Acc: 0.9502\n",
      "Epoch 21 - Train Loss: 0.0007, Train Acc: 0.9674, Val Loss: 0.0011, Val Acc: 0.9527\n",
      "Epoch 22 - Train Loss: 0.0007, Train Acc: 0.9683, Val Loss: 0.0011, Val Acc: 0.9530\n",
      "Epoch 23 - Train Loss: 0.0007, Train Acc: 0.9693, Val Loss: 0.0011, Val Acc: 0.9525\n",
      "Epoch 24 - Train Loss: 0.0007, Train Acc: 0.9695, Val Loss: 0.0011, Val Acc: 0.9530\n",
      "Epoch 25 - Train Loss: 0.0007, Train Acc: 0.9713, Val Loss: 0.0011, Val Acc: 0.9528\n",
      "Epoch 26 - Train Loss: 0.0007, Train Acc: 0.9710, Val Loss: 0.0011, Val Acc: 0.9535\n",
      "Epoch 27 - Train Loss: 0.0006, Train Acc: 0.9711, Val Loss: 0.0011, Val Acc: 0.9547\n",
      "Epoch 28 - Train Loss: 0.0006, Train Acc: 0.9719, Val Loss: 0.0011, Val Acc: 0.9537\n",
      "Epoch 29 - Train Loss: 0.0006, Train Acc: 0.9722, Val Loss: 0.0011, Val Acc: 0.9537\n",
      "Epoch 30 - Train Loss: 0.0006, Train Acc: 0.9728, Val Loss: 0.0011, Val Acc: 0.9555\n",
      "Epoch 31 - Train Loss: 0.0006, Train Acc: 0.9727, Val Loss: 0.0011, Val Acc: 0.9547\n",
      "Epoch 32 - Train Loss: 0.0006, Train Acc: 0.9732, Val Loss: 0.0011, Val Acc: 0.9539\n",
      "Epoch 33 - Train Loss: 0.0006, Train Acc: 0.9736, Val Loss: 0.0011, Val Acc: 0.9538\n",
      "Epoch 34 - Train Loss: 0.0006, Train Acc: 0.9734, Val Loss: 0.0011, Val Acc: 0.9546\n",
      "Epoch 35 - Train Loss: 0.0006, Train Acc: 0.9740, Val Loss: 0.0011, Val Acc: 0.9544\n",
      "Epoch 36 - Train Loss: 0.0006, Train Acc: 0.9742, Val Loss: 0.0011, Val Acc: 0.9536\n",
      "Epoch 37 - Train Loss: 0.0006, Train Acc: 0.9740, Val Loss: 0.0011, Val Acc: 0.9547\n",
      "Epoch 38 - Train Loss: 0.0006, Train Acc: 0.9743, Val Loss: 0.0011, Val Acc: 0.9547\n",
      "Epoch 39 - Train Loss: 0.0006, Train Acc: 0.9747, Val Loss: 0.0011, Val Acc: 0.9551\n",
      "Epoch 40 - Train Loss: 0.0006, Train Acc: 0.9742, Val Loss: 0.0011, Val Acc: 0.9551\n",
      "Epoch 41 - Train Loss: 0.0006, Train Acc: 0.9741, Val Loss: 0.0011, Val Acc: 0.9560\n",
      "Epoch 42 - Train Loss: 0.0006, Train Acc: 0.9745, Val Loss: 0.0011, Val Acc: 0.9551\n",
      "Epoch 43 - Train Loss: 0.0006, Train Acc: 0.9748, Val Loss: 0.0011, Val Acc: 0.9552\n",
      "Epoch 44 - Train Loss: 0.0006, Train Acc: 0.9749, Val Loss: 0.0011, Val Acc: 0.9552\n",
      "Epoch 45 - Train Loss: 0.0006, Train Acc: 0.9747, Val Loss: 0.0011, Val Acc: 0.9552\n",
      "Epoch 46 - Train Loss: 0.0006, Train Acc: 0.9743, Val Loss: 0.0011, Val Acc: 0.9553\n",
      "Epoch 47 - Train Loss: 0.0006, Train Acc: 0.9748, Val Loss: 0.0011, Val Acc: 0.9554\n",
      "Epoch 48 - Train Loss: 0.0006, Train Acc: 0.9746, Val Loss: 0.0011, Val Acc: 0.9546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 01:14:40,970] Trial 2 finished with value: 0.0010662422513962387 and parameters: {'learning_rate': 0.009348015240233945, 'conv_dropout_rate': 0.05635240330691979, 'linear_dropout_rate': 0.010912049033665994, 'batch_norm': False}. Best is trial 2 with value: 0.0010662422513962387.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Train Loss: 0.0006, Train Acc: 0.9748, Val Loss: 0.0011, Val Acc: 0.9555\n",
      "Epoch 0 - Train Loss: 0.0060, Train Acc: 0.7165, Val Loss: 0.0049, Val Acc: 0.7757\n",
      "Epoch 1 - Train Loss: 0.0036, Train Acc: 0.8341, Val Loss: 0.0058, Val Acc: 0.7496\n",
      "Epoch 2 - Train Loss: 0.0028, Train Acc: 0.8717, Val Loss: 0.0055, Val Acc: 0.7628\n",
      "Epoch 3 - Train Loss: 0.0023, Train Acc: 0.8962, Val Loss: 0.0051, Val Acc: 0.8143\n",
      "Epoch 4 - Train Loss: 0.0020, Train Acc: 0.9120, Val Loss: 0.0017, Val Acc: 0.9288\n",
      "Epoch 5 - Train Loss: 0.0015, Train Acc: 0.9322, Val Loss: 0.0013, Val Acc: 0.9426\n",
      "Epoch 6 - Train Loss: 0.0014, Train Acc: 0.9371, Val Loss: 0.0011, Val Acc: 0.9520\n",
      "Epoch 7 - Train Loss: 0.0013, Train Acc: 0.9410, Val Loss: 0.0012, Val Acc: 0.9458\n",
      "Epoch 8 - Train Loss: 0.0013, Train Acc: 0.9441, Val Loss: 0.0011, Val Acc: 0.9543\n",
      "Epoch 9 - Train Loss: 0.0012, Train Acc: 0.9458, Val Loss: 0.0015, Val Acc: 0.9379\n",
      "Epoch 10 - Train Loss: 0.0010, Train Acc: 0.9535, Val Loss: 0.0008, Val Acc: 0.9676\n",
      "Epoch 11 - Train Loss: 0.0010, Train Acc: 0.9556, Val Loss: 0.0010, Val Acc: 0.9562\n",
      "Epoch 12 - Train Loss: 0.0010, Train Acc: 0.9558, Val Loss: 0.0010, Val Acc: 0.9605\n",
      "Epoch 13 - Train Loss: 0.0010, Train Acc: 0.9568, Val Loss: 0.0008, Val Acc: 0.9686\n",
      "Epoch 14 - Train Loss: 0.0009, Train Acc: 0.9584, Val Loss: 0.0009, Val Acc: 0.9635\n",
      "Epoch 15 - Train Loss: 0.0009, Train Acc: 0.9611, Val Loss: 0.0008, Val Acc: 0.9690\n",
      "Epoch 16 - Train Loss: 0.0009, Train Acc: 0.9615, Val Loss: 0.0008, Val Acc: 0.9688\n",
      "Epoch 17 - Train Loss: 0.0008, Train Acc: 0.9633, Val Loss: 0.0008, Val Acc: 0.9680\n",
      "Epoch 18 - Train Loss: 0.0008, Train Acc: 0.9633, Val Loss: 0.0008, Val Acc: 0.9679\n",
      "Epoch 19 - Train Loss: 0.0008, Train Acc: 0.9635, Val Loss: 0.0008, Val Acc: 0.9659\n",
      "Epoch 20 - Train Loss: 0.0008, Train Acc: 0.9647, Val Loss: 0.0007, Val Acc: 0.9713\n",
      "Epoch 21 - Train Loss: 0.0008, Train Acc: 0.9650, Val Loss: 0.0007, Val Acc: 0.9721\n",
      "Epoch 22 - Train Loss: 0.0008, Train Acc: 0.9657, Val Loss: 0.0007, Val Acc: 0.9720\n",
      "Epoch 23 - Train Loss: 0.0008, Train Acc: 0.9662, Val Loss: 0.0006, Val Acc: 0.9731\n",
      "Epoch 24 - Train Loss: 0.0008, Train Acc: 0.9657, Val Loss: 0.0008, Val Acc: 0.9681\n",
      "Epoch 25 - Train Loss: 0.0008, Train Acc: 0.9671, Val Loss: 0.0007, Val Acc: 0.9742\n",
      "Epoch 26 - Train Loss: 0.0008, Train Acc: 0.9667, Val Loss: 0.0007, Val Acc: 0.9721\n",
      "Epoch 27 - Train Loss: 0.0007, Train Acc: 0.9675, Val Loss: 0.0007, Val Acc: 0.9719\n",
      "Epoch 28 - Train Loss: 0.0007, Train Acc: 0.9676, Val Loss: 0.0007, Val Acc: 0.9720\n",
      "Epoch 29 - Train Loss: 0.0007, Train Acc: 0.9677, Val Loss: 0.0007, Val Acc: 0.9711\n",
      "Epoch 30 - Train Loss: 0.0007, Train Acc: 0.9686, Val Loss: 0.0007, Val Acc: 0.9722\n",
      "Epoch 31 - Train Loss: 0.0007, Train Acc: 0.9678, Val Loss: 0.0007, Val Acc: 0.9714\n",
      "Epoch 32 - Train Loss: 0.0007, Train Acc: 0.9683, Val Loss: 0.0007, Val Acc: 0.9731\n",
      "Epoch 33 - Train Loss: 0.0007, Train Acc: 0.9693, Val Loss: 0.0007, Val Acc: 0.9721\n",
      "Epoch 34 - Train Loss: 0.0007, Train Acc: 0.9674, Val Loss: 0.0006, Val Acc: 0.9737\n",
      "Epoch 35 - Train Loss: 0.0007, Train Acc: 0.9683, Val Loss: 0.0007, Val Acc: 0.9729\n",
      "Epoch 36 - Train Loss: 0.0007, Train Acc: 0.9685, Val Loss: 0.0006, Val Acc: 0.9737\n",
      "Epoch 37 - Train Loss: 0.0007, Train Acc: 0.9690, Val Loss: 0.0007, Val Acc: 0.9724\n",
      "Epoch 38 - Train Loss: 0.0007, Train Acc: 0.9689, Val Loss: 0.0006, Val Acc: 0.9732\n",
      "Epoch 39 - Train Loss: 0.0007, Train Acc: 0.9693, Val Loss: 0.0007, Val Acc: 0.9718\n",
      "Epoch 40 - Train Loss: 0.0007, Train Acc: 0.9686, Val Loss: 0.0007, Val Acc: 0.9733\n",
      "Epoch 41 - Train Loss: 0.0007, Train Acc: 0.9694, Val Loss: 0.0007, Val Acc: 0.9728\n",
      "Epoch 42 - Train Loss: 0.0007, Train Acc: 0.9690, Val Loss: 0.0007, Val Acc: 0.9717\n",
      "Epoch 43 - Train Loss: 0.0007, Train Acc: 0.9692, Val Loss: 0.0007, Val Acc: 0.9719\n",
      "Epoch 44 - Train Loss: 0.0007, Train Acc: 0.9687, Val Loss: 0.0007, Val Acc: 0.9711\n",
      "Epoch 45 - Train Loss: 0.0007, Train Acc: 0.9696, Val Loss: 0.0007, Val Acc: 0.9719\n",
      "Epoch 46 - Train Loss: 0.0007, Train Acc: 0.9694, Val Loss: 0.0006, Val Acc: 0.9736\n",
      "Epoch 47 - Train Loss: 0.0007, Train Acc: 0.9690, Val Loss: 0.0007, Val Acc: 0.9737\n",
      "Epoch 48 - Train Loss: 0.0007, Train Acc: 0.9690, Val Loss: 0.0007, Val Acc: 0.9724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 01:22:41,458] Trial 3 finished with value: 0.0006343805276211955 and parameters: {'learning_rate': 0.006992749444995678, 'conv_dropout_rate': 0.1864711026813954, 'linear_dropout_rate': 0.33999192976285103, 'batch_norm': True}. Best is trial 3 with value: 0.0006343805276211955.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Train Loss: 0.0007, Train Acc: 0.9688, Val Loss: 0.0007, Val Acc: 0.9725\n",
      "Epoch 0 - Train Loss: 0.0049, Train Acc: 0.7698, Val Loss: 0.0102, Val Acc: 0.6207\n",
      "Epoch 1 - Train Loss: 0.0025, Train Acc: 0.8849, Val Loss: 0.0042, Val Acc: 0.8211\n",
      "Epoch 2 - Train Loss: 0.0018, Train Acc: 0.9209, Val Loss: 0.0025, Val Acc: 0.8923\n",
      "Epoch 3 - Train Loss: 0.0014, Train Acc: 0.9375, Val Loss: 0.0020, Val Acc: 0.9112\n",
      "Epoch 4 - Train Loss: 0.0012, Train Acc: 0.9482, Val Loss: 0.0011, Val Acc: 0.9489\n",
      "Epoch 5 - Train Loss: 0.0008, Train Acc: 0.9641, Val Loss: 0.0009, Val Acc: 0.9626\n",
      "Epoch 6 - Train Loss: 0.0007, Train Acc: 0.9680, Val Loss: 0.0011, Val Acc: 0.9532\n",
      "Epoch 7 - Train Loss: 0.0007, Train Acc: 0.9702, Val Loss: 0.0017, Val Acc: 0.9288\n",
      "Epoch 8 - Train Loss: 0.0006, Train Acc: 0.9724, Val Loss: 0.0007, Val Acc: 0.9686\n",
      "Epoch 9 - Train Loss: 0.0006, Train Acc: 0.9745, Val Loss: 0.0012, Val Acc: 0.9489\n",
      "Epoch 10 - Train Loss: 0.0005, Train Acc: 0.9811, Val Loss: 0.0007, Val Acc: 0.9714\n",
      "Epoch 11 - Train Loss: 0.0004, Train Acc: 0.9816, Val Loss: 0.0007, Val Acc: 0.9734\n",
      "Epoch 12 - Train Loss: 0.0004, Train Acc: 0.9829, Val Loss: 0.0007, Val Acc: 0.9717\n",
      "Epoch 13 - Train Loss: 0.0004, Train Acc: 0.9836, Val Loss: 0.0006, Val Acc: 0.9753\n",
      "Epoch 14 - Train Loss: 0.0004, Train Acc: 0.9844, Val Loss: 0.0006, Val Acc: 0.9756\n",
      "Epoch 15 - Train Loss: 0.0003, Train Acc: 0.9877, Val Loss: 0.0006, Val Acc: 0.9768\n",
      "Epoch 16 - Train Loss: 0.0003, Train Acc: 0.9876, Val Loss: 0.0006, Val Acc: 0.9761\n",
      "Epoch 17 - Train Loss: 0.0003, Train Acc: 0.9884, Val Loss: 0.0005, Val Acc: 0.9779\n",
      "Epoch 18 - Train Loss: 0.0003, Train Acc: 0.9884, Val Loss: 0.0006, Val Acc: 0.9786\n",
      "Epoch 19 - Train Loss: 0.0003, Train Acc: 0.9889, Val Loss: 0.0005, Val Acc: 0.9779\n",
      "Epoch 20 - Train Loss: 0.0002, Train Acc: 0.9902, Val Loss: 0.0006, Val Acc: 0.9763\n",
      "Epoch 21 - Train Loss: 0.0002, Train Acc: 0.9897, Val Loss: 0.0005, Val Acc: 0.9789\n",
      "Epoch 22 - Train Loss: 0.0002, Train Acc: 0.9908, Val Loss: 0.0005, Val Acc: 0.9785\n",
      "Epoch 23 - Train Loss: 0.0002, Train Acc: 0.9910, Val Loss: 0.0005, Val Acc: 0.9785\n",
      "Epoch 24 - Train Loss: 0.0002, Train Acc: 0.9907, Val Loss: 0.0005, Val Acc: 0.9780\n",
      "Epoch 25 - Train Loss: 0.0002, Train Acc: 0.9916, Val Loss: 0.0005, Val Acc: 0.9804\n",
      "Epoch 26 - Train Loss: 0.0002, Train Acc: 0.9914, Val Loss: 0.0005, Val Acc: 0.9786\n",
      "Epoch 27 - Train Loss: 0.0002, Train Acc: 0.9915, Val Loss: 0.0005, Val Acc: 0.9790\n",
      "Epoch 28 - Train Loss: 0.0002, Train Acc: 0.9923, Val Loss: 0.0005, Val Acc: 0.9787\n",
      "Epoch 29 - Train Loss: 0.0002, Train Acc: 0.9917, Val Loss: 0.0005, Val Acc: 0.9787\n",
      "Epoch 30 - Train Loss: 0.0002, Train Acc: 0.9921, Val Loss: 0.0005, Val Acc: 0.9796\n",
      "Epoch 31 - Train Loss: 0.0002, Train Acc: 0.9923, Val Loss: 0.0005, Val Acc: 0.9796\n",
      "Epoch 32 - Train Loss: 0.0002, Train Acc: 0.9925, Val Loss: 0.0005, Val Acc: 0.9782\n",
      "Epoch 33 - Train Loss: 0.0002, Train Acc: 0.9922, Val Loss: 0.0005, Val Acc: 0.9793\n",
      "Epoch 34 - Train Loss: 0.0002, Train Acc: 0.9920, Val Loss: 0.0005, Val Acc: 0.9789\n",
      "Epoch 35 - Train Loss: 0.0002, Train Acc: 0.9926, Val Loss: 0.0005, Val Acc: 0.9795\n",
      "Epoch 36 - Train Loss: 0.0002, Train Acc: 0.9921, Val Loss: 0.0005, Val Acc: 0.9791\n",
      "Epoch 37 - Train Loss: 0.0002, Train Acc: 0.9922, Val Loss: 0.0005, Val Acc: 0.9789\n",
      "Epoch 38 - Train Loss: 0.0002, Train Acc: 0.9925, Val Loss: 0.0005, Val Acc: 0.9783\n",
      "Epoch 39 - Train Loss: 0.0002, Train Acc: 0.9924, Val Loss: 0.0005, Val Acc: 0.9793\n",
      "Epoch 40 - Train Loss: 0.0002, Train Acc: 0.9926, Val Loss: 0.0005, Val Acc: 0.9792\n",
      "Epoch 41 - Train Loss: 0.0002, Train Acc: 0.9929, Val Loss: 0.0005, Val Acc: 0.9785\n",
      "Epoch 42 - Train Loss: 0.0002, Train Acc: 0.9927, Val Loss: 0.0005, Val Acc: 0.9798\n",
      "Epoch 43 - Train Loss: 0.0002, Train Acc: 0.9926, Val Loss: 0.0005, Val Acc: 0.9798\n",
      "Epoch 44 - Train Loss: 0.0002, Train Acc: 0.9926, Val Loss: 0.0005, Val Acc: 0.9771\n",
      "Epoch 45 - Train Loss: 0.0002, Train Acc: 0.9933, Val Loss: 0.0005, Val Acc: 0.9788\n",
      "Epoch 46 - Train Loss: 0.0002, Train Acc: 0.9929, Val Loss: 0.0005, Val Acc: 0.9797\n",
      "Epoch 47 - Train Loss: 0.0002, Train Acc: 0.9925, Val Loss: 0.0005, Val Acc: 0.9787\n",
      "Epoch 48 - Train Loss: 0.0002, Train Acc: 0.9931, Val Loss: 0.0005, Val Acc: 0.9784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 01:30:38,862] Trial 4 finished with value: 0.0005016948412232449 and parameters: {'learning_rate': 0.005549275259630864, 'conv_dropout_rate': 0.00889034556838526, 'linear_dropout_rate': 0.11097060057588942, 'batch_norm': True}. Best is trial 4 with value: 0.0005016948412232449.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Train Loss: 0.0002, Train Acc: 0.9923, Val Loss: 0.0005, Val Acc: 0.9789\n",
      "Epoch 0 - Train Loss: 0.0054, Train Acc: 0.7438, Val Loss: 0.0064, Val Acc: 0.7078\n",
      "Epoch 1 - Train Loss: 0.0031, Train Acc: 0.8546, Val Loss: 0.0036, Val Acc: 0.8278\n",
      "Epoch 2 - Train Loss: 0.0024, Train Acc: 0.8913, Val Loss: 0.0051, Val Acc: 0.7919\n",
      "Epoch 3 - Train Loss: 0.0019, Train Acc: 0.9142, Val Loss: 0.0037, Val Acc: 0.8590\n",
      "Epoch 4 - Train Loss: 0.0016, Train Acc: 0.9293, Val Loss: 0.0017, Val Acc: 0.9229\n",
      "Epoch 5 - Train Loss: 0.0012, Train Acc: 0.9472, Val Loss: 0.0011, Val Acc: 0.9537\n",
      "Epoch 6 - Train Loss: 0.0011, Train Acc: 0.9510, Val Loss: 0.0010, Val Acc: 0.9588\n",
      "Epoch 7 - Train Loss: 0.0010, Train Acc: 0.9555, Val Loss: 0.0010, Val Acc: 0.9582\n",
      "Epoch 8 - Train Loss: 0.0010, Train Acc: 0.9567, Val Loss: 0.0011, Val Acc: 0.9543\n",
      "Epoch 9 - Train Loss: 0.0009, Train Acc: 0.9581, Val Loss: 0.0011, Val Acc: 0.9570\n",
      "Epoch 10 - Train Loss: 0.0008, Train Acc: 0.9656, Val Loss: 0.0008, Val Acc: 0.9688\n",
      "Epoch 11 - Train Loss: 0.0007, Train Acc: 0.9674, Val Loss: 0.0009, Val Acc: 0.9635\n",
      "Epoch 12 - Train Loss: 0.0007, Train Acc: 0.9680, Val Loss: 0.0008, Val Acc: 0.9690\n",
      "Epoch 13 - Train Loss: 0.0007, Train Acc: 0.9694, Val Loss: 0.0007, Val Acc: 0.9723\n",
      "Epoch 14 - Train Loss: 0.0007, Train Acc: 0.9705, Val Loss: 0.0008, Val Acc: 0.9659\n",
      "Epoch 15 - Train Loss: 0.0006, Train Acc: 0.9726, Val Loss: 0.0007, Val Acc: 0.9713\n",
      "Epoch 16 - Train Loss: 0.0006, Train Acc: 0.9728, Val Loss: 0.0007, Val Acc: 0.9703\n",
      "Epoch 17 - Train Loss: 0.0006, Train Acc: 0.9743, Val Loss: 0.0007, Val Acc: 0.9721\n",
      "Epoch 18 - Train Loss: 0.0006, Train Acc: 0.9746, Val Loss: 0.0008, Val Acc: 0.9691\n",
      "Epoch 19 - Train Loss: 0.0006, Train Acc: 0.9749, Val Loss: 0.0007, Val Acc: 0.9720\n",
      "Epoch 20 - Train Loss: 0.0005, Train Acc: 0.9763, Val Loss: 0.0006, Val Acc: 0.9743\n",
      "Epoch 21 - Train Loss: 0.0005, Train Acc: 0.9758, Val Loss: 0.0007, Val Acc: 0.9723\n",
      "Epoch 22 - Train Loss: 0.0005, Train Acc: 0.9759, Val Loss: 0.0007, Val Acc: 0.9727\n",
      "Epoch 23 - Train Loss: 0.0005, Train Acc: 0.9771, Val Loss: 0.0006, Val Acc: 0.9745\n",
      "Epoch 24 - Train Loss: 0.0005, Train Acc: 0.9778, Val Loss: 0.0007, Val Acc: 0.9741\n",
      "Epoch 25 - Train Loss: 0.0005, Train Acc: 0.9784, Val Loss: 0.0006, Val Acc: 0.9745\n",
      "Epoch 26 - Train Loss: 0.0005, Train Acc: 0.9778, Val Loss: 0.0006, Val Acc: 0.9744\n",
      "Epoch 27 - Train Loss: 0.0005, Train Acc: 0.9785, Val Loss: 0.0006, Val Acc: 0.9740\n",
      "Epoch 28 - Train Loss: 0.0005, Train Acc: 0.9786, Val Loss: 0.0006, Val Acc: 0.9743\n",
      "Epoch 29 - Train Loss: 0.0005, Train Acc: 0.9789, Val Loss: 0.0006, Val Acc: 0.9738\n",
      "Epoch 30 - Train Loss: 0.0005, Train Acc: 0.9782, Val Loss: 0.0006, Val Acc: 0.9732\n",
      "Epoch 31 - Train Loss: 0.0005, Train Acc: 0.9787, Val Loss: 0.0006, Val Acc: 0.9746\n",
      "Epoch 32 - Train Loss: 0.0005, Train Acc: 0.9796, Val Loss: 0.0006, Val Acc: 0.9744\n",
      "Epoch 33 - Train Loss: 0.0005, Train Acc: 0.9789, Val Loss: 0.0006, Val Acc: 0.9756\n",
      "Epoch 34 - Train Loss: 0.0005, Train Acc: 0.9786, Val Loss: 0.0006, Val Acc: 0.9748\n",
      "Epoch 35 - Train Loss: 0.0005, Train Acc: 0.9795, Val Loss: 0.0006, Val Acc: 0.9752\n",
      "Epoch 36 - Train Loss: 0.0005, Train Acc: 0.9785, Val Loss: 0.0006, Val Acc: 0.9755\n",
      "Epoch 37 - Train Loss: 0.0005, Train Acc: 0.9791, Val Loss: 0.0006, Val Acc: 0.9739\n",
      "Epoch 38 - Train Loss: 0.0005, Train Acc: 0.9793, Val Loss: 0.0006, Val Acc: 0.9731\n",
      "Epoch 39 - Train Loss: 0.0005, Train Acc: 0.9799, Val Loss: 0.0006, Val Acc: 0.9736\n",
      "Epoch 40 - Train Loss: 0.0005, Train Acc: 0.9793, Val Loss: 0.0006, Val Acc: 0.9748\n",
      "Epoch 41 - Train Loss: 0.0005, Train Acc: 0.9793, Val Loss: 0.0006, Val Acc: 0.9745\n",
      "Epoch 42 - Train Loss: 0.0005, Train Acc: 0.9799, Val Loss: 0.0006, Val Acc: 0.9735\n",
      "Epoch 43 - Train Loss: 0.0005, Train Acc: 0.9796, Val Loss: 0.0006, Val Acc: 0.9739\n",
      "Epoch 44 - Train Loss: 0.0005, Train Acc: 0.9799, Val Loss: 0.0007, Val Acc: 0.9731\n",
      "Epoch 45 - Train Loss: 0.0004, Train Acc: 0.9803, Val Loss: 0.0006, Val Acc: 0.9745\n",
      "Epoch 46 - Train Loss: 0.0005, Train Acc: 0.9790, Val Loss: 0.0006, Val Acc: 0.9758\n",
      "Epoch 47 - Train Loss: 0.0005, Train Acc: 0.9790, Val Loss: 0.0006, Val Acc: 0.9749\n",
      "Epoch 48 - Train Loss: 0.0005, Train Acc: 0.9797, Val Loss: 0.0006, Val Acc: 0.9754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 01:38:37,445] Trial 5 finished with value: 0.0005922728414196329 and parameters: {'learning_rate': 0.008908463134778761, 'conv_dropout_rate': 0.18179446757094564, 'linear_dropout_rate': 0.041946215349832505, 'batch_norm': True}. Best is trial 4 with value: 0.0005016948412232449.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Train Loss: 0.0005, Train Acc: 0.9796, Val Loss: 0.0006, Val Acc: 0.9739\n",
      "Epoch 0 - Train Loss: 0.0110, Train Acc: 0.4502, Val Loss: 0.0091, Val Acc: 0.5538\n",
      "Epoch 1 - Train Loss: 0.0068, Train Acc: 0.6692, Val Loss: 0.0059, Val Acc: 0.7260\n",
      "Epoch 2 - Train Loss: 0.0051, Train Acc: 0.7603, Val Loss: 0.0053, Val Acc: 0.7445\n",
      "Epoch 3 - Train Loss: 0.0041, Train Acc: 0.8062, Val Loss: 0.0034, Val Acc: 0.8430\n",
      "Epoch 4 - Train Loss: 0.0035, Train Acc: 0.8348, Val Loss: 0.0030, Val Acc: 0.8593\n",
      "Epoch 5 - Train Loss: 0.0028, Train Acc: 0.8688, Val Loss: 0.0025, Val Acc: 0.8826\n",
      "Epoch 6 - Train Loss: 0.0026, Train Acc: 0.8796, Val Loss: 0.0025, Val Acc: 0.8848\n",
      "Epoch 7 - Train Loss: 0.0024, Train Acc: 0.8874, Val Loss: 0.0024, Val Acc: 0.8918\n",
      "Epoch 8 - Train Loss: 0.0022, Train Acc: 0.8954, Val Loss: 0.0022, Val Acc: 0.9039\n",
      "Epoch 9 - Train Loss: 0.0021, Train Acc: 0.9049, Val Loss: 0.0023, Val Acc: 0.8911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 01:40:20,194] Trial 6 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Train Loss: 0.0118, Train Acc: 0.4175, Val Loss: 0.0100, Val Acc: 0.4994\n",
      "Epoch 1 - Train Loss: 0.0071, Train Acc: 0.6444, Val Loss: 0.0061, Val Acc: 0.7032\n",
      "Epoch 2 - Train Loss: 0.0058, Train Acc: 0.7205, Val Loss: 0.0055, Val Acc: 0.7338\n",
      "Epoch 3 - Train Loss: 0.0046, Train Acc: 0.7802, Val Loss: 0.0043, Val Acc: 0.8003\n",
      "Epoch 4 - Train Loss: 0.0039, Train Acc: 0.8173, Val Loss: 0.0038, Val Acc: 0.8230\n",
      "Epoch 5 - Train Loss: 0.0031, Train Acc: 0.8532, Val Loss: 0.0030, Val Acc: 0.8599\n",
      "Epoch 6 - Train Loss: 0.0028, Train Acc: 0.8664, Val Loss: 0.0027, Val Acc: 0.8764\n",
      "Epoch 7 - Train Loss: 0.0026, Train Acc: 0.8773, Val Loss: 0.0026, Val Acc: 0.8770\n",
      "Epoch 8 - Train Loss: 0.0024, Train Acc: 0.8878, Val Loss: 0.0022, Val Acc: 0.8985\n",
      "Epoch 9 - Train Loss: 0.0022, Train Acc: 0.8968, Val Loss: 0.0025, Val Acc: 0.8831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 01:42:05,252] Trial 7 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Train Loss: 0.0088, Train Acc: 0.5784, Val Loss: 0.0070, Val Acc: 0.6656\n",
      "Epoch 1 - Train Loss: 0.0059, Train Acc: 0.7253, Val Loss: 0.0071, Val Acc: 0.6433\n",
      "Epoch 2 - Train Loss: 0.0049, Train Acc: 0.7737, Val Loss: 0.0049, Val Acc: 0.7734\n",
      "Epoch 3 - Train Loss: 0.0043, Train Acc: 0.8033, Val Loss: 0.0049, Val Acc: 0.7695\n",
      "Epoch 4 - Train Loss: 0.0037, Train Acc: 0.8300, Val Loss: 0.0052, Val Acc: 0.7487\n",
      "Epoch 5 - Train Loss: 0.0033, Train Acc: 0.8521, Val Loss: 0.0031, Val Acc: 0.8567\n",
      "Epoch 6 - Train Loss: 0.0031, Train Acc: 0.8600, Val Loss: 0.0035, Val Acc: 0.8376\n",
      "Epoch 7 - Train Loss: 0.0030, Train Acc: 0.8664, Val Loss: 0.0030, Val Acc: 0.8655\n",
      "Epoch 8 - Train Loss: 0.0028, Train Acc: 0.8732, Val Loss: 0.0027, Val Acc: 0.8768\n",
      "Epoch 9 - Train Loss: 0.0027, Train Acc: 0.8801, Val Loss: 0.0032, Val Acc: 0.8569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 01:43:50,059] Trial 8 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Train Loss: 0.0143, Train Acc: 0.2948, Val Loss: 0.0155, Val Acc: 0.3559\n",
      "Epoch 1 - Train Loss: 0.0086, Train Acc: 0.5787, Val Loss: 0.0078, Val Acc: 0.6134\n",
      "Epoch 2 - Train Loss: 0.0073, Train Acc: 0.6365, Val Loss: 0.0072, Val Acc: 0.6482\n",
      "Epoch 3 - Train Loss: 0.0065, Train Acc: 0.6830, Val Loss: 0.0059, Val Acc: 0.7097\n",
      "Epoch 4 - Train Loss: 0.0057, Train Acc: 0.7253, Val Loss: 0.0054, Val Acc: 0.7514\n",
      "Epoch 5 - Train Loss: 0.0048, Train Acc: 0.7728, Val Loss: 0.0045, Val Acc: 0.7984\n",
      "Epoch 6 - Train Loss: 0.0046, Train Acc: 0.7880, Val Loss: 0.0046, Val Acc: 0.7878\n",
      "Epoch 7 - Train Loss: 0.0043, Train Acc: 0.7983, Val Loss: 0.0048, Val Acc: 0.7829\n",
      "Epoch 8 - Train Loss: 0.0042, Train Acc: 0.8045, Val Loss: 0.0039, Val Acc: 0.8212\n",
      "Epoch 9 - Train Loss: 0.0040, Train Acc: 0.8116, Val Loss: 0.0038, Val Acc: 0.8241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 01:45:33,330] Trial 9 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Train Loss: 0.0063, Train Acc: 0.7007, Val Loss: 0.0061, Val Acc: 0.7233\n",
      "Epoch 1 - Train Loss: 0.0038, Train Acc: 0.8254, Val Loss: 0.0060, Val Acc: 0.7393\n",
      "Epoch 2 - Train Loss: 0.0030, Train Acc: 0.8633, Val Loss: 0.0025, Val Acc: 0.8839\n",
      "Epoch 3 - Train Loss: 0.0024, Train Acc: 0.8904, Val Loss: 0.0077, Val Acc: 0.7277\n",
      "Epoch 4 - Train Loss: 0.0021, Train Acc: 0.9074, Val Loss: 0.0015, Val Acc: 0.9317\n",
      "Epoch 5 - Train Loss: 0.0017, Train Acc: 0.9249, Val Loss: 0.0012, Val Acc: 0.9478\n",
      "Epoch 6 - Train Loss: 0.0015, Train Acc: 0.9303, Val Loss: 0.0015, Val Acc: 0.9325\n",
      "Epoch 7 - Train Loss: 0.0015, Train Acc: 0.9338, Val Loss: 0.0012, Val Acc: 0.9438\n",
      "Epoch 8 - Train Loss: 0.0014, Train Acc: 0.9365, Val Loss: 0.0010, Val Acc: 0.9573\n",
      "Epoch 9 - Train Loss: 0.0014, Train Acc: 0.9395, Val Loss: 0.0014, Val Acc: 0.9412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 01:47:19,485] Trial 10 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Train Loss: 0.0065, Train Acc: 0.6910, Val Loss: 0.0113, Val Acc: 0.6347\n",
      "Epoch 1 - Train Loss: 0.0040, Train Acc: 0.8117, Val Loss: 0.0069, Val Acc: 0.6982\n",
      "Epoch 2 - Train Loss: 0.0032, Train Acc: 0.8492, Val Loss: 0.0046, Val Acc: 0.7948\n",
      "Epoch 3 - Train Loss: 0.0027, Train Acc: 0.8764, Val Loss: 0.0093, Val Acc: 0.7051\n",
      "Epoch 4 - Train Loss: 0.0023, Train Acc: 0.8932, Val Loss: 0.0019, Val Acc: 0.9183\n",
      "Epoch 5 - Train Loss: 0.0019, Train Acc: 0.9126, Val Loss: 0.0014, Val Acc: 0.9367\n",
      "Epoch 6 - Train Loss: 0.0018, Train Acc: 0.9195, Val Loss: 0.0019, Val Acc: 0.9165\n",
      "Epoch 7 - Train Loss: 0.0017, Train Acc: 0.9220, Val Loss: 0.0023, Val Acc: 0.8971\n",
      "Epoch 8 - Train Loss: 0.0017, Train Acc: 0.9240, Val Loss: 0.0018, Val Acc: 0.9209\n",
      "Epoch 9 - Train Loss: 0.0016, Train Acc: 0.9290, Val Loss: 0.0015, Val Acc: 0.9379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 01:49:04,237] Trial 11 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Train Loss: 0.0058, Train Acc: 0.7272, Val Loss: 0.0107, Val Acc: 0.6645\n",
      "Epoch 1 - Train Loss: 0.0034, Train Acc: 0.8413, Val Loss: 0.0040, Val Acc: 0.8063\n",
      "Epoch 2 - Train Loss: 0.0026, Train Acc: 0.8774, Val Loss: 0.0046, Val Acc: 0.8109\n",
      "Epoch 3 - Train Loss: 0.0021, Train Acc: 0.9033, Val Loss: 0.0055, Val Acc: 0.7999\n",
      "Epoch 4 - Train Loss: 0.0018, Train Acc: 0.9168, Val Loss: 0.0018, Val Acc: 0.9216\n",
      "Epoch 5 - Train Loss: 0.0014, Train Acc: 0.9351, Val Loss: 0.0014, Val Acc: 0.9388\n",
      "Epoch 6 - Train Loss: 0.0013, Train Acc: 0.9399, Val Loss: 0.0016, Val Acc: 0.9301\n",
      "Epoch 7 - Train Loss: 0.0012, Train Acc: 0.9452, Val Loss: 0.0011, Val Acc: 0.9545\n",
      "Epoch 8 - Train Loss: 0.0012, Train Acc: 0.9469, Val Loss: 0.0012, Val Acc: 0.9515\n",
      "Epoch 9 - Train Loss: 0.0011, Train Acc: 0.9486, Val Loss: 0.0017, Val Acc: 0.9306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 01:50:49,544] Trial 12 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Train Loss: 0.0051, Train Acc: 0.7594, Val Loss: 0.0089, Val Acc: 0.6704\n",
      "Epoch 1 - Train Loss: 0.0027, Train Acc: 0.8758, Val Loss: 0.0037, Val Acc: 0.8387\n",
      "Epoch 2 - Train Loss: 0.0019, Train Acc: 0.9124, Val Loss: 0.0026, Val Acc: 0.8909\n",
      "Epoch 3 - Train Loss: 0.0016, Train Acc: 0.9297, Val Loss: 0.0015, Val Acc: 0.9361\n",
      "Epoch 4 - Train Loss: 0.0013, Train Acc: 0.9410, Val Loss: 0.0012, Val Acc: 0.9489\n",
      "Epoch 5 - Train Loss: 0.0010, Train Acc: 0.9576, Val Loss: 0.0009, Val Acc: 0.9628\n",
      "Epoch 6 - Train Loss: 0.0009, Train Acc: 0.9613, Val Loss: 0.0009, Val Acc: 0.9606\n",
      "Epoch 7 - Train Loss: 0.0008, Train Acc: 0.9633, Val Loss: 0.0016, Val Acc: 0.9291\n",
      "Epoch 8 - Train Loss: 0.0008, Train Acc: 0.9650, Val Loss: 0.0009, Val Acc: 0.9621\n",
      "Epoch 9 - Train Loss: 0.0007, Train Acc: 0.9679, Val Loss: 0.0009, Val Acc: 0.9627\n",
      "Epoch 10 - Train Loss: 0.0006, Train Acc: 0.9741, Val Loss: 0.0007, Val Acc: 0.9719\n",
      "Epoch 11 - Train Loss: 0.0005, Train Acc: 0.9763, Val Loss: 0.0008, Val Acc: 0.9675\n",
      "Epoch 12 - Train Loss: 0.0006, Train Acc: 0.9760, Val Loss: 0.0006, Val Acc: 0.9734\n",
      "Epoch 13 - Train Loss: 0.0005, Train Acc: 0.9774, Val Loss: 0.0006, Val Acc: 0.9730\n",
      "Epoch 14 - Train Loss: 0.0005, Train Acc: 0.9782, Val Loss: 0.0006, Val Acc: 0.9744\n",
      "Epoch 15 - Train Loss: 0.0004, Train Acc: 0.9816, Val Loss: 0.0006, Val Acc: 0.9759\n",
      "Epoch 16 - Train Loss: 0.0004, Train Acc: 0.9811, Val Loss: 0.0006, Val Acc: 0.9754\n",
      "Epoch 17 - Train Loss: 0.0004, Train Acc: 0.9817, Val Loss: 0.0005, Val Acc: 0.9775\n",
      "Epoch 18 - Train Loss: 0.0004, Train Acc: 0.9813, Val Loss: 0.0005, Val Acc: 0.9764\n",
      "Epoch 19 - Train Loss: 0.0004, Train Acc: 0.9827, Val Loss: 0.0005, Val Acc: 0.9757\n",
      "Epoch 20 - Train Loss: 0.0004, Train Acc: 0.9836, Val Loss: 0.0006, Val Acc: 0.9754\n",
      "Epoch 21 - Train Loss: 0.0004, Train Acc: 0.9837, Val Loss: 0.0006, Val Acc: 0.9771\n",
      "Epoch 22 - Train Loss: 0.0004, Train Acc: 0.9847, Val Loss: 0.0005, Val Acc: 0.9779\n",
      "Epoch 23 - Train Loss: 0.0004, Train Acc: 0.9844, Val Loss: 0.0005, Val Acc: 0.9780\n",
      "Epoch 24 - Train Loss: 0.0004, Train Acc: 0.9845, Val Loss: 0.0005, Val Acc: 0.9787\n",
      "Epoch 25 - Train Loss: 0.0003, Train Acc: 0.9857, Val Loss: 0.0005, Val Acc: 0.9783\n",
      "Epoch 26 - Train Loss: 0.0003, Train Acc: 0.9853, Val Loss: 0.0005, Val Acc: 0.9794\n",
      "Epoch 27 - Train Loss: 0.0003, Train Acc: 0.9855, Val Loss: 0.0005, Val Acc: 0.9792\n",
      "Epoch 28 - Train Loss: 0.0003, Train Acc: 0.9864, Val Loss: 0.0005, Val Acc: 0.9784\n",
      "Epoch 29 - Train Loss: 0.0003, Train Acc: 0.9859, Val Loss: 0.0005, Val Acc: 0.9783\n",
      "Epoch 30 - Train Loss: 0.0003, Train Acc: 0.9862, Val Loss: 0.0005, Val Acc: 0.9783\n",
      "Epoch 31 - Train Loss: 0.0003, Train Acc: 0.9861, Val Loss: 0.0005, Val Acc: 0.9791\n",
      "Epoch 32 - Train Loss: 0.0003, Train Acc: 0.9864, Val Loss: 0.0005, Val Acc: 0.9788\n",
      "Epoch 33 - Train Loss: 0.0003, Train Acc: 0.9867, Val Loss: 0.0005, Val Acc: 0.9791\n",
      "Epoch 34 - Train Loss: 0.0003, Train Acc: 0.9865, Val Loss: 0.0005, Val Acc: 0.9790\n",
      "Epoch 35 - Train Loss: 0.0003, Train Acc: 0.9863, Val Loss: 0.0005, Val Acc: 0.9791\n",
      "Epoch 36 - Train Loss: 0.0003, Train Acc: 0.9862, Val Loss: 0.0005, Val Acc: 0.9792\n",
      "Epoch 37 - Train Loss: 0.0003, Train Acc: 0.9862, Val Loss: 0.0005, Val Acc: 0.9787\n",
      "Epoch 38 - Train Loss: 0.0003, Train Acc: 0.9870, Val Loss: 0.0005, Val Acc: 0.9787\n",
      "Epoch 39 - Train Loss: 0.0003, Train Acc: 0.9865, Val Loss: 0.0005, Val Acc: 0.9785\n",
      "Epoch 40 - Train Loss: 0.0003, Train Acc: 0.9870, Val Loss: 0.0005, Val Acc: 0.9788\n",
      "Epoch 41 - Train Loss: 0.0003, Train Acc: 0.9871, Val Loss: 0.0005, Val Acc: 0.9790\n",
      "Epoch 42 - Train Loss: 0.0003, Train Acc: 0.9870, Val Loss: 0.0005, Val Acc: 0.9781\n",
      "Epoch 43 - Train Loss: 0.0003, Train Acc: 0.9872, Val Loss: 0.0005, Val Acc: 0.9789\n",
      "Epoch 44 - Train Loss: 0.0003, Train Acc: 0.9870, Val Loss: 0.0005, Val Acc: 0.9785\n",
      "Epoch 45 - Train Loss: 0.0003, Train Acc: 0.9874, Val Loss: 0.0005, Val Acc: 0.9794\n",
      "Epoch 46 - Train Loss: 0.0003, Train Acc: 0.9867, Val Loss: 0.0005, Val Acc: 0.9792\n",
      "Epoch 47 - Train Loss: 0.0003, Train Acc: 0.9866, Val Loss: 0.0005, Val Acc: 0.9802\n",
      "Epoch 48 - Train Loss: 0.0003, Train Acc: 0.9865, Val Loss: 0.0005, Val Acc: 0.9785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 01:58:47,360] Trial 13 finished with value: 0.0004871595099416436 and parameters: {'learning_rate': 0.007789861629609755, 'conv_dropout_rate': 0.07236113326271656, 'linear_dropout_rate': 0.07906931047650823, 'batch_norm': True}. Best is trial 13 with value: 0.0004871595099416436.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Train Loss: 0.0003, Train Acc: 0.9864, Val Loss: 0.0005, Val Acc: 0.9781\n",
      "Epoch 0 - Train Loss: 0.0054, Train Acc: 0.7434, Val Loss: 0.0063, Val Acc: 0.7388\n",
      "Epoch 1 - Train Loss: 0.0030, Train Acc: 0.8629, Val Loss: 0.0023, Val Acc: 0.8922\n",
      "Epoch 2 - Train Loss: 0.0022, Train Acc: 0.9006, Val Loss: 0.0018, Val Acc: 0.9200\n",
      "Epoch 3 - Train Loss: 0.0017, Train Acc: 0.9224, Val Loss: 0.0059, Val Acc: 0.8100\n",
      "Epoch 4 - Train Loss: 0.0014, Train Acc: 0.9350, Val Loss: 0.0021, Val Acc: 0.9167\n",
      "Epoch 5 - Train Loss: 0.0011, Train Acc: 0.9520, Val Loss: 0.0010, Val Acc: 0.9579\n",
      "Epoch 6 - Train Loss: 0.0010, Train Acc: 0.9554, Val Loss: 0.0011, Val Acc: 0.9504\n",
      "Epoch 7 - Train Loss: 0.0009, Train Acc: 0.9578, Val Loss: 0.0008, Val Acc: 0.9646\n",
      "Epoch 8 - Train Loss: 0.0009, Train Acc: 0.9603, Val Loss: 0.0010, Val Acc: 0.9592\n",
      "Epoch 9 - Train Loss: 0.0008, Train Acc: 0.9632, Val Loss: 0.0010, Val Acc: 0.9552\n",
      "Epoch 10 - Train Loss: 0.0007, Train Acc: 0.9695, Val Loss: 0.0006, Val Acc: 0.9724\n",
      "Epoch 11 - Train Loss: 0.0007, Train Acc: 0.9711, Val Loss: 0.0007, Val Acc: 0.9702\n",
      "Epoch 12 - Train Loss: 0.0006, Train Acc: 0.9720, Val Loss: 0.0007, Val Acc: 0.9733\n",
      "Epoch 13 - Train Loss: 0.0006, Train Acc: 0.9729, Val Loss: 0.0006, Val Acc: 0.9756\n",
      "Epoch 14 - Train Loss: 0.0006, Train Acc: 0.9733, Val Loss: 0.0008, Val Acc: 0.9665\n",
      "Epoch 15 - Train Loss: 0.0005, Train Acc: 0.9775, Val Loss: 0.0006, Val Acc: 0.9736\n",
      "Epoch 16 - Train Loss: 0.0005, Train Acc: 0.9769, Val Loss: 0.0006, Val Acc: 0.9729\n",
      "Epoch 17 - Train Loss: 0.0005, Train Acc: 0.9777, Val Loss: 0.0006, Val Acc: 0.9747\n",
      "Epoch 18 - Train Loss: 0.0005, Train Acc: 0.9772, Val Loss: 0.0006, Val Acc: 0.9759\n",
      "Epoch 19 - Train Loss: 0.0005, Train Acc: 0.9788, Val Loss: 0.0006, Val Acc: 0.9768\n",
      "Epoch 20 - Train Loss: 0.0005, Train Acc: 0.9793, Val Loss: 0.0006, Val Acc: 0.9747\n",
      "Epoch 21 - Train Loss: 0.0005, Train Acc: 0.9794, Val Loss: 0.0005, Val Acc: 0.9782\n",
      "Epoch 22 - Train Loss: 0.0005, Train Acc: 0.9802, Val Loss: 0.0006, Val Acc: 0.9771\n",
      "Epoch 23 - Train Loss: 0.0005, Train Acc: 0.9803, Val Loss: 0.0006, Val Acc: 0.9775\n",
      "Epoch 24 - Train Loss: 0.0005, Train Acc: 0.9802, Val Loss: 0.0006, Val Acc: 0.9755\n",
      "Epoch 25 - Train Loss: 0.0004, Train Acc: 0.9813, Val Loss: 0.0005, Val Acc: 0.9788\n",
      "Epoch 26 - Train Loss: 0.0004, Train Acc: 0.9812, Val Loss: 0.0005, Val Acc: 0.9793\n",
      "Epoch 27 - Train Loss: 0.0004, Train Acc: 0.9817, Val Loss: 0.0005, Val Acc: 0.9794\n",
      "Epoch 28 - Train Loss: 0.0004, Train Acc: 0.9815, Val Loss: 0.0005, Val Acc: 0.9798\n",
      "Epoch 29 - Train Loss: 0.0004, Train Acc: 0.9817, Val Loss: 0.0005, Val Acc: 0.9775\n",
      "Epoch 30 - Train Loss: 0.0004, Train Acc: 0.9816, Val Loss: 0.0005, Val Acc: 0.9790\n",
      "Epoch 31 - Train Loss: 0.0004, Train Acc: 0.9814, Val Loss: 0.0005, Val Acc: 0.9789\n",
      "Epoch 32 - Train Loss: 0.0004, Train Acc: 0.9819, Val Loss: 0.0005, Val Acc: 0.9786\n",
      "Epoch 33 - Train Loss: 0.0004, Train Acc: 0.9824, Val Loss: 0.0005, Val Acc: 0.9812\n",
      "Epoch 34 - Train Loss: 0.0004, Train Acc: 0.9819, Val Loss: 0.0005, Val Acc: 0.9785\n",
      "Epoch 35 - Train Loss: 0.0004, Train Acc: 0.9820, Val Loss: 0.0005, Val Acc: 0.9793\n",
      "Epoch 36 - Train Loss: 0.0004, Train Acc: 0.9817, Val Loss: 0.0005, Val Acc: 0.9792\n",
      "Epoch 37 - Train Loss: 0.0004, Train Acc: 0.9820, Val Loss: 0.0005, Val Acc: 0.9797\n",
      "Epoch 38 - Train Loss: 0.0004, Train Acc: 0.9822, Val Loss: 0.0005, Val Acc: 0.9778\n",
      "Epoch 39 - Train Loss: 0.0004, Train Acc: 0.9816, Val Loss: 0.0005, Val Acc: 0.9786\n",
      "Epoch 40 - Train Loss: 0.0004, Train Acc: 0.9822, Val Loss: 0.0005, Val Acc: 0.9794\n",
      "Epoch 41 - Train Loss: 0.0004, Train Acc: 0.9825, Val Loss: 0.0005, Val Acc: 0.9787\n",
      "Epoch 42 - Train Loss: 0.0004, Train Acc: 0.9825, Val Loss: 0.0005, Val Acc: 0.9790\n",
      "Epoch 43 - Train Loss: 0.0004, Train Acc: 0.9824, Val Loss: 0.0005, Val Acc: 0.9791\n",
      "Epoch 44 - Train Loss: 0.0004, Train Acc: 0.9822, Val Loss: 0.0005, Val Acc: 0.9781\n",
      "Epoch 45 - Train Loss: 0.0004, Train Acc: 0.9828, Val Loss: 0.0005, Val Acc: 0.9792\n",
      "Epoch 46 - Train Loss: 0.0004, Train Acc: 0.9824, Val Loss: 0.0005, Val Acc: 0.9795\n",
      "Epoch 47 - Train Loss: 0.0004, Train Acc: 0.9827, Val Loss: 0.0005, Val Acc: 0.9805\n",
      "Epoch 48 - Train Loss: 0.0004, Train Acc: 0.9828, Val Loss: 0.0005, Val Acc: 0.9788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 02:06:46,183] Trial 14 finished with value: 0.0005040973893551827 and parameters: {'learning_rate': 0.005063368401796009, 'conv_dropout_rate': 0.06300583890515721, 'linear_dropout_rate': 0.22887021323914541, 'batch_norm': True}. Best is trial 13 with value: 0.0004871595099416436.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Train Loss: 0.0004, Train Acc: 0.9825, Val Loss: 0.0005, Val Acc: 0.9787\n",
      "Epoch 0 - Train Loss: 0.0051, Train Acc: 0.7576, Val Loss: 0.0055, Val Acc: 0.7586\n",
      "Epoch 1 - Train Loss: 0.0028, Train Acc: 0.8733, Val Loss: 0.0038, Val Acc: 0.8329\n",
      "Epoch 2 - Train Loss: 0.0019, Train Acc: 0.9123, Val Loss: 0.0021, Val Acc: 0.9031\n",
      "Epoch 3 - Train Loss: 0.0016, Train Acc: 0.9300, Val Loss: 0.0014, Val Acc: 0.9396\n",
      "Epoch 4 - Train Loss: 0.0013, Train Acc: 0.9395, Val Loss: 0.0010, Val Acc: 0.9547\n",
      "Epoch 5 - Train Loss: 0.0009, Train Acc: 0.9577, Val Loss: 0.0009, Val Acc: 0.9615\n",
      "Epoch 6 - Train Loss: 0.0009, Train Acc: 0.9620, Val Loss: 0.0012, Val Acc: 0.9465\n",
      "Epoch 7 - Train Loss: 0.0008, Train Acc: 0.9637, Val Loss: 0.0011, Val Acc: 0.9540\n",
      "Epoch 8 - Train Loss: 0.0008, Train Acc: 0.9659, Val Loss: 0.0011, Val Acc: 0.9582\n",
      "Epoch 9 - Train Loss: 0.0007, Train Acc: 0.9670, Val Loss: 0.0008, Val Acc: 0.9655\n",
      "Epoch 10 - Train Loss: 0.0006, Train Acc: 0.9741, Val Loss: 0.0007, Val Acc: 0.9721\n",
      "Epoch 11 - Train Loss: 0.0006, Train Acc: 0.9759, Val Loss: 0.0008, Val Acc: 0.9697\n",
      "Epoch 12 - Train Loss: 0.0006, Train Acc: 0.9752, Val Loss: 0.0006, Val Acc: 0.9753\n",
      "Epoch 13 - Train Loss: 0.0005, Train Acc: 0.9775, Val Loss: 0.0006, Val Acc: 0.9756\n",
      "Epoch 14 - Train Loss: 0.0005, Train Acc: 0.9782, Val Loss: 0.0006, Val Acc: 0.9748\n",
      "Epoch 15 - Train Loss: 0.0004, Train Acc: 0.9812, Val Loss: 0.0005, Val Acc: 0.9777\n",
      "Epoch 16 - Train Loss: 0.0004, Train Acc: 0.9817, Val Loss: 0.0006, Val Acc: 0.9750\n",
      "Epoch 17 - Train Loss: 0.0004, Train Acc: 0.9819, Val Loss: 0.0005, Val Acc: 0.9778\n",
      "Epoch 18 - Train Loss: 0.0004, Train Acc: 0.9820, Val Loss: 0.0006, Val Acc: 0.9772\n",
      "Epoch 19 - Train Loss: 0.0004, Train Acc: 0.9828, Val Loss: 0.0005, Val Acc: 0.9795\n",
      "Epoch 20 - Train Loss: 0.0004, Train Acc: 0.9839, Val Loss: 0.0006, Val Acc: 0.9756\n",
      "Epoch 21 - Train Loss: 0.0004, Train Acc: 0.9839, Val Loss: 0.0005, Val Acc: 0.9787\n",
      "Epoch 22 - Train Loss: 0.0004, Train Acc: 0.9842, Val Loss: 0.0005, Val Acc: 0.9793\n",
      "Epoch 23 - Train Loss: 0.0004, Train Acc: 0.9845, Val Loss: 0.0005, Val Acc: 0.9787\n",
      "Epoch 24 - Train Loss: 0.0004, Train Acc: 0.9849, Val Loss: 0.0005, Val Acc: 0.9787\n",
      "Epoch 25 - Train Loss: 0.0003, Train Acc: 0.9855, Val Loss: 0.0005, Val Acc: 0.9804\n",
      "Epoch 26 - Train Loss: 0.0003, Train Acc: 0.9850, Val Loss: 0.0005, Val Acc: 0.9809\n",
      "Epoch 27 - Train Loss: 0.0003, Train Acc: 0.9863, Val Loss: 0.0005, Val Acc: 0.9801\n",
      "Epoch 28 - Train Loss: 0.0003, Train Acc: 0.9858, Val Loss: 0.0005, Val Acc: 0.9803\n",
      "Epoch 29 - Train Loss: 0.0003, Train Acc: 0.9861, Val Loss: 0.0005, Val Acc: 0.9783\n",
      "Epoch 30 - Train Loss: 0.0003, Train Acc: 0.9863, Val Loss: 0.0005, Val Acc: 0.9796\n",
      "Epoch 31 - Train Loss: 0.0003, Train Acc: 0.9863, Val Loss: 0.0005, Val Acc: 0.9799\n",
      "Epoch 32 - Train Loss: 0.0003, Train Acc: 0.9867, Val Loss: 0.0005, Val Acc: 0.9794\n",
      "Epoch 33 - Train Loss: 0.0003, Train Acc: 0.9874, Val Loss: 0.0005, Val Acc: 0.9803\n",
      "Epoch 34 - Train Loss: 0.0003, Train Acc: 0.9865, Val Loss: 0.0005, Val Acc: 0.9795\n",
      "Epoch 35 - Train Loss: 0.0003, Train Acc: 0.9866, Val Loss: 0.0005, Val Acc: 0.9795\n",
      "Epoch 36 - Train Loss: 0.0003, Train Acc: 0.9860, Val Loss: 0.0005, Val Acc: 0.9799\n",
      "Epoch 37 - Train Loss: 0.0003, Train Acc: 0.9865, Val Loss: 0.0005, Val Acc: 0.9794\n",
      "Epoch 38 - Train Loss: 0.0003, Train Acc: 0.9866, Val Loss: 0.0005, Val Acc: 0.9787\n",
      "Epoch 39 - Train Loss: 0.0003, Train Acc: 0.9866, Val Loss: 0.0005, Val Acc: 0.9803\n",
      "Epoch 40 - Train Loss: 0.0003, Train Acc: 0.9869, Val Loss: 0.0005, Val Acc: 0.9808\n",
      "Epoch 41 - Train Loss: 0.0003, Train Acc: 0.9870, Val Loss: 0.0005, Val Acc: 0.9811\n",
      "Epoch 42 - Train Loss: 0.0003, Train Acc: 0.9868, Val Loss: 0.0005, Val Acc: 0.9794\n",
      "Epoch 43 - Train Loss: 0.0003, Train Acc: 0.9867, Val Loss: 0.0005, Val Acc: 0.9795\n",
      "Epoch 44 - Train Loss: 0.0003, Train Acc: 0.9869, Val Loss: 0.0005, Val Acc: 0.9787\n",
      "Epoch 45 - Train Loss: 0.0003, Train Acc: 0.9871, Val Loss: 0.0005, Val Acc: 0.9798\n",
      "Epoch 46 - Train Loss: 0.0003, Train Acc: 0.9866, Val Loss: 0.0005, Val Acc: 0.9810\n",
      "Epoch 47 - Train Loss: 0.0003, Train Acc: 0.9869, Val Loss: 0.0005, Val Acc: 0.9803\n",
      "Epoch 48 - Train Loss: 0.0003, Train Acc: 0.9875, Val Loss: 0.0005, Val Acc: 0.9806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 02:14:29,533] Trial 15 finished with value: 0.00045894512019288846 and parameters: {'learning_rate': 0.007271066342854263, 'conv_dropout_rate': 0.05822855512226752, 'linear_dropout_rate': 0.17337990885692822, 'batch_norm': True}. Best is trial 15 with value: 0.00045894512019288846.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Train Loss: 0.0003, Train Acc: 0.9867, Val Loss: 0.0005, Val Acc: 0.9804\n",
      "Epoch 0 - Train Loss: 0.0054, Train Acc: 0.7419, Val Loss: 0.0077, Val Acc: 0.7319\n",
      "Epoch 1 - Train Loss: 0.0030, Train Acc: 0.8604, Val Loss: 0.0042, Val Acc: 0.8228\n",
      "Epoch 2 - Train Loss: 0.0022, Train Acc: 0.9016, Val Loss: 0.0019, Val Acc: 0.9112\n",
      "Epoch 3 - Train Loss: 0.0017, Train Acc: 0.9225, Val Loss: 0.0024, Val Acc: 0.8977\n",
      "Epoch 4 - Train Loss: 0.0015, Train Acc: 0.9357, Val Loss: 0.0010, Val Acc: 0.9550\n",
      "Epoch 5 - Train Loss: 0.0011, Train Acc: 0.9532, Val Loss: 0.0008, Val Acc: 0.9649\n",
      "Epoch 6 - Train Loss: 0.0010, Train Acc: 0.9570, Val Loss: 0.0012, Val Acc: 0.9507\n",
      "Epoch 7 - Train Loss: 0.0009, Train Acc: 0.9596, Val Loss: 0.0010, Val Acc: 0.9572\n",
      "Epoch 8 - Train Loss: 0.0009, Train Acc: 0.9614, Val Loss: 0.0008, Val Acc: 0.9648\n",
      "Epoch 9 - Train Loss: 0.0008, Train Acc: 0.9640, Val Loss: 0.0009, Val Acc: 0.9643\n",
      "Epoch 10 - Train Loss: 0.0007, Train Acc: 0.9706, Val Loss: 0.0007, Val Acc: 0.9710\n",
      "Epoch 11 - Train Loss: 0.0006, Train Acc: 0.9718, Val Loss: 0.0007, Val Acc: 0.9694\n",
      "Epoch 12 - Train Loss: 0.0006, Train Acc: 0.9718, Val Loss: 0.0007, Val Acc: 0.9698\n",
      "Epoch 13 - Train Loss: 0.0006, Train Acc: 0.9738, Val Loss: 0.0006, Val Acc: 0.9749\n",
      "Epoch 14 - Train Loss: 0.0006, Train Acc: 0.9742, Val Loss: 0.0007, Val Acc: 0.9710\n",
      "Epoch 15 - Train Loss: 0.0005, Train Acc: 0.9776, Val Loss: 0.0006, Val Acc: 0.9737\n",
      "Epoch 16 - Train Loss: 0.0005, Train Acc: 0.9774, Val Loss: 0.0006, Val Acc: 0.9767\n",
      "Epoch 17 - Train Loss: 0.0005, Train Acc: 0.9779, Val Loss: 0.0006, Val Acc: 0.9779\n",
      "Epoch 18 - Train Loss: 0.0005, Train Acc: 0.9787, Val Loss: 0.0006, Val Acc: 0.9782\n",
      "Epoch 19 - Train Loss: 0.0005, Train Acc: 0.9791, Val Loss: 0.0006, Val Acc: 0.9781\n",
      "Epoch 20 - Train Loss: 0.0005, Train Acc: 0.9805, Val Loss: 0.0005, Val Acc: 0.9787\n",
      "Epoch 21 - Train Loss: 0.0004, Train Acc: 0.9800, Val Loss: 0.0005, Val Acc: 0.9778\n",
      "Epoch 22 - Train Loss: 0.0004, Train Acc: 0.9807, Val Loss: 0.0005, Val Acc: 0.9779\n",
      "Epoch 23 - Train Loss: 0.0004, Train Acc: 0.9806, Val Loss: 0.0005, Val Acc: 0.9777\n",
      "Epoch 24 - Train Loss: 0.0004, Train Acc: 0.9812, Val Loss: 0.0005, Val Acc: 0.9780\n",
      "Epoch 25 - Train Loss: 0.0004, Train Acc: 0.9824, Val Loss: 0.0005, Val Acc: 0.9801\n",
      "Epoch 26 - Train Loss: 0.0004, Train Acc: 0.9814, Val Loss: 0.0005, Val Acc: 0.9787\n",
      "Epoch 27 - Train Loss: 0.0004, Train Acc: 0.9824, Val Loss: 0.0005, Val Acc: 0.9789\n",
      "Epoch 28 - Train Loss: 0.0004, Train Acc: 0.9825, Val Loss: 0.0005, Val Acc: 0.9794\n",
      "Epoch 29 - Train Loss: 0.0004, Train Acc: 0.9824, Val Loss: 0.0005, Val Acc: 0.9789\n",
      "Epoch 30 - Train Loss: 0.0004, Train Acc: 0.9825, Val Loss: 0.0005, Val Acc: 0.9791\n",
      "Epoch 31 - Train Loss: 0.0004, Train Acc: 0.9825, Val Loss: 0.0005, Val Acc: 0.9789\n",
      "Epoch 32 - Train Loss: 0.0004, Train Acc: 0.9828, Val Loss: 0.0005, Val Acc: 0.9797\n",
      "Epoch 33 - Train Loss: 0.0004, Train Acc: 0.9833, Val Loss: 0.0005, Val Acc: 0.9800\n",
      "Epoch 34 - Train Loss: 0.0004, Train Acc: 0.9821, Val Loss: 0.0005, Val Acc: 0.9792\n",
      "Epoch 35 - Train Loss: 0.0004, Train Acc: 0.9825, Val Loss: 0.0005, Val Acc: 0.9794\n",
      "Epoch 36 - Train Loss: 0.0004, Train Acc: 0.9830, Val Loss: 0.0005, Val Acc: 0.9793\n",
      "Epoch 37 - Train Loss: 0.0004, Train Acc: 0.9827, Val Loss: 0.0005, Val Acc: 0.9796\n",
      "Epoch 38 - Train Loss: 0.0004, Train Acc: 0.9828, Val Loss: 0.0005, Val Acc: 0.9793\n",
      "Epoch 39 - Train Loss: 0.0004, Train Acc: 0.9827, Val Loss: 0.0005, Val Acc: 0.9794\n",
      "Epoch 40 - Train Loss: 0.0004, Train Acc: 0.9833, Val Loss: 0.0005, Val Acc: 0.9794\n",
      "Epoch 41 - Train Loss: 0.0004, Train Acc: 0.9836, Val Loss: 0.0005, Val Acc: 0.9796\n",
      "Epoch 42 - Train Loss: 0.0004, Train Acc: 0.9834, Val Loss: 0.0005, Val Acc: 0.9790\n",
      "Epoch 43 - Train Loss: 0.0004, Train Acc: 0.9831, Val Loss: 0.0005, Val Acc: 0.9794\n",
      "Epoch 44 - Train Loss: 0.0004, Train Acc: 0.9832, Val Loss: 0.0005, Val Acc: 0.9791\n",
      "Epoch 45 - Train Loss: 0.0004, Train Acc: 0.9835, Val Loss: 0.0005, Val Acc: 0.9798\n",
      "Epoch 46 - Train Loss: 0.0004, Train Acc: 0.9833, Val Loss: 0.0005, Val Acc: 0.9793\n",
      "Epoch 47 - Train Loss: 0.0004, Train Acc: 0.9831, Val Loss: 0.0005, Val Acc: 0.9788\n",
      "Epoch 48 - Train Loss: 0.0004, Train Acc: 0.9840, Val Loss: 0.0005, Val Acc: 0.9794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 02:22:22,256] Trial 16 finished with value: 0.00048787330008405705 and parameters: {'learning_rate': 0.0074894347041346366, 'conv_dropout_rate': 0.0700806912705049, 'linear_dropout_rate': 0.2982040197481861, 'batch_norm': True}. Best is trial 15 with value: 0.00045894512019288846.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Train Loss: 0.0004, Train Acc: 0.9831, Val Loss: 0.0005, Val Acc: 0.9796\n",
      "Epoch 0 - Train Loss: 0.0053, Train Acc: 0.7504, Val Loss: 0.0103, Val Acc: 0.6959\n",
      "Epoch 1 - Train Loss: 0.0028, Train Acc: 0.8704, Val Loss: 0.0032, Val Acc: 0.8593\n",
      "Epoch 2 - Train Loss: 0.0020, Train Acc: 0.9091, Val Loss: 0.0025, Val Acc: 0.8958\n",
      "Epoch 3 - Train Loss: 0.0016, Train Acc: 0.9256, Val Loss: 0.0018, Val Acc: 0.9243\n",
      "Epoch 4 - Train Loss: 0.0014, Train Acc: 0.9372, Val Loss: 0.0013, Val Acc: 0.9386\n",
      "Epoch 5 - Train Loss: 0.0010, Train Acc: 0.9537, Val Loss: 0.0009, Val Acc: 0.9626\n",
      "Epoch 6 - Train Loss: 0.0009, Train Acc: 0.9580, Val Loss: 0.0011, Val Acc: 0.9536\n",
      "Epoch 7 - Train Loss: 0.0009, Train Acc: 0.9603, Val Loss: 0.0012, Val Acc: 0.9491\n",
      "Epoch 8 - Train Loss: 0.0009, Train Acc: 0.9614, Val Loss: 0.0011, Val Acc: 0.9528\n",
      "Epoch 9 - Train Loss: 0.0008, Train Acc: 0.9647, Val Loss: 0.0009, Val Acc: 0.9608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 02:24:07,393] Trial 17 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Train Loss: 0.0051, Train Acc: 0.7621, Val Loss: 0.0082, Val Acc: 0.7022\n",
      "Epoch 1 - Train Loss: 0.0027, Train Acc: 0.8768, Val Loss: 0.0041, Val Acc: 0.8030\n",
      "Epoch 2 - Train Loss: 0.0019, Train Acc: 0.9153, Val Loss: 0.0018, Val Acc: 0.9156\n",
      "Epoch 3 - Train Loss: 0.0015, Train Acc: 0.9321, Val Loss: 0.0024, Val Acc: 0.8982\n",
      "Epoch 4 - Train Loss: 0.0012, Train Acc: 0.9443, Val Loss: 0.0017, Val Acc: 0.9239\n",
      "Epoch 5 - Train Loss: 0.0009, Train Acc: 0.9598, Val Loss: 0.0008, Val Acc: 0.9654\n",
      "Epoch 6 - Train Loss: 0.0008, Train Acc: 0.9636, Val Loss: 0.0010, Val Acc: 0.9555\n",
      "Epoch 7 - Train Loss: 0.0008, Train Acc: 0.9663, Val Loss: 0.0009, Val Acc: 0.9574\n",
      "Epoch 8 - Train Loss: 0.0007, Train Acc: 0.9675, Val Loss: 0.0009, Val Acc: 0.9613\n",
      "Epoch 9 - Train Loss: 0.0007, Train Acc: 0.9700, Val Loss: 0.0009, Val Acc: 0.9675\n",
      "Epoch 10 - Train Loss: 0.0005, Train Acc: 0.9761, Val Loss: 0.0006, Val Acc: 0.9725\n",
      "Epoch 11 - Train Loss: 0.0005, Train Acc: 0.9780, Val Loss: 0.0007, Val Acc: 0.9709\n",
      "Epoch 12 - Train Loss: 0.0005, Train Acc: 0.9783, Val Loss: 0.0006, Val Acc: 0.9752\n",
      "Epoch 13 - Train Loss: 0.0005, Train Acc: 0.9790, Val Loss: 0.0006, Val Acc: 0.9758\n",
      "Epoch 14 - Train Loss: 0.0004, Train Acc: 0.9800, Val Loss: 0.0006, Val Acc: 0.9744\n",
      "Epoch 15 - Train Loss: 0.0004, Train Acc: 0.9828, Val Loss: 0.0005, Val Acc: 0.9786\n",
      "Epoch 16 - Train Loss: 0.0004, Train Acc: 0.9832, Val Loss: 0.0006, Val Acc: 0.9755\n",
      "Epoch 17 - Train Loss: 0.0004, Train Acc: 0.9835, Val Loss: 0.0005, Val Acc: 0.9787\n",
      "Epoch 18 - Train Loss: 0.0004, Train Acc: 0.9842, Val Loss: 0.0005, Val Acc: 0.9797\n",
      "Epoch 19 - Train Loss: 0.0004, Train Acc: 0.9843, Val Loss: 0.0005, Val Acc: 0.9796\n",
      "Epoch 20 - Train Loss: 0.0003, Train Acc: 0.9861, Val Loss: 0.0005, Val Acc: 0.9800\n",
      "Epoch 21 - Train Loss: 0.0003, Train Acc: 0.9860, Val Loss: 0.0005, Val Acc: 0.9792\n",
      "Epoch 22 - Train Loss: 0.0003, Train Acc: 0.9866, Val Loss: 0.0005, Val Acc: 0.9796\n",
      "Epoch 23 - Train Loss: 0.0003, Train Acc: 0.9857, Val Loss: 0.0005, Val Acc: 0.9791\n",
      "Epoch 24 - Train Loss: 0.0003, Train Acc: 0.9862, Val Loss: 0.0005, Val Acc: 0.9815\n",
      "Epoch 25 - Train Loss: 0.0003, Train Acc: 0.9870, Val Loss: 0.0005, Val Acc: 0.9796\n",
      "Epoch 26 - Train Loss: 0.0003, Train Acc: 0.9872, Val Loss: 0.0005, Val Acc: 0.9799\n",
      "Epoch 27 - Train Loss: 0.0003, Train Acc: 0.9874, Val Loss: 0.0005, Val Acc: 0.9807\n",
      "Epoch 28 - Train Loss: 0.0003, Train Acc: 0.9875, Val Loss: 0.0005, Val Acc: 0.9814\n",
      "Epoch 29 - Train Loss: 0.0003, Train Acc: 0.9874, Val Loss: 0.0005, Val Acc: 0.9804\n",
      "Epoch 30 - Train Loss: 0.0003, Train Acc: 0.9878, Val Loss: 0.0005, Val Acc: 0.9814\n",
      "Epoch 31 - Train Loss: 0.0003, Train Acc: 0.9877, Val Loss: 0.0005, Val Acc: 0.9805\n",
      "Epoch 32 - Train Loss: 0.0003, Train Acc: 0.9879, Val Loss: 0.0005, Val Acc: 0.9813\n",
      "Epoch 33 - Train Loss: 0.0003, Train Acc: 0.9882, Val Loss: 0.0005, Val Acc: 0.9805\n",
      "Epoch 34 - Train Loss: 0.0003, Train Acc: 0.9883, Val Loss: 0.0005, Val Acc: 0.9814\n",
      "Epoch 35 - Train Loss: 0.0003, Train Acc: 0.9878, Val Loss: 0.0005, Val Acc: 0.9804\n",
      "Epoch 36 - Train Loss: 0.0003, Train Acc: 0.9880, Val Loss: 0.0005, Val Acc: 0.9816\n",
      "Epoch 37 - Train Loss: 0.0003, Train Acc: 0.9882, Val Loss: 0.0005, Val Acc: 0.9812\n",
      "Epoch 38 - Train Loss: 0.0003, Train Acc: 0.9882, Val Loss: 0.0005, Val Acc: 0.9804\n",
      "Epoch 39 - Train Loss: 0.0003, Train Acc: 0.9883, Val Loss: 0.0005, Val Acc: 0.9813\n",
      "Epoch 40 - Train Loss: 0.0003, Train Acc: 0.9885, Val Loss: 0.0005, Val Acc: 0.9814\n",
      "Epoch 41 - Train Loss: 0.0003, Train Acc: 0.9887, Val Loss: 0.0005, Val Acc: 0.9800\n",
      "Epoch 42 - Train Loss: 0.0003, Train Acc: 0.9887, Val Loss: 0.0005, Val Acc: 0.9813\n",
      "Epoch 43 - Train Loss: 0.0003, Train Acc: 0.9884, Val Loss: 0.0005, Val Acc: 0.9803\n",
      "Epoch 44 - Train Loss: 0.0003, Train Acc: 0.9882, Val Loss: 0.0005, Val Acc: 0.9812\n",
      "Epoch 45 - Train Loss: 0.0003, Train Acc: 0.9886, Val Loss: 0.0005, Val Acc: 0.9823\n",
      "Epoch 46 - Train Loss: 0.0003, Train Acc: 0.9883, Val Loss: 0.0005, Val Acc: 0.9815\n",
      "Epoch 47 - Train Loss: 0.0003, Train Acc: 0.9890, Val Loss: 0.0005, Val Acc: 0.9807\n",
      "Epoch 48 - Train Loss: 0.0003, Train Acc: 0.9887, Val Loss: 0.0005, Val Acc: 0.9814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 02:32:06,865] Trial 18 finished with value: 0.0004563848245520292 and parameters: {'learning_rate': 0.006875079756898771, 'conv_dropout_rate': 0.044304581530436524, 'linear_dropout_rate': 0.15974308798725187, 'batch_norm': True}. Best is trial 18 with value: 0.0004563848245520292.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Train Loss: 0.0003, Train Acc: 0.9879, Val Loss: 0.0005, Val Acc: 0.9811\n",
      "Epoch 0 - Train Loss: 0.0050, Train Acc: 0.7652, Val Loss: 0.0127, Val Acc: 0.6383\n",
      "Epoch 1 - Train Loss: 0.0026, Train Acc: 0.8828, Val Loss: 0.0044, Val Acc: 0.8023\n",
      "Epoch 2 - Train Loss: 0.0018, Train Acc: 0.9171, Val Loss: 0.0028, Val Acc: 0.8870\n",
      "Epoch 3 - Train Loss: 0.0015, Train Acc: 0.9336, Val Loss: 0.0028, Val Acc: 0.8898\n",
      "Epoch 4 - Train Loss: 0.0012, Train Acc: 0.9448, Val Loss: 0.0014, Val Acc: 0.9330\n",
      "Epoch 5 - Train Loss: 0.0008, Train Acc: 0.9624, Val Loss: 0.0008, Val Acc: 0.9670\n",
      "Epoch 6 - Train Loss: 0.0007, Train Acc: 0.9663, Val Loss: 0.0011, Val Acc: 0.9534\n",
      "Epoch 7 - Train Loss: 0.0007, Train Acc: 0.9681, Val Loss: 0.0010, Val Acc: 0.9538\n",
      "Epoch 8 - Train Loss: 0.0007, Train Acc: 0.9708, Val Loss: 0.0012, Val Acc: 0.9511\n",
      "Epoch 9 - Train Loss: 0.0006, Train Acc: 0.9732, Val Loss: 0.0008, Val Acc: 0.9675\n",
      "Epoch 10 - Train Loss: 0.0005, Train Acc: 0.9791, Val Loss: 0.0007, Val Acc: 0.9727\n",
      "Epoch 11 - Train Loss: 0.0004, Train Acc: 0.9808, Val Loss: 0.0007, Val Acc: 0.9731\n",
      "Epoch 12 - Train Loss: 0.0004, Train Acc: 0.9812, Val Loss: 0.0007, Val Acc: 0.9735\n",
      "Epoch 13 - Train Loss: 0.0004, Train Acc: 0.9819, Val Loss: 0.0006, Val Acc: 0.9743\n",
      "Epoch 14 - Train Loss: 0.0004, Train Acc: 0.9829, Val Loss: 0.0006, Val Acc: 0.9754\n",
      "Epoch 15 - Train Loss: 0.0003, Train Acc: 0.9860, Val Loss: 0.0006, Val Acc: 0.9767\n",
      "Epoch 16 - Train Loss: 0.0003, Train Acc: 0.9863, Val Loss: 0.0006, Val Acc: 0.9775\n",
      "Epoch 17 - Train Loss: 0.0003, Train Acc: 0.9870, Val Loss: 0.0005, Val Acc: 0.9778\n",
      "Epoch 18 - Train Loss: 0.0003, Train Acc: 0.9869, Val Loss: 0.0005, Val Acc: 0.9793\n",
      "Epoch 19 - Train Loss: 0.0003, Train Acc: 0.9874, Val Loss: 0.0005, Val Acc: 0.9790\n",
      "Epoch 20 - Train Loss: 0.0003, Train Acc: 0.9888, Val Loss: 0.0005, Val Acc: 0.9779\n",
      "Epoch 21 - Train Loss: 0.0003, Train Acc: 0.9885, Val Loss: 0.0005, Val Acc: 0.9807\n",
      "Epoch 22 - Train Loss: 0.0003, Train Acc: 0.9892, Val Loss: 0.0005, Val Acc: 0.9784\n",
      "Epoch 23 - Train Loss: 0.0002, Train Acc: 0.9896, Val Loss: 0.0005, Val Acc: 0.9789\n",
      "Epoch 24 - Train Loss: 0.0003, Train Acc: 0.9890, Val Loss: 0.0005, Val Acc: 0.9805\n",
      "Epoch 25 - Train Loss: 0.0002, Train Acc: 0.9902, Val Loss: 0.0005, Val Acc: 0.9800\n",
      "Epoch 26 - Train Loss: 0.0002, Train Acc: 0.9892, Val Loss: 0.0005, Val Acc: 0.9803\n",
      "Epoch 27 - Train Loss: 0.0002, Train Acc: 0.9901, Val Loss: 0.0005, Val Acc: 0.9808\n",
      "Epoch 28 - Train Loss: 0.0002, Train Acc: 0.9905, Val Loss: 0.0005, Val Acc: 0.9810\n",
      "Epoch 29 - Train Loss: 0.0002, Train Acc: 0.9909, Val Loss: 0.0005, Val Acc: 0.9803\n",
      "Epoch 30 - Train Loss: 0.0002, Train Acc: 0.9908, Val Loss: 0.0005, Val Acc: 0.9803\n",
      "Epoch 31 - Train Loss: 0.0002, Train Acc: 0.9910, Val Loss: 0.0005, Val Acc: 0.9810\n",
      "Epoch 32 - Train Loss: 0.0002, Train Acc: 0.9910, Val Loss: 0.0005, Val Acc: 0.9802\n",
      "Epoch 33 - Train Loss: 0.0002, Train Acc: 0.9913, Val Loss: 0.0005, Val Acc: 0.9809\n",
      "Epoch 34 - Train Loss: 0.0002, Train Acc: 0.9906, Val Loss: 0.0005, Val Acc: 0.9813\n",
      "Epoch 35 - Train Loss: 0.0002, Train Acc: 0.9908, Val Loss: 0.0005, Val Acc: 0.9805\n",
      "Epoch 36 - Train Loss: 0.0002, Train Acc: 0.9907, Val Loss: 0.0005, Val Acc: 0.9812\n",
      "Epoch 37 - Train Loss: 0.0002, Train Acc: 0.9914, Val Loss: 0.0005, Val Acc: 0.9814\n",
      "Epoch 38 - Train Loss: 0.0002, Train Acc: 0.9911, Val Loss: 0.0005, Val Acc: 0.9801\n",
      "Epoch 39 - Train Loss: 0.0002, Train Acc: 0.9911, Val Loss: 0.0005, Val Acc: 0.9808\n",
      "Epoch 40 - Train Loss: 0.0002, Train Acc: 0.9914, Val Loss: 0.0005, Val Acc: 0.9808\n",
      "Epoch 41 - Train Loss: 0.0002, Train Acc: 0.9915, Val Loss: 0.0005, Val Acc: 0.9810\n",
      "Epoch 42 - Train Loss: 0.0002, Train Acc: 0.9915, Val Loss: 0.0005, Val Acc: 0.9802\n",
      "Epoch 43 - Train Loss: 0.0002, Train Acc: 0.9912, Val Loss: 0.0005, Val Acc: 0.9806\n",
      "Epoch 44 - Train Loss: 0.0002, Train Acc: 0.9913, Val Loss: 0.0005, Val Acc: 0.9795\n",
      "Epoch 45 - Train Loss: 0.0002, Train Acc: 0.9918, Val Loss: 0.0005, Val Acc: 0.9810\n",
      "Epoch 46 - Train Loss: 0.0002, Train Acc: 0.9908, Val Loss: 0.0005, Val Acc: 0.9802\n",
      "Epoch 47 - Train Loss: 0.0002, Train Acc: 0.9909, Val Loss: 0.0005, Val Acc: 0.9811\n",
      "Epoch 48 - Train Loss: 0.0002, Train Acc: 0.9913, Val Loss: 0.0005, Val Acc: 0.9803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 02:40:04,401] Trial 19 finished with value: 0.00046525780064631396 and parameters: {'learning_rate': 0.009996773643595896, 'conv_dropout_rate': 0.037241052129838886, 'linear_dropout_rate': 0.1581881393993519, 'batch_norm': True}. Best is trial 18 with value: 0.0004563848245520292.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Train Loss: 0.0002, Train Acc: 0.9908, Val Loss: 0.0005, Val Acc: 0.9803\n",
      "Epoch 0 - Train Loss: 0.0051, Train Acc: 0.7563, Val Loss: 0.0061, Val Acc: 0.7492\n",
      "Epoch 1 - Train Loss: 0.0027, Train Acc: 0.8748, Val Loss: 0.0028, Val Acc: 0.8648\n",
      "Epoch 2 - Train Loss: 0.0020, Train Acc: 0.9106, Val Loss: 0.0024, Val Acc: 0.8912\n",
      "Epoch 3 - Train Loss: 0.0016, Train Acc: 0.9305, Val Loss: 0.0029, Val Acc: 0.8806\n",
      "Epoch 4 - Train Loss: 0.0013, Train Acc: 0.9418, Val Loss: 0.0013, Val Acc: 0.9433\n",
      "Epoch 5 - Train Loss: 0.0010, Train Acc: 0.9579, Val Loss: 0.0009, Val Acc: 0.9636\n",
      "Epoch 6 - Train Loss: 0.0009, Train Acc: 0.9626, Val Loss: 0.0012, Val Acc: 0.9475\n",
      "Epoch 7 - Train Loss: 0.0008, Train Acc: 0.9651, Val Loss: 0.0009, Val Acc: 0.9640\n",
      "Epoch 8 - Train Loss: 0.0008, Train Acc: 0.9670, Val Loss: 0.0010, Val Acc: 0.9605\n",
      "Epoch 9 - Train Loss: 0.0007, Train Acc: 0.9688, Val Loss: 0.0009, Val Acc: 0.9588\n",
      "Epoch 10 - Train Loss: 0.0006, Train Acc: 0.9755, Val Loss: 0.0006, Val Acc: 0.9746\n",
      "Epoch 11 - Train Loss: 0.0005, Train Acc: 0.9769, Val Loss: 0.0007, Val Acc: 0.9711\n",
      "Epoch 12 - Train Loss: 0.0005, Train Acc: 0.9776, Val Loss: 0.0006, Val Acc: 0.9749\n",
      "Epoch 13 - Train Loss: 0.0005, Train Acc: 0.9779, Val Loss: 0.0006, Val Acc: 0.9739\n",
      "Epoch 14 - Train Loss: 0.0005, Train Acc: 0.9790, Val Loss: 0.0006, Val Acc: 0.9746\n",
      "Epoch 15 - Train Loss: 0.0004, Train Acc: 0.9825, Val Loss: 0.0005, Val Acc: 0.9772\n",
      "Epoch 16 - Train Loss: 0.0004, Train Acc: 0.9829, Val Loss: 0.0006, Val Acc: 0.9766\n",
      "Epoch 17 - Train Loss: 0.0004, Train Acc: 0.9833, Val Loss: 0.0005, Val Acc: 0.9788\n",
      "Epoch 18 - Train Loss: 0.0004, Train Acc: 0.9834, Val Loss: 0.0005, Val Acc: 0.9779\n",
      "Epoch 19 - Train Loss: 0.0004, Train Acc: 0.9839, Val Loss: 0.0005, Val Acc: 0.9798\n",
      "Epoch 20 - Train Loss: 0.0003, Train Acc: 0.9851, Val Loss: 0.0005, Val Acc: 0.9789\n",
      "Epoch 21 - Train Loss: 0.0003, Train Acc: 0.9858, Val Loss: 0.0005, Val Acc: 0.9791\n",
      "Epoch 22 - Train Loss: 0.0003, Train Acc: 0.9851, Val Loss: 0.0005, Val Acc: 0.9791\n",
      "Epoch 23 - Train Loss: 0.0003, Train Acc: 0.9857, Val Loss: 0.0005, Val Acc: 0.9791\n",
      "Epoch 24 - Train Loss: 0.0003, Train Acc: 0.9856, Val Loss: 0.0005, Val Acc: 0.9781\n",
      "Epoch 25 - Train Loss: 0.0003, Train Acc: 0.9872, Val Loss: 0.0005, Val Acc: 0.9819\n",
      "Epoch 26 - Train Loss: 0.0003, Train Acc: 0.9867, Val Loss: 0.0005, Val Acc: 0.9814\n",
      "Epoch 27 - Train Loss: 0.0003, Train Acc: 0.9867, Val Loss: 0.0005, Val Acc: 0.9804\n",
      "Epoch 28 - Train Loss: 0.0003, Train Acc: 0.9874, Val Loss: 0.0005, Val Acc: 0.9805\n",
      "Epoch 29 - Train Loss: 0.0003, Train Acc: 0.9874, Val Loss: 0.0005, Val Acc: 0.9803\n",
      "Epoch 30 - Train Loss: 0.0003, Train Acc: 0.9873, Val Loss: 0.0005, Val Acc: 0.9801\n",
      "Epoch 31 - Train Loss: 0.0003, Train Acc: 0.9871, Val Loss: 0.0005, Val Acc: 0.9802\n",
      "Epoch 32 - Train Loss: 0.0003, Train Acc: 0.9873, Val Loss: 0.0005, Val Acc: 0.9804\n",
      "Epoch 33 - Train Loss: 0.0003, Train Acc: 0.9876, Val Loss: 0.0005, Val Acc: 0.9818\n",
      "Epoch 34 - Train Loss: 0.0003, Train Acc: 0.9874, Val Loss: 0.0005, Val Acc: 0.9806\n",
      "Epoch 35 - Train Loss: 0.0003, Train Acc: 0.9879, Val Loss: 0.0005, Val Acc: 0.9804\n",
      "Epoch 36 - Train Loss: 0.0003, Train Acc: 0.9875, Val Loss: 0.0005, Val Acc: 0.9815\n",
      "Epoch 37 - Train Loss: 0.0003, Train Acc: 0.9876, Val Loss: 0.0005, Val Acc: 0.9802\n",
      "Epoch 38 - Train Loss: 0.0003, Train Acc: 0.9877, Val Loss: 0.0005, Val Acc: 0.9804\n",
      "Epoch 39 - Train Loss: 0.0003, Train Acc: 0.9874, Val Loss: 0.0005, Val Acc: 0.9803\n",
      "Epoch 40 - Train Loss: 0.0003, Train Acc: 0.9878, Val Loss: 0.0005, Val Acc: 0.9804\n",
      "Epoch 41 - Train Loss: 0.0003, Train Acc: 0.9877, Val Loss: 0.0005, Val Acc: 0.9808\n",
      "Epoch 42 - Train Loss: 0.0003, Train Acc: 0.9882, Val Loss: 0.0005, Val Acc: 0.9803\n",
      "Epoch 43 - Train Loss: 0.0003, Train Acc: 0.9873, Val Loss: 0.0005, Val Acc: 0.9817\n",
      "Epoch 44 - Train Loss: 0.0003, Train Acc: 0.9876, Val Loss: 0.0005, Val Acc: 0.9801\n",
      "Epoch 45 - Train Loss: 0.0003, Train Acc: 0.9880, Val Loss: 0.0005, Val Acc: 0.9806\n",
      "Epoch 46 - Train Loss: 0.0003, Train Acc: 0.9877, Val Loss: 0.0005, Val Acc: 0.9813\n",
      "Epoch 47 - Train Loss: 0.0003, Train Acc: 0.9878, Val Loss: 0.0005, Val Acc: 0.9807\n",
      "Epoch 48 - Train Loss: 0.0003, Train Acc: 0.9883, Val Loss: 0.0005, Val Acc: 0.9808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 02:48:03,729] Trial 20 finished with value: 0.0004526564758959882 and parameters: {'learning_rate': 0.006685940438423604, 'conv_dropout_rate': 0.034247852437351865, 'linear_dropout_rate': 0.2342757835800624, 'batch_norm': True}. Best is trial 20 with value: 0.0004526564758959882.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Train Loss: 0.0003, Train Acc: 0.9878, Val Loss: 0.0005, Val Acc: 0.9803\n",
      "Epoch 0 - Train Loss: 0.0053, Train Acc: 0.7520, Val Loss: 0.0050, Val Acc: 0.7813\n",
      "Epoch 1 - Train Loss: 0.0029, Train Acc: 0.8667, Val Loss: 0.0036, Val Acc: 0.8350\n",
      "Epoch 2 - Train Loss: 0.0021, Train Acc: 0.9037, Val Loss: 0.0027, Val Acc: 0.8766\n",
      "Epoch 3 - Train Loss: 0.0017, Train Acc: 0.9222, Val Loss: 0.0020, Val Acc: 0.9134\n",
      "Epoch 4 - Train Loss: 0.0015, Train Acc: 0.9354, Val Loss: 0.0020, Val Acc: 0.9108\n",
      "Epoch 5 - Train Loss: 0.0011, Train Acc: 0.9511, Val Loss: 0.0010, Val Acc: 0.9581\n",
      "Epoch 6 - Train Loss: 0.0010, Train Acc: 0.9576, Val Loss: 0.0013, Val Acc: 0.9423\n",
      "Epoch 7 - Train Loss: 0.0009, Train Acc: 0.9600, Val Loss: 0.0010, Val Acc: 0.9560\n",
      "Epoch 8 - Train Loss: 0.0009, Train Acc: 0.9609, Val Loss: 0.0009, Val Acc: 0.9631\n",
      "Epoch 9 - Train Loss: 0.0008, Train Acc: 0.9645, Val Loss: 0.0010, Val Acc: 0.9586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 02:49:48,696] Trial 21 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Train Loss: 0.0051, Train Acc: 0.7599, Val Loss: 0.0069, Val Acc: 0.7289\n",
      "Epoch 1 - Train Loss: 0.0027, Train Acc: 0.8766, Val Loss: 0.0044, Val Acc: 0.8065\n",
      "Epoch 2 - Train Loss: 0.0019, Train Acc: 0.9127, Val Loss: 0.0024, Val Acc: 0.8955\n",
      "Epoch 3 - Train Loss: 0.0016, Train Acc: 0.9282, Val Loss: 0.0013, Val Acc: 0.9437\n",
      "Epoch 4 - Train Loss: 0.0013, Train Acc: 0.9410, Val Loss: 0.0017, Val Acc: 0.9253\n",
      "Epoch 5 - Train Loss: 0.0010, Train Acc: 0.9569, Val Loss: 0.0010, Val Acc: 0.9572\n",
      "Epoch 6 - Train Loss: 0.0009, Train Acc: 0.9612, Val Loss: 0.0009, Val Acc: 0.9609\n",
      "Epoch 7 - Train Loss: 0.0008, Train Acc: 0.9639, Val Loss: 0.0010, Val Acc: 0.9564\n",
      "Epoch 8 - Train Loss: 0.0008, Train Acc: 0.9654, Val Loss: 0.0009, Val Acc: 0.9608\n",
      "Epoch 9 - Train Loss: 0.0007, Train Acc: 0.9671, Val Loss: 0.0008, Val Acc: 0.9652\n",
      "Epoch 10 - Train Loss: 0.0006, Train Acc: 0.9741, Val Loss: 0.0007, Val Acc: 0.9732\n",
      "Epoch 11 - Train Loss: 0.0006, Train Acc: 0.9754, Val Loss: 0.0007, Val Acc: 0.9694\n",
      "Epoch 12 - Train Loss: 0.0005, Train Acc: 0.9765, Val Loss: 0.0007, Val Acc: 0.9718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 02:52:05,679] Trial 22 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Train Loss: 0.0054, Train Acc: 0.7435, Val Loss: 0.0115, Val Acc: 0.6724\n",
      "Epoch 1 - Train Loss: 0.0030, Train Acc: 0.8620, Val Loss: 0.0034, Val Acc: 0.8501\n",
      "Epoch 2 - Train Loss: 0.0022, Train Acc: 0.8988, Val Loss: 0.0022, Val Acc: 0.8964\n",
      "Epoch 3 - Train Loss: 0.0018, Train Acc: 0.9198, Val Loss: 0.0020, Val Acc: 0.9210\n",
      "Epoch 4 - Train Loss: 0.0015, Train Acc: 0.9328, Val Loss: 0.0012, Val Acc: 0.9483\n",
      "Epoch 5 - Train Loss: 0.0011, Train Acc: 0.9495, Val Loss: 0.0009, Val Acc: 0.9592\n",
      "Epoch 6 - Train Loss: 0.0010, Train Acc: 0.9543, Val Loss: 0.0011, Val Acc: 0.9502\n",
      "Epoch 7 - Train Loss: 0.0010, Train Acc: 0.9564, Val Loss: 0.0010, Val Acc: 0.9563\n",
      "Epoch 8 - Train Loss: 0.0009, Train Acc: 0.9590, Val Loss: 0.0009, Val Acc: 0.9605\n",
      "Epoch 9 - Train Loss: 0.0009, Train Acc: 0.9617, Val Loss: 0.0008, Val Acc: 0.9630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 02:53:51,822] Trial 23 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Train Loss: 0.0056, Train Acc: 0.7353, Val Loss: 0.0050, Val Acc: 0.7667\n",
      "Epoch 1 - Train Loss: 0.0032, Train Acc: 0.8542, Val Loss: 0.0050, Val Acc: 0.7803\n",
      "Epoch 2 - Train Loss: 0.0024, Train Acc: 0.8895, Val Loss: 0.0037, Val Acc: 0.8508\n",
      "Epoch 3 - Train Loss: 0.0020, Train Acc: 0.9108, Val Loss: 0.0049, Val Acc: 0.8239\n",
      "Epoch 4 - Train Loss: 0.0017, Train Acc: 0.9244, Val Loss: 0.0013, Val Acc: 0.9419\n",
      "Epoch 5 - Train Loss: 0.0013, Train Acc: 0.9421, Val Loss: 0.0011, Val Acc: 0.9526\n",
      "Epoch 6 - Train Loss: 0.0012, Train Acc: 0.9464, Val Loss: 0.0016, Val Acc: 0.9306\n",
      "Epoch 7 - Train Loss: 0.0011, Train Acc: 0.9486, Val Loss: 0.0012, Val Acc: 0.9479\n",
      "Epoch 8 - Train Loss: 0.0011, Train Acc: 0.9503, Val Loss: 0.0019, Val Acc: 0.9206\n",
      "Epoch 9 - Train Loss: 0.0010, Train Acc: 0.9533, Val Loss: 0.0016, Val Acc: 0.9287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 02:55:37,315] Trial 24 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Train Loss: 0.0054, Train Acc: 0.7457, Val Loss: 0.0063, Val Acc: 0.7184\n",
      "Epoch 1 - Train Loss: 0.0029, Train Acc: 0.8679, Val Loss: 0.0056, Val Acc: 0.7614\n",
      "Epoch 2 - Train Loss: 0.0022, Train Acc: 0.9029, Val Loss: 0.0056, Val Acc: 0.8045\n",
      "Epoch 3 - Train Loss: 0.0018, Train Acc: 0.9211, Val Loss: 0.0025, Val Acc: 0.9016\n",
      "Epoch 4 - Train Loss: 0.0015, Train Acc: 0.9328, Val Loss: 0.0039, Val Acc: 0.8547\n",
      "Epoch 5 - Train Loss: 0.0011, Train Acc: 0.9509, Val Loss: 0.0009, Val Acc: 0.9608\n",
      "Epoch 6 - Train Loss: 0.0010, Train Acc: 0.9549, Val Loss: 0.0012, Val Acc: 0.9483\n",
      "Epoch 7 - Train Loss: 0.0010, Train Acc: 0.9570, Val Loss: 0.0013, Val Acc: 0.9416\n",
      "Epoch 8 - Train Loss: 0.0009, Train Acc: 0.9591, Val Loss: 0.0014, Val Acc: 0.9419\n",
      "Epoch 9 - Train Loss: 0.0009, Train Acc: 0.9610, Val Loss: 0.0010, Val Acc: 0.9571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 02:57:24,747] Trial 25 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Train Loss: 0.0052, Train Acc: 0.7557, Val Loss: 0.0075, Val Acc: 0.7314\n",
      "Epoch 1 - Train Loss: 0.0028, Train Acc: 0.8715, Val Loss: 0.0020, Val Acc: 0.9122\n",
      "Epoch 2 - Train Loss: 0.0021, Train Acc: 0.9082, Val Loss: 0.0028, Val Acc: 0.8712\n",
      "Epoch 3 - Train Loss: 0.0017, Train Acc: 0.9263, Val Loss: 0.0022, Val Acc: 0.9060\n",
      "Epoch 4 - Train Loss: 0.0014, Train Acc: 0.9396, Val Loss: 0.0018, Val Acc: 0.9259\n",
      "Epoch 5 - Train Loss: 0.0010, Train Acc: 0.9550, Val Loss: 0.0008, Val Acc: 0.9649\n",
      "Epoch 6 - Train Loss: 0.0009, Train Acc: 0.9614, Val Loss: 0.0013, Val Acc: 0.9452\n",
      "Epoch 7 - Train Loss: 0.0009, Train Acc: 0.9626, Val Loss: 0.0013, Val Acc: 0.9404\n",
      "Epoch 8 - Train Loss: 0.0008, Train Acc: 0.9646, Val Loss: 0.0009, Val Acc: 0.9592\n",
      "Epoch 9 - Train Loss: 0.0007, Train Acc: 0.9673, Val Loss: 0.0008, Val Acc: 0.9632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 02:59:10,398] Trial 26 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Train Loss: 0.0127, Train Acc: 0.3754, Val Loss: 0.0141, Val Acc: 0.3653\n",
      "Epoch 1 - Train Loss: 0.0077, Train Acc: 0.6158, Val Loss: 0.0068, Val Acc: 0.6705\n",
      "Epoch 2 - Train Loss: 0.0064, Train Acc: 0.6846, Val Loss: 0.0058, Val Acc: 0.7173\n",
      "Epoch 3 - Train Loss: 0.0055, Train Acc: 0.7419, Val Loss: 0.0048, Val Acc: 0.7794\n",
      "Epoch 4 - Train Loss: 0.0046, Train Acc: 0.7837, Val Loss: 0.0046, Val Acc: 0.7841\n",
      "Epoch 5 - Train Loss: 0.0039, Train Acc: 0.8164, Val Loss: 0.0037, Val Acc: 0.8274\n",
      "Epoch 6 - Train Loss: 0.0036, Train Acc: 0.8290, Val Loss: 0.0034, Val Acc: 0.8399\n",
      "Epoch 7 - Train Loss: 0.0034, Train Acc: 0.8390, Val Loss: 0.0045, Val Acc: 0.7893\n",
      "Epoch 8 - Train Loss: 0.0032, Train Acc: 0.8474, Val Loss: 0.0030, Val Acc: 0.8543\n",
      "Epoch 9 - Train Loss: 0.0030, Train Acc: 0.8561, Val Loss: 0.0033, Val Acc: 0.8400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 03:00:54,825] Trial 27 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Train Loss: 0.0065, Train Acc: 0.6928, Val Loss: 0.0075, Val Acc: 0.6641\n",
      "Epoch 1 - Train Loss: 0.0038, Train Acc: 0.8219, Val Loss: 0.0038, Val Acc: 0.8225\n",
      "Epoch 2 - Train Loss: 0.0029, Train Acc: 0.8661, Val Loss: 0.0040, Val Acc: 0.8337\n",
      "Epoch 3 - Train Loss: 0.0024, Train Acc: 0.8919, Val Loss: 0.0035, Val Acc: 0.8539\n",
      "Epoch 4 - Train Loss: 0.0020, Train Acc: 0.9096, Val Loss: 0.0018, Val Acc: 0.9203\n",
      "Epoch 5 - Train Loss: 0.0016, Train Acc: 0.9265, Val Loss: 0.0013, Val Acc: 0.9462\n",
      "Epoch 6 - Train Loss: 0.0015, Train Acc: 0.9315, Val Loss: 0.0013, Val Acc: 0.9419\n",
      "Epoch 7 - Train Loss: 0.0015, Train Acc: 0.9350, Val Loss: 0.0012, Val Acc: 0.9446\n",
      "Epoch 8 - Train Loss: 0.0014, Train Acc: 0.9374, Val Loss: 0.0016, Val Acc: 0.9271\n",
      "Epoch 9 - Train Loss: 0.0013, Train Acc: 0.9406, Val Loss: 0.0014, Val Acc: 0.9424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 03:02:42,144] Trial 28 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Train Loss: 0.0049, Train Acc: 0.7677, Val Loss: 0.0093, Val Acc: 0.7060\n",
      "Epoch 1 - Train Loss: 0.0026, Train Acc: 0.8831, Val Loss: 0.0024, Val Acc: 0.8962\n",
      "Epoch 2 - Train Loss: 0.0018, Train Acc: 0.9189, Val Loss: 0.0019, Val Acc: 0.9158\n",
      "Epoch 3 - Train Loss: 0.0015, Train Acc: 0.9343, Val Loss: 0.0027, Val Acc: 0.8764\n",
      "Epoch 4 - Train Loss: 0.0012, Train Acc: 0.9454, Val Loss: 0.0012, Val Acc: 0.9459\n",
      "Epoch 5 - Train Loss: 0.0009, Train Acc: 0.9622, Val Loss: 0.0009, Val Acc: 0.9628\n",
      "Epoch 6 - Train Loss: 0.0008, Train Acc: 0.9660, Val Loss: 0.0008, Val Acc: 0.9618\n",
      "Epoch 7 - Train Loss: 0.0007, Train Acc: 0.9682, Val Loss: 0.0010, Val Acc: 0.9582\n",
      "Epoch 8 - Train Loss: 0.0007, Train Acc: 0.9698, Val Loss: 0.0009, Val Acc: 0.9632\n",
      "Epoch 9 - Train Loss: 0.0006, Train Acc: 0.9712, Val Loss: 0.0008, Val Acc: 0.9659\n",
      "Epoch 10 - Train Loss: 0.0005, Train Acc: 0.9786, Val Loss: 0.0006, Val Acc: 0.9732\n",
      "Epoch 11 - Train Loss: 0.0005, Train Acc: 0.9792, Val Loss: 0.0007, Val Acc: 0.9695\n",
      "Epoch 12 - Train Loss: 0.0004, Train Acc: 0.9805, Val Loss: 0.0007, Val Acc: 0.9701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 03:04:57,555] Trial 29 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Train Loss: 0.0109, Train Acc: 0.4549, Val Loss: 0.0086, Val Acc: 0.5652\n",
      "Epoch 1 - Train Loss: 0.0069, Train Acc: 0.6634, Val Loss: 0.0060, Val Acc: 0.7095\n",
      "Epoch 2 - Train Loss: 0.0052, Train Acc: 0.7573, Val Loss: 0.0048, Val Acc: 0.7739\n",
      "Epoch 3 - Train Loss: 0.0043, Train Acc: 0.7983, Val Loss: 0.0051, Val Acc: 0.7680\n",
      "Epoch 4 - Train Loss: 0.0037, Train Acc: 0.8235, Val Loss: 0.0037, Val Acc: 0.8300\n",
      "Epoch 5 - Train Loss: 0.0030, Train Acc: 0.8565, Val Loss: 0.0027, Val Acc: 0.8692\n",
      "Epoch 6 - Train Loss: 0.0028, Train Acc: 0.8691, Val Loss: 0.0028, Val Acc: 0.8730\n",
      "Epoch 7 - Train Loss: 0.0026, Train Acc: 0.8763, Val Loss: 0.0026, Val Acc: 0.8835\n",
      "Epoch 8 - Train Loss: 0.0025, Train Acc: 0.8827, Val Loss: 0.0022, Val Acc: 0.9017\n",
      "Epoch 9 - Train Loss: 0.0023, Train Acc: 0.8914, Val Loss: 0.0026, Val Acc: 0.8731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 03:06:42,147] Trial 30 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.006685940438423604, 'conv_dropout_rate': 0.034247852437351865, 'linear_dropout_rate': 0.2342757835800624, 'batch_norm': True}\n",
      "Best Value: 0.0004526564758959882\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.MedianPruner(n_startup_trials=3, n_warmup_steps=10),\n",
    "                           storage=storage_url, study_name=study_name, load_if_exists=True)\n",
    "study.optimize(objective, n_trials = 30)\n",
    "\n",
    "# 結果の出力\n",
    "print(\"Best Parameters:\", study.best_params)\n",
    "print(\"Best Value:\", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "366bd356-8eb1-452d-978d-edebdc508994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Trial Number: 20\n",
      "Best Parameters: {'learning_rate': 0.006685940438423604, 'conv_dropout_rate': 0.034247852437351865, 'linear_dropout_rate': 0.2342757835800624, 'batch_norm': True}\n"
     ]
    }
   ],
   "source": [
    "# 保存済みのOptuna Studyをロード\n",
    "# study_name=\"250129_PathMNIST_2blocks\"\n",
    "study = optuna.load_study(study_name=study_name, storage=storage_url)\n",
    "\n",
    "# ベストトライアルを取得\n",
    "best_trial = study.best_trial\n",
    "print(f\"Best Trial Number: {best_trial.number}\")\n",
    "print(f\"Best Parameters: {best_trial.params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e08e3f1-5178-4484-a3f2-02bce4e313f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0061, Test Accuracy: 0.8703\n"
     ]
    }
   ],
   "source": [
    "# ベストトライアルのモデルパスを取得\n",
    "best_model_path = best_trial.user_attrs.get(\"model_path\", None)\n",
    "\n",
    "# モデル定義\n",
    "model = SmallVGG_2blocks(\n",
    "    in_channels=3,\n",
    "    num_classes=9,\n",
    "    conv_dropout_rate=best_trial.params[\"conv_dropout_rate\"],\n",
    "    linear_dropout_rate=best_trial.params[\"linear_dropout_rate\"],\n",
    "    batch_norm=best_trial.params[\"batch_norm\"],\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=best_trial.params[\"learning_rate\"],  momentum=0.9)\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# モデルロードと評価\n",
    "model.load_state_dict(torch.load(best_model_path, weights_only=True))\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "test_correct = 0\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        labels = labels.squeeze().long()  # ラベルを1Dに変換\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        test_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "test_acc = test_correct / len(test_loader.dataset)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b51aae-2c60-4460-8fa4-f17d56e7bd84",
   "metadata": {},
   "source": [
    "## 3blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a306700b-a743-4303-a47e-e1180695af0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 乱数シード設定\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "class SmallVGG_3blocks(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, conv_dropout_rate, linear_dropout_rate, batch_norm=True):\n",
    "        super(SmallVGG_3blocks, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # ブロック1\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32) if batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(conv_dropout_rate) if conv_dropout_rate else nn.Identity(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32) if batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 28x28 -> 14x14            \n",
    "            nn.Dropout(conv_dropout_rate) if conv_dropout_rate else nn.Identity(),\n",
    "\n",
    "            # ブロック2\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64) if batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(conv_dropout_rate) if conv_dropout_rate else nn.Identity(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64) if batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 14x14 -> 7x7            \n",
    "            nn.Dropout(conv_dropout_rate) if conv_dropout_rate else nn.Identity(),\n",
    "\n",
    "            # ブロック3\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128) if batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(conv_dropout_rate) if conv_dropout_rate else nn.Identity(),            \n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128) if batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 7x7 -> 3x3            \n",
    "            nn.Dropout(conv_dropout_rate) if conv_dropout_rate else nn.Identity(),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128 * 3 * 3, 256),  # Flattenした特徴量を全結合層に入力\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(linear_dropout_rate),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # フラット化\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ccb00c93-880b-48ca-9493-b2431f4b39a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_url = \"postgresql+psycopg2://optuna_user:optuna_password@portfolio3-postgres:5432/optuna_db\"\n",
    "study_name = \"250129_PathMNIST_3blocks\" # 変更要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ef09677-24f7-4c97-a2cc-0cc583f9fbcd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "VHCSdyFucw0a",
    "outputId": "70fe6df8-d4bf-4641-8198-d7a5c5b05f71"
   },
   "outputs": [],
   "source": [
    "# ログを出力しない\n",
    "# optuna.logging.disable_default_handler()\n",
    "\n",
    "def objective(trial):\n",
    "    set_seed(42)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # GPUが利用可能なら使用\n",
    "\n",
    "    # ハイパーパラメータのサンプリング\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2)\n",
    "    conv_dropout_rate = trial.suggest_float('conv_dropout_rate', 0.0, 0.3)\n",
    "    linear_dropout_rate = trial.suggest_float('linear_dropout_rate', 0.0, 0.5)\n",
    "    batch_norm = trial.suggest_categorical('batch_norm', [True, False])\n",
    "    # モデル定義\n",
    "    model = SmallVGG_3blocks(\n",
    "        in_channels=3,\n",
    "        num_classes=9,\n",
    "        conv_dropout_rate=conv_dropout_rate,\n",
    "        linear_dropout_rate=linear_dropout_rate,\n",
    "        batch_norm=batch_norm,\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate,  momentum=0.9)\n",
    "    scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "    # モデル保存用ディレクトリを作成（ローカル環境でも保存）\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_epoch = 0\n",
    "    best_acc = 0.0  # 最良の検証精度\n",
    "    #best_model_state = None  # モデルの状態を保存するための変数\n",
    "\n",
    "    for epoch in range(50):\n",
    "        # 訓練モード\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "            labels = labels.squeeze().long()  # ラベルを1Dに変換し、整数型にキャスト\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_acc = train_correct / len(train_loader.dataset)\n",
    "\n",
    "        # 検証モード\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in val_loader:\n",
    "                imgs, labels = imgs.to(device), labels.to(device)\n",
    "                labels = labels.squeeze().long()  # ラベルを1Dに変換\n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                val_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc = val_correct / len(val_loader.dataset)\n",
    "\n",
    "        # ベストモデルを記録\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch\n",
    "            best_acc = val_acc\n",
    "            \n",
    "            # トライアル番号を使ったモデル保存\n",
    "            model_path = f\"models/trial_{study_name}_{trial.number}.pth\"\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "            # モデルパスや関連情報をトライアルに記録\n",
    "            trial.set_user_attr(\"model_path\", model_path)\n",
    "            trial.set_user_attr(\"best_val_loss\", best_val_loss)\n",
    "            trial.set_user_attr(\"best_epoch\", best_epoch)\n",
    "\n",
    "        # 学習率スケジューラを更新\n",
    "        scheduler.step()\n",
    "\n",
    "        # 進捗報告（トライアルの進行状況を追跡し、プルーニングするかどうかを判断）\n",
    "        trial.report(val_loss, epoch) # スカラー値のみ\n",
    "\n",
    "        # プルーニング判定（現在のトライアルが有望でないと判断した場合、トライアルを途中で終了）\n",
    "        if trial.should_prune():\n",
    "          raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        print(f\"Epoch {epoch} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ed0d95a1-3552-4212-9941-c096a1909d2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 04:04:28,382] Using an existing study with name '250129_PathMNIST_3blocks' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Train Loss: 0.0154, Train Acc: 0.2249, Val Loss: 0.0122, Val Acc: 0.3513\n",
      "Epoch 1 - Train Loss: 0.0088, Train Acc: 0.5541, Val Loss: 0.0074, Val Acc: 0.5970\n",
      "Epoch 2 - Train Loss: 0.0063, Train Acc: 0.6883, Val Loss: 0.0052, Val Acc: 0.7546\n",
      "Epoch 3 - Train Loss: 0.0045, Train Acc: 0.7875, Val Loss: 0.0036, Val Acc: 0.8294\n",
      "Epoch 4 - Train Loss: 0.0036, Train Acc: 0.8296, Val Loss: 0.0034, Val Acc: 0.8406\n",
      "Epoch 5 - Train Loss: 0.0028, Train Acc: 0.8665, Val Loss: 0.0025, Val Acc: 0.8815\n",
      "Epoch 6 - Train Loss: 0.0025, Train Acc: 0.8814, Val Loss: 0.0023, Val Acc: 0.8909\n",
      "Epoch 7 - Train Loss: 0.0023, Train Acc: 0.8924, Val Loss: 0.0026, Val Acc: 0.8809\n",
      "Epoch 8 - Train Loss: 0.0021, Train Acc: 0.9020, Val Loss: 0.0019, Val Acc: 0.9130\n",
      "Epoch 9 - Train Loss: 0.0020, Train Acc: 0.9095, Val Loss: 0.0018, Val Acc: 0.9175\n",
      "Epoch 10 - Train Loss: 0.0016, Train Acc: 0.9283, Val Loss: 0.0018, Val Acc: 0.9203\n",
      "Epoch 11 - Train Loss: 0.0015, Train Acc: 0.9331, Val Loss: 0.0015, Val Acc: 0.9358\n",
      "Epoch 12 - Train Loss: 0.0014, Train Acc: 0.9351, Val Loss: 0.0015, Val Acc: 0.9369\n",
      "Epoch 13 - Train Loss: 0.0014, Train Acc: 0.9373, Val Loss: 0.0015, Val Acc: 0.9318\n",
      "Epoch 14 - Train Loss: 0.0013, Train Acc: 0.9407, Val Loss: 0.0013, Val Acc: 0.9447\n",
      "Epoch 15 - Train Loss: 0.0012, Train Acc: 0.9477, Val Loss: 0.0012, Val Acc: 0.9487\n",
      "Epoch 16 - Train Loss: 0.0011, Train Acc: 0.9504, Val Loss: 0.0012, Val Acc: 0.9470\n",
      "Epoch 17 - Train Loss: 0.0011, Train Acc: 0.9510, Val Loss: 0.0012, Val Acc: 0.9456\n",
      "Epoch 18 - Train Loss: 0.0011, Train Acc: 0.9522, Val Loss: 0.0011, Val Acc: 0.9513\n",
      "Epoch 19 - Train Loss: 0.0011, Train Acc: 0.9531, Val Loss: 0.0011, Val Acc: 0.9504\n",
      "Epoch 20 - Train Loss: 0.0010, Train Acc: 0.9570, Val Loss: 0.0010, Val Acc: 0.9559\n",
      "Epoch 21 - Train Loss: 0.0010, Train Acc: 0.9570, Val Loss: 0.0010, Val Acc: 0.9557\n",
      "Epoch 22 - Train Loss: 0.0010, Train Acc: 0.9582, Val Loss: 0.0010, Val Acc: 0.9571\n",
      "Epoch 23 - Train Loss: 0.0009, Train Acc: 0.9585, Val Loss: 0.0010, Val Acc: 0.9556\n",
      "Epoch 24 - Train Loss: 0.0009, Train Acc: 0.9590, Val Loss: 0.0010, Val Acc: 0.9566\n",
      "Epoch 25 - Train Loss: 0.0009, Train Acc: 0.9610, Val Loss: 0.0010, Val Acc: 0.9587\n",
      "Epoch 26 - Train Loss: 0.0009, Train Acc: 0.9609, Val Loss: 0.0009, Val Acc: 0.9588\n",
      "Epoch 27 - Train Loss: 0.0009, Train Acc: 0.9618, Val Loss: 0.0009, Val Acc: 0.9595\n",
      "Epoch 28 - Train Loss: 0.0009, Train Acc: 0.9621, Val Loss: 0.0009, Val Acc: 0.9606\n",
      "Epoch 29 - Train Loss: 0.0009, Train Acc: 0.9626, Val Loss: 0.0009, Val Acc: 0.9615\n",
      "Epoch 30 - Train Loss: 0.0008, Train Acc: 0.9630, Val Loss: 0.0009, Val Acc: 0.9623\n",
      "Epoch 31 - Train Loss: 0.0008, Train Acc: 0.9632, Val Loss: 0.0009, Val Acc: 0.9613\n",
      "Epoch 32 - Train Loss: 0.0008, Train Acc: 0.9637, Val Loss: 0.0009, Val Acc: 0.9599\n",
      "Epoch 33 - Train Loss: 0.0008, Train Acc: 0.9642, Val Loss: 0.0009, Val Acc: 0.9602\n",
      "Epoch 34 - Train Loss: 0.0008, Train Acc: 0.9643, Val Loss: 0.0009, Val Acc: 0.9623\n",
      "Epoch 35 - Train Loss: 0.0008, Train Acc: 0.9639, Val Loss: 0.0009, Val Acc: 0.9618\n",
      "Epoch 36 - Train Loss: 0.0008, Train Acc: 0.9639, Val Loss: 0.0009, Val Acc: 0.9621\n",
      "Epoch 37 - Train Loss: 0.0008, Train Acc: 0.9653, Val Loss: 0.0009, Val Acc: 0.9622\n",
      "Epoch 38 - Train Loss: 0.0008, Train Acc: 0.9646, Val Loss: 0.0009, Val Acc: 0.9618\n",
      "Epoch 39 - Train Loss: 0.0008, Train Acc: 0.9647, Val Loss: 0.0009, Val Acc: 0.9624\n",
      "Epoch 40 - Train Loss: 0.0008, Train Acc: 0.9648, Val Loss: 0.0009, Val Acc: 0.9622\n",
      "Epoch 41 - Train Loss: 0.0008, Train Acc: 0.9652, Val Loss: 0.0009, Val Acc: 0.9616\n",
      "Epoch 42 - Train Loss: 0.0008, Train Acc: 0.9645, Val Loss: 0.0009, Val Acc: 0.9628\n",
      "Epoch 43 - Train Loss: 0.0008, Train Acc: 0.9651, Val Loss: 0.0009, Val Acc: 0.9623\n",
      "Epoch 44 - Train Loss: 0.0008, Train Acc: 0.9652, Val Loss: 0.0009, Val Acc: 0.9618\n",
      "Epoch 45 - Train Loss: 0.0008, Train Acc: 0.9652, Val Loss: 0.0009, Val Acc: 0.9625\n",
      "Epoch 46 - Train Loss: 0.0008, Train Acc: 0.9652, Val Loss: 0.0009, Val Acc: 0.9617\n",
      "Epoch 47 - Train Loss: 0.0008, Train Acc: 0.9653, Val Loss: 0.0009, Val Acc: 0.9623\n",
      "Epoch 48 - Train Loss: 0.0008, Train Acc: 0.9653, Val Loss: 0.0009, Val Acc: 0.9624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 04:12:28,813] Trial 1 finished with value: 0.000881636798827172 and parameters: {'learning_rate': 0.009360903533567054, 'conv_dropout_rate': 0.04368114029983541, 'linear_dropout_rate': 0.122054568719, 'batch_norm': False}. Best is trial 1 with value: 0.000881636798827172.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Train Loss: 0.0008, Train Acc: 0.9649, Val Loss: 0.0009, Val Acc: 0.9629\n",
      "Epoch 0 - Train Loss: 0.0147, Train Acc: 0.2619, Val Loss: 0.0105, Val Acc: 0.4708\n",
      "Epoch 1 - Train Loss: 0.0085, Train Acc: 0.5796, Val Loss: 0.0074, Val Acc: 0.6442\n",
      "Epoch 2 - Train Loss: 0.0063, Train Acc: 0.6935, Val Loss: 0.0059, Val Acc: 0.7181\n",
      "Epoch 3 - Train Loss: 0.0049, Train Acc: 0.7632, Val Loss: 0.0044, Val Acc: 0.7929\n",
      "Epoch 4 - Train Loss: 0.0042, Train Acc: 0.8034, Val Loss: 0.0039, Val Acc: 0.8116\n",
      "Epoch 5 - Train Loss: 0.0034, Train Acc: 0.8409, Val Loss: 0.0031, Val Acc: 0.8563\n",
      "Epoch 6 - Train Loss: 0.0031, Train Acc: 0.8534, Val Loss: 0.0029, Val Acc: 0.8634\n",
      "Epoch 7 - Train Loss: 0.0030, Train Acc: 0.8607, Val Loss: 0.0035, Val Acc: 0.8370\n",
      "Epoch 8 - Train Loss: 0.0028, Train Acc: 0.8695, Val Loss: 0.0025, Val Acc: 0.8830\n",
      "Epoch 9 - Train Loss: 0.0027, Train Acc: 0.8761, Val Loss: 0.0022, Val Acc: 0.8965\n",
      "Epoch 10 - Train Loss: 0.0023, Train Acc: 0.8944, Val Loss: 0.0020, Val Acc: 0.9064\n",
      "Epoch 11 - Train Loss: 0.0022, Train Acc: 0.8980, Val Loss: 0.0020, Val Acc: 0.9124\n",
      "Epoch 12 - Train Loss: 0.0021, Train Acc: 0.9018, Val Loss: 0.0021, Val Acc: 0.9035\n",
      "Epoch 13 - Train Loss: 0.0021, Train Acc: 0.9043, Val Loss: 0.0020, Val Acc: 0.9055\n",
      "Epoch 14 - Train Loss: 0.0020, Train Acc: 0.9064, Val Loss: 0.0023, Val Acc: 0.8960\n",
      "Epoch 15 - Train Loss: 0.0019, Train Acc: 0.9140, Val Loss: 0.0018, Val Acc: 0.9171\n",
      "Epoch 16 - Train Loss: 0.0018, Train Acc: 0.9169, Val Loss: 0.0017, Val Acc: 0.9225\n",
      "Epoch 17 - Train Loss: 0.0018, Train Acc: 0.9182, Val Loss: 0.0018, Val Acc: 0.9171\n",
      "Epoch 18 - Train Loss: 0.0018, Train Acc: 0.9190, Val Loss: 0.0017, Val Acc: 0.9244\n",
      "Epoch 19 - Train Loss: 0.0018, Train Acc: 0.9199, Val Loss: 0.0018, Val Acc: 0.9199\n",
      "Epoch 20 - Train Loss: 0.0017, Train Acc: 0.9246, Val Loss: 0.0016, Val Acc: 0.9276\n",
      "Epoch 21 - Train Loss: 0.0017, Train Acc: 0.9263, Val Loss: 0.0017, Val Acc: 0.9234\n",
      "Epoch 22 - Train Loss: 0.0017, Train Acc: 0.9245, Val Loss: 0.0016, Val Acc: 0.9267\n",
      "Epoch 23 - Train Loss: 0.0017, Train Acc: 0.9257, Val Loss: 0.0016, Val Acc: 0.9286\n",
      "Epoch 24 - Train Loss: 0.0017, Train Acc: 0.9258, Val Loss: 0.0017, Val Acc: 0.9223\n",
      "Epoch 25 - Train Loss: 0.0016, Train Acc: 0.9276, Val Loss: 0.0016, Val Acc: 0.9284\n",
      "Epoch 26 - Train Loss: 0.0016, Train Acc: 0.9279, Val Loss: 0.0017, Val Acc: 0.9249\n",
      "Epoch 27 - Train Loss: 0.0016, Train Acc: 0.9283, Val Loss: 0.0015, Val Acc: 0.9309\n",
      "Epoch 28 - Train Loss: 0.0016, Train Acc: 0.9284, Val Loss: 0.0016, Val Acc: 0.9267\n",
      "Epoch 29 - Train Loss: 0.0016, Train Acc: 0.9291, Val Loss: 0.0016, Val Acc: 0.9277\n",
      "Epoch 30 - Train Loss: 0.0016, Train Acc: 0.9303, Val Loss: 0.0016, Val Acc: 0.9281\n",
      "Epoch 31 - Train Loss: 0.0016, Train Acc: 0.9305, Val Loss: 0.0016, Val Acc: 0.9259\n",
      "Epoch 32 - Train Loss: 0.0016, Train Acc: 0.9300, Val Loss: 0.0016, Val Acc: 0.9276\n",
      "Epoch 33 - Train Loss: 0.0016, Train Acc: 0.9301, Val Loss: 0.0016, Val Acc: 0.9300\n",
      "Epoch 34 - Train Loss: 0.0016, Train Acc: 0.9295, Val Loss: 0.0016, Val Acc: 0.9272\n",
      "Epoch 35 - Train Loss: 0.0015, Train Acc: 0.9310, Val Loss: 0.0016, Val Acc: 0.9305\n",
      "Epoch 36 - Train Loss: 0.0015, Train Acc: 0.9315, Val Loss: 0.0016, Val Acc: 0.9290\n",
      "Epoch 37 - Train Loss: 0.0016, Train Acc: 0.9297, Val Loss: 0.0016, Val Acc: 0.9299\n",
      "Epoch 38 - Train Loss: 0.0015, Train Acc: 0.9302, Val Loss: 0.0015, Val Acc: 0.9310\n",
      "Epoch 39 - Train Loss: 0.0015, Train Acc: 0.9315, Val Loss: 0.0016, Val Acc: 0.9306\n",
      "Epoch 40 - Train Loss: 0.0015, Train Acc: 0.9319, Val Loss: 0.0016, Val Acc: 0.9286\n",
      "Epoch 41 - Train Loss: 0.0015, Train Acc: 0.9316, Val Loss: 0.0015, Val Acc: 0.9305\n",
      "Epoch 42 - Train Loss: 0.0015, Train Acc: 0.9316, Val Loss: 0.0016, Val Acc: 0.9300\n",
      "Epoch 43 - Train Loss: 0.0015, Train Acc: 0.9310, Val Loss: 0.0015, Val Acc: 0.9307\n",
      "Epoch 44 - Train Loss: 0.0015, Train Acc: 0.9317, Val Loss: 0.0016, Val Acc: 0.9305\n",
      "Epoch 45 - Train Loss: 0.0015, Train Acc: 0.9310, Val Loss: 0.0016, Val Acc: 0.9299\n",
      "Epoch 46 - Train Loss: 0.0015, Train Acc: 0.9311, Val Loss: 0.0016, Val Acc: 0.9302\n",
      "Epoch 47 - Train Loss: 0.0015, Train Acc: 0.9314, Val Loss: 0.0016, Val Acc: 0.9306\n",
      "Epoch 48 - Train Loss: 0.0015, Train Acc: 0.9320, Val Loss: 0.0016, Val Acc: 0.9300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 04:20:28,226] Trial 2 finished with value: 0.0015400007504169963 and parameters: {'learning_rate': 0.008848414613241411, 'conv_dropout_rate': 0.29806335832232844, 'linear_dropout_rate': 0.22434229708277564, 'batch_norm': False}. Best is trial 1 with value: 0.000881636798827172.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Train Loss: 0.0015, Train Acc: 0.9323, Val Loss: 0.0016, Val Acc: 0.9300\n",
      "Epoch 0 - Train Loss: 0.0171, Train Acc: 0.1407, Val Loss: 0.0172, Val Acc: 0.1431\n",
      "Epoch 1 - Train Loss: 0.0130, Train Acc: 0.3556, Val Loss: 0.0114, Val Acc: 0.4427\n",
      "Epoch 2 - Train Loss: 0.0089, Train Acc: 0.5441, Val Loss: 0.0080, Val Acc: 0.5975\n",
      "Epoch 3 - Train Loss: 0.0078, Train Acc: 0.6038, Val Loss: 0.0069, Val Acc: 0.6525\n",
      "Epoch 4 - Train Loss: 0.0067, Train Acc: 0.6607, Val Loss: 0.0068, Val Acc: 0.6568\n",
      "Epoch 5 - Train Loss: 0.0058, Train Acc: 0.7064, Val Loss: 0.0055, Val Acc: 0.7192\n",
      "Epoch 6 - Train Loss: 0.0053, Train Acc: 0.7290, Val Loss: 0.0050, Val Acc: 0.7514\n",
      "Epoch 7 - Train Loss: 0.0049, Train Acc: 0.7581, Val Loss: 0.0043, Val Acc: 0.7949\n",
      "Epoch 8 - Train Loss: 0.0045, Train Acc: 0.7833, Val Loss: 0.0045, Val Acc: 0.7835\n",
      "Epoch 9 - Train Loss: 0.0043, Train Acc: 0.7992, Val Loss: 0.0039, Val Acc: 0.8155\n",
      "Epoch 10 - Train Loss: 0.0038, Train Acc: 0.8215, Val Loss: 0.0033, Val Acc: 0.8471\n",
      "Epoch 11 - Train Loss: 0.0037, Train Acc: 0.8288, Val Loss: 0.0040, Val Acc: 0.8075\n",
      "Epoch 12 - Train Loss: 0.0035, Train Acc: 0.8358, Val Loss: 0.0033, Val Acc: 0.8496\n",
      "Epoch 13 - Train Loss: 0.0034, Train Acc: 0.8416, Val Loss: 0.0034, Val Acc: 0.8435\n",
      "Epoch 14 - Train Loss: 0.0033, Train Acc: 0.8474, Val Loss: 0.0033, Val Acc: 0.8516\n",
      "Epoch 15 - Train Loss: 0.0030, Train Acc: 0.8602, Val Loss: 0.0027, Val Acc: 0.8788\n",
      "Epoch 16 - Train Loss: 0.0029, Train Acc: 0.8643, Val Loss: 0.0026, Val Acc: 0.8839\n",
      "Epoch 17 - Train Loss: 0.0029, Train Acc: 0.8678, Val Loss: 0.0027, Val Acc: 0.8770\n",
      "Epoch 18 - Train Loss: 0.0028, Train Acc: 0.8722, Val Loss: 0.0025, Val Acc: 0.8894\n",
      "Epoch 19 - Train Loss: 0.0028, Train Acc: 0.8750, Val Loss: 0.0026, Val Acc: 0.8882\n",
      "Epoch 20 - Train Loss: 0.0026, Train Acc: 0.8807, Val Loss: 0.0025, Val Acc: 0.8850\n",
      "Epoch 21 - Train Loss: 0.0026, Train Acc: 0.8811, Val Loss: 0.0025, Val Acc: 0.8905\n",
      "Epoch 22 - Train Loss: 0.0026, Train Acc: 0.8832, Val Loss: 0.0023, Val Acc: 0.8973\n",
      "Epoch 23 - Train Loss: 0.0025, Train Acc: 0.8853, Val Loss: 0.0024, Val Acc: 0.8940\n",
      "Epoch 24 - Train Loss: 0.0025, Train Acc: 0.8855, Val Loss: 0.0024, Val Acc: 0.8931\n",
      "Epoch 25 - Train Loss: 0.0025, Train Acc: 0.8888, Val Loss: 0.0023, Val Acc: 0.8968\n",
      "Epoch 26 - Train Loss: 0.0024, Train Acc: 0.8890, Val Loss: 0.0023, Val Acc: 0.8979\n",
      "Epoch 27 - Train Loss: 0.0024, Train Acc: 0.8910, Val Loss: 0.0023, Val Acc: 0.9022\n",
      "Epoch 28 - Train Loss: 0.0024, Train Acc: 0.8919, Val Loss: 0.0023, Val Acc: 0.8999\n",
      "Epoch 29 - Train Loss: 0.0024, Train Acc: 0.8909, Val Loss: 0.0022, Val Acc: 0.9042\n",
      "Epoch 30 - Train Loss: 0.0024, Train Acc: 0.8927, Val Loss: 0.0022, Val Acc: 0.9035\n",
      "Epoch 31 - Train Loss: 0.0024, Train Acc: 0.8929, Val Loss: 0.0022, Val Acc: 0.8996\n",
      "Epoch 32 - Train Loss: 0.0024, Train Acc: 0.8933, Val Loss: 0.0022, Val Acc: 0.9017\n",
      "Epoch 33 - Train Loss: 0.0024, Train Acc: 0.8931, Val Loss: 0.0022, Val Acc: 0.9034\n",
      "Epoch 34 - Train Loss: 0.0024, Train Acc: 0.8941, Val Loss: 0.0023, Val Acc: 0.8989\n",
      "Epoch 35 - Train Loss: 0.0023, Train Acc: 0.8948, Val Loss: 0.0022, Val Acc: 0.9042\n",
      "Epoch 36 - Train Loss: 0.0023, Train Acc: 0.8960, Val Loss: 0.0022, Val Acc: 0.9031\n",
      "Epoch 37 - Train Loss: 0.0023, Train Acc: 0.8954, Val Loss: 0.0022, Val Acc: 0.9040\n",
      "Epoch 38 - Train Loss: 0.0023, Train Acc: 0.8947, Val Loss: 0.0022, Val Acc: 0.9053\n",
      "Epoch 39 - Train Loss: 0.0023, Train Acc: 0.8958, Val Loss: 0.0022, Val Acc: 0.9038\n",
      "Epoch 40 - Train Loss: 0.0023, Train Acc: 0.8950, Val Loss: 0.0022, Val Acc: 0.9032\n",
      "Epoch 41 - Train Loss: 0.0023, Train Acc: 0.8967, Val Loss: 0.0022, Val Acc: 0.9062\n",
      "Epoch 42 - Train Loss: 0.0023, Train Acc: 0.8959, Val Loss: 0.0022, Val Acc: 0.9043\n",
      "Epoch 43 - Train Loss: 0.0023, Train Acc: 0.8950, Val Loss: 0.0022, Val Acc: 0.9037\n",
      "Epoch 44 - Train Loss: 0.0023, Train Acc: 0.8953, Val Loss: 0.0022, Val Acc: 0.9051\n",
      "Epoch 45 - Train Loss: 0.0023, Train Acc: 0.8961, Val Loss: 0.0022, Val Acc: 0.9045\n",
      "Epoch 46 - Train Loss: 0.0023, Train Acc: 0.8958, Val Loss: 0.0022, Val Acc: 0.9048\n",
      "Epoch 47 - Train Loss: 0.0023, Train Acc: 0.8964, Val Loss: 0.0022, Val Acc: 0.9050\n",
      "Epoch 48 - Train Loss: 0.0023, Train Acc: 0.8962, Val Loss: 0.0022, Val Acc: 0.9054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 04:28:26,696] Trial 3 finished with value: 0.0021577930403370612 and parameters: {'learning_rate': 0.0036942420778976162, 'conv_dropout_rate': 0.2027571284944024, 'linear_dropout_rate': 0.46639228742007266, 'batch_norm': False}. Best is trial 1 with value: 0.000881636798827172.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Train Loss: 0.0023, Train Acc: 0.8970, Val Loss: 0.0022, Val Acc: 0.9045\n",
      "Epoch 0 - Train Loss: 0.0149, Train Acc: 0.2504, Val Loss: 0.0109, Val Acc: 0.4465\n",
      "Epoch 1 - Train Loss: 0.0085, Train Acc: 0.5724, Val Loss: 0.0079, Val Acc: 0.6167\n",
      "Epoch 2 - Train Loss: 0.0065, Train Acc: 0.6805, Val Loss: 0.0056, Val Acc: 0.7166\n",
      "Epoch 3 - Train Loss: 0.0051, Train Acc: 0.7493, Val Loss: 0.0046, Val Acc: 0.7730\n",
      "Epoch 4 - Train Loss: 0.0043, Train Acc: 0.7988, Val Loss: 0.0037, Val Acc: 0.8185\n",
      "Epoch 5 - Train Loss: 0.0034, Train Acc: 0.8374, Val Loss: 0.0031, Val Acc: 0.8579\n",
      "Epoch 6 - Train Loss: 0.0032, Train Acc: 0.8506, Val Loss: 0.0030, Val Acc: 0.8538\n",
      "Epoch 7 - Train Loss: 0.0030, Train Acc: 0.8598, Val Loss: 0.0033, Val Acc: 0.8386\n",
      "Epoch 8 - Train Loss: 0.0028, Train Acc: 0.8715, Val Loss: 0.0026, Val Acc: 0.8751\n",
      "Epoch 9 - Train Loss: 0.0026, Train Acc: 0.8780, Val Loss: 0.0022, Val Acc: 0.9008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 04:30:11,351] Trial 4 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Train Loss: 0.0065, Train Acc: 0.6877, Val Loss: 0.0052, Val Acc: 0.7547\n",
      "Epoch 1 - Train Loss: 0.0038, Train Acc: 0.8262, Val Loss: 0.0081, Val Acc: 0.6716\n",
      "Epoch 2 - Train Loss: 0.0030, Train Acc: 0.8667, Val Loss: 0.0031, Val Acc: 0.8650\n",
      "Epoch 3 - Train Loss: 0.0024, Train Acc: 0.8902, Val Loss: 0.0037, Val Acc: 0.8384\n",
      "Epoch 4 - Train Loss: 0.0021, Train Acc: 0.9055, Val Loss: 0.0028, Val Acc: 0.8786\n",
      "Epoch 5 - Train Loss: 0.0017, Train Acc: 0.9239, Val Loss: 0.0015, Val Acc: 0.9345\n",
      "Epoch 6 - Train Loss: 0.0016, Train Acc: 0.9287, Val Loss: 0.0015, Val Acc: 0.9343\n",
      "Epoch 7 - Train Loss: 0.0015, Train Acc: 0.9329, Val Loss: 0.0014, Val Acc: 0.9380\n",
      "Epoch 8 - Train Loss: 0.0015, Train Acc: 0.9355, Val Loss: 0.0014, Val Acc: 0.9362\n",
      "Epoch 9 - Train Loss: 0.0014, Train Acc: 0.9389, Val Loss: 0.0011, Val Acc: 0.9512\n",
      "Epoch 10 - Train Loss: 0.0012, Train Acc: 0.9454, Val Loss: 0.0011, Val Acc: 0.9546\n",
      "Epoch 11 - Train Loss: 0.0012, Train Acc: 0.9474, Val Loss: 0.0010, Val Acc: 0.9543\n",
      "Epoch 12 - Train Loss: 0.0012, Train Acc: 0.9480, Val Loss: 0.0009, Val Acc: 0.9614\n",
      "Epoch 13 - Train Loss: 0.0011, Train Acc: 0.9491, Val Loss: 0.0012, Val Acc: 0.9468\n",
      "Epoch 14 - Train Loss: 0.0011, Train Acc: 0.9516, Val Loss: 0.0014, Val Acc: 0.9429\n",
      "Epoch 15 - Train Loss: 0.0011, Train Acc: 0.9533, Val Loss: 0.0009, Val Acc: 0.9627\n",
      "Epoch 16 - Train Loss: 0.0010, Train Acc: 0.9543, Val Loss: 0.0010, Val Acc: 0.9586\n",
      "Epoch 17 - Train Loss: 0.0010, Train Acc: 0.9549, Val Loss: 0.0009, Val Acc: 0.9630\n",
      "Epoch 18 - Train Loss: 0.0010, Train Acc: 0.9552, Val Loss: 0.0010, Val Acc: 0.9568\n",
      "Epoch 19 - Train Loss: 0.0010, Train Acc: 0.9559, Val Loss: 0.0008, Val Acc: 0.9654\n",
      "Epoch 20 - Train Loss: 0.0010, Train Acc: 0.9577, Val Loss: 0.0008, Val Acc: 0.9655\n",
      "Epoch 21 - Train Loss: 0.0009, Train Acc: 0.9580, Val Loss: 0.0008, Val Acc: 0.9657\n",
      "Epoch 22 - Train Loss: 0.0009, Train Acc: 0.9583, Val Loss: 0.0008, Val Acc: 0.9638\n",
      "Epoch 23 - Train Loss: 0.0009, Train Acc: 0.9582, Val Loss: 0.0008, Val Acc: 0.9643\n",
      "Epoch 24 - Train Loss: 0.0009, Train Acc: 0.9605, Val Loss: 0.0008, Val Acc: 0.9659\n",
      "Epoch 25 - Train Loss: 0.0009, Train Acc: 0.9596, Val Loss: 0.0008, Val Acc: 0.9662\n",
      "Epoch 26 - Train Loss: 0.0009, Train Acc: 0.9594, Val Loss: 0.0008, Val Acc: 0.9672\n",
      "Epoch 27 - Train Loss: 0.0009, Train Acc: 0.9598, Val Loss: 0.0008, Val Acc: 0.9659\n",
      "Epoch 28 - Train Loss: 0.0009, Train Acc: 0.9598, Val Loss: 0.0008, Val Acc: 0.9665\n",
      "Epoch 29 - Train Loss: 0.0009, Train Acc: 0.9617, Val Loss: 0.0008, Val Acc: 0.9669\n",
      "Epoch 30 - Train Loss: 0.0009, Train Acc: 0.9613, Val Loss: 0.0008, Val Acc: 0.9675\n",
      "Epoch 31 - Train Loss: 0.0009, Train Acc: 0.9612, Val Loss: 0.0008, Val Acc: 0.9669\n",
      "Epoch 32 - Train Loss: 0.0009, Train Acc: 0.9615, Val Loss: 0.0008, Val Acc: 0.9676\n",
      "Epoch 33 - Train Loss: 0.0009, Train Acc: 0.9609, Val Loss: 0.0008, Val Acc: 0.9659\n",
      "Epoch 34 - Train Loss: 0.0009, Train Acc: 0.9615, Val Loss: 0.0008, Val Acc: 0.9671\n",
      "Epoch 35 - Train Loss: 0.0009, Train Acc: 0.9611, Val Loss: 0.0008, Val Acc: 0.9675\n",
      "Epoch 36 - Train Loss: 0.0009, Train Acc: 0.9612, Val Loss: 0.0008, Val Acc: 0.9673\n",
      "Epoch 37 - Train Loss: 0.0009, Train Acc: 0.9625, Val Loss: 0.0008, Val Acc: 0.9661\n",
      "Epoch 38 - Train Loss: 0.0009, Train Acc: 0.9619, Val Loss: 0.0008, Val Acc: 0.9685\n",
      "Epoch 39 - Train Loss: 0.0009, Train Acc: 0.9617, Val Loss: 0.0007, Val Acc: 0.9680\n",
      "Epoch 40 - Train Loss: 0.0009, Train Acc: 0.9602, Val Loss: 0.0007, Val Acc: 0.9683\n",
      "Epoch 41 - Train Loss: 0.0009, Train Acc: 0.9618, Val Loss: 0.0007, Val Acc: 0.9693\n",
      "Epoch 42 - Train Loss: 0.0009, Train Acc: 0.9613, Val Loss: 0.0008, Val Acc: 0.9679\n",
      "Epoch 43 - Train Loss: 0.0009, Train Acc: 0.9617, Val Loss: 0.0008, Val Acc: 0.9677\n",
      "Epoch 44 - Train Loss: 0.0009, Train Acc: 0.9622, Val Loss: 0.0007, Val Acc: 0.9708\n",
      "Epoch 45 - Train Loss: 0.0009, Train Acc: 0.9616, Val Loss: 0.0008, Val Acc: 0.9678\n",
      "Epoch 46 - Train Loss: 0.0009, Train Acc: 0.9623, Val Loss: 0.0008, Val Acc: 0.9658\n",
      "Epoch 47 - Train Loss: 0.0009, Train Acc: 0.9619, Val Loss: 0.0008, Val Acc: 0.9669\n",
      "Epoch 48 - Train Loss: 0.0009, Train Acc: 0.9617, Val Loss: 0.0007, Val Acc: 0.9680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 04:38:25,710] Trial 5 finished with value: 0.0006948431542494008 and parameters: {'learning_rate': 0.006337645995721278, 'conv_dropout_rate': 0.27237790704596837, 'linear_dropout_rate': 0.2857206868287266, 'batch_norm': True}. Best is trial 5 with value: 0.0006948431542494008.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Train Loss: 0.0009, Train Acc: 0.9616, Val Loss: 0.0008, Val Acc: 0.9677\n",
      "Epoch 0 - Train Loss: 0.0054, Train Acc: 0.7438, Val Loss: 0.0055, Val Acc: 0.7657\n",
      "Epoch 1 - Train Loss: 0.0029, Train Acc: 0.8709, Val Loss: 0.0104, Val Acc: 0.6885\n",
      "Epoch 2 - Train Loss: 0.0021, Train Acc: 0.9076, Val Loss: 0.0024, Val Acc: 0.8924\n",
      "Epoch 3 - Train Loss: 0.0017, Train Acc: 0.9265, Val Loss: 0.0047, Val Acc: 0.8429\n",
      "Epoch 4 - Train Loss: 0.0014, Train Acc: 0.9379, Val Loss: 0.0014, Val Acc: 0.9386\n",
      "Epoch 5 - Train Loss: 0.0010, Train Acc: 0.9548, Val Loss: 0.0009, Val Acc: 0.9636\n",
      "Epoch 6 - Train Loss: 0.0010, Train Acc: 0.9579, Val Loss: 0.0008, Val Acc: 0.9662\n",
      "Epoch 7 - Train Loss: 0.0009, Train Acc: 0.9599, Val Loss: 0.0013, Val Acc: 0.9471\n",
      "Epoch 8 - Train Loss: 0.0009, Train Acc: 0.9599, Val Loss: 0.0008, Val Acc: 0.9670\n",
      "Epoch 9 - Train Loss: 0.0008, Train Acc: 0.9647, Val Loss: 0.0008, Val Acc: 0.9655\n",
      "Epoch 10 - Train Loss: 0.0007, Train Acc: 0.9708, Val Loss: 0.0006, Val Acc: 0.9758\n",
      "Epoch 11 - Train Loss: 0.0006, Train Acc: 0.9722, Val Loss: 0.0006, Val Acc: 0.9763\n",
      "Epoch 12 - Train Loss: 0.0006, Train Acc: 0.9727, Val Loss: 0.0006, Val Acc: 0.9768\n",
      "Epoch 13 - Train Loss: 0.0006, Train Acc: 0.9721, Val Loss: 0.0006, Val Acc: 0.9760\n",
      "Epoch 14 - Train Loss: 0.0006, Train Acc: 0.9743, Val Loss: 0.0006, Val Acc: 0.9768\n",
      "Epoch 15 - Train Loss: 0.0005, Train Acc: 0.9770, Val Loss: 0.0005, Val Acc: 0.9814\n",
      "Epoch 16 - Train Loss: 0.0005, Train Acc: 0.9773, Val Loss: 0.0005, Val Acc: 0.9783\n",
      "Epoch 17 - Train Loss: 0.0005, Train Acc: 0.9777, Val Loss: 0.0005, Val Acc: 0.9803\n",
      "Epoch 18 - Train Loss: 0.0005, Train Acc: 0.9783, Val Loss: 0.0005, Val Acc: 0.9794\n",
      "Epoch 19 - Train Loss: 0.0005, Train Acc: 0.9788, Val Loss: 0.0005, Val Acc: 0.9810\n",
      "Epoch 20 - Train Loss: 0.0005, Train Acc: 0.9804, Val Loss: 0.0005, Val Acc: 0.9799\n",
      "Epoch 21 - Train Loss: 0.0004, Train Acc: 0.9810, Val Loss: 0.0005, Val Acc: 0.9817\n",
      "Epoch 22 - Train Loss: 0.0004, Train Acc: 0.9804, Val Loss: 0.0004, Val Acc: 0.9813\n",
      "Epoch 23 - Train Loss: 0.0004, Train Acc: 0.9813, Val Loss: 0.0004, Val Acc: 0.9832\n",
      "Epoch 24 - Train Loss: 0.0004, Train Acc: 0.9816, Val Loss: 0.0004, Val Acc: 0.9818\n",
      "Epoch 25 - Train Loss: 0.0004, Train Acc: 0.9813, Val Loss: 0.0004, Val Acc: 0.9833\n",
      "Epoch 26 - Train Loss: 0.0004, Train Acc: 0.9815, Val Loss: 0.0004, Val Acc: 0.9834\n",
      "Epoch 27 - Train Loss: 0.0004, Train Acc: 0.9821, Val Loss: 0.0004, Val Acc: 0.9825\n",
      "Epoch 28 - Train Loss: 0.0004, Train Acc: 0.9824, Val Loss: 0.0004, Val Acc: 0.9824\n",
      "Epoch 29 - Train Loss: 0.0004, Train Acc: 0.9824, Val Loss: 0.0004, Val Acc: 0.9831\n",
      "Epoch 30 - Train Loss: 0.0004, Train Acc: 0.9825, Val Loss: 0.0004, Val Acc: 0.9817\n",
      "Epoch 31 - Train Loss: 0.0004, Train Acc: 0.9822, Val Loss: 0.0004, Val Acc: 0.9828\n",
      "Epoch 32 - Train Loss: 0.0004, Train Acc: 0.9825, Val Loss: 0.0004, Val Acc: 0.9833\n",
      "Epoch 33 - Train Loss: 0.0004, Train Acc: 0.9830, Val Loss: 0.0004, Val Acc: 0.9832\n",
      "Epoch 34 - Train Loss: 0.0004, Train Acc: 0.9831, Val Loss: 0.0004, Val Acc: 0.9826\n",
      "Epoch 35 - Train Loss: 0.0004, Train Acc: 0.9830, Val Loss: 0.0004, Val Acc: 0.9828\n",
      "Epoch 36 - Train Loss: 0.0004, Train Acc: 0.9828, Val Loss: 0.0004, Val Acc: 0.9836\n",
      "Epoch 37 - Train Loss: 0.0004, Train Acc: 0.9831, Val Loss: 0.0004, Val Acc: 0.9828\n",
      "Epoch 38 - Train Loss: 0.0004, Train Acc: 0.9828, Val Loss: 0.0004, Val Acc: 0.9836\n",
      "Epoch 39 - Train Loss: 0.0004, Train Acc: 0.9834, Val Loss: 0.0004, Val Acc: 0.9835\n",
      "Epoch 40 - Train Loss: 0.0004, Train Acc: 0.9830, Val Loss: 0.0004, Val Acc: 0.9840\n",
      "Epoch 41 - Train Loss: 0.0004, Train Acc: 0.9834, Val Loss: 0.0004, Val Acc: 0.9835\n",
      "Epoch 42 - Train Loss: 0.0004, Train Acc: 0.9834, Val Loss: 0.0004, Val Acc: 0.9833\n",
      "Epoch 43 - Train Loss: 0.0004, Train Acc: 0.9831, Val Loss: 0.0004, Val Acc: 0.9841\n",
      "Epoch 44 - Train Loss: 0.0004, Train Acc: 0.9826, Val Loss: 0.0004, Val Acc: 0.9838\n",
      "Epoch 45 - Train Loss: 0.0004, Train Acc: 0.9833, Val Loss: 0.0004, Val Acc: 0.9837\n",
      "Epoch 46 - Train Loss: 0.0004, Train Acc: 0.9833, Val Loss: 0.0004, Val Acc: 0.9829\n",
      "Epoch 47 - Train Loss: 0.0004, Train Acc: 0.9833, Val Loss: 0.0004, Val Acc: 0.9820\n",
      "Epoch 48 - Train Loss: 0.0004, Train Acc: 0.9835, Val Loss: 0.0004, Val Acc: 0.9830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 04:46:38,184] Trial 6 finished with value: 0.0004020875215758292 and parameters: {'learning_rate': 0.009849726676666785, 'conv_dropout_rate': 0.11372702842743837, 'linear_dropout_rate': 0.41095552409651276, 'batch_norm': True}. Best is trial 6 with value: 0.0004020875215758292.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Train Loss: 0.0004, Train Acc: 0.9841, Val Loss: 0.0004, Val Acc: 0.9838\n",
      "Epoch 0 - Train Loss: 0.0111, Train Acc: 0.4811, Val Loss: 0.0084, Val Acc: 0.6048\n",
      "Epoch 1 - Train Loss: 0.0069, Train Acc: 0.6762, Val Loss: 0.0070, Val Acc: 0.6743\n",
      "Epoch 2 - Train Loss: 0.0058, Train Acc: 0.7298, Val Loss: 0.0061, Val Acc: 0.6933\n",
      "Epoch 3 - Train Loss: 0.0050, Train Acc: 0.7677, Val Loss: 0.0056, Val Acc: 0.7227\n",
      "Epoch 4 - Train Loss: 0.0045, Train Acc: 0.7892, Val Loss: 0.0051, Val Acc: 0.7531\n",
      "Epoch 5 - Train Loss: 0.0042, Train Acc: 0.8052, Val Loss: 0.0049, Val Acc: 0.7681\n",
      "Epoch 6 - Train Loss: 0.0040, Train Acc: 0.8137, Val Loss: 0.0048, Val Acc: 0.7777\n",
      "Epoch 7 - Train Loss: 0.0038, Train Acc: 0.8209, Val Loss: 0.0047, Val Acc: 0.7860\n",
      "Epoch 8 - Train Loss: 0.0037, Train Acc: 0.8279, Val Loss: 0.0047, Val Acc: 0.7861\n",
      "Epoch 9 - Train Loss: 0.0035, Train Acc: 0.8365, Val Loss: 0.0043, Val Acc: 0.8002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 04:48:27,114] Trial 7 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Train Loss: 0.0076, Train Acc: 0.6358, Val Loss: 0.0061, Val Acc: 0.7188\n",
      "Epoch 1 - Train Loss: 0.0045, Train Acc: 0.7935, Val Loss: 0.0159, Val Acc: 0.4915\n",
      "Epoch 2 - Train Loss: 0.0036, Train Acc: 0.8346, Val Loss: 0.0037, Val Acc: 0.8330\n",
      "Epoch 3 - Train Loss: 0.0030, Train Acc: 0.8650, Val Loss: 0.0059, Val Acc: 0.7878\n",
      "Epoch 4 - Train Loss: 0.0026, Train Acc: 0.8820, Val Loss: 0.0041, Val Acc: 0.8199\n",
      "Epoch 5 - Train Loss: 0.0021, Train Acc: 0.9031, Val Loss: 0.0018, Val Acc: 0.9228\n",
      "Epoch 6 - Train Loss: 0.0020, Train Acc: 0.9094, Val Loss: 0.0021, Val Acc: 0.9114\n",
      "Epoch 7 - Train Loss: 0.0019, Train Acc: 0.9132, Val Loss: 0.0018, Val Acc: 0.9223\n",
      "Epoch 8 - Train Loss: 0.0018, Train Acc: 0.9180, Val Loss: 0.0018, Val Acc: 0.9172\n",
      "Epoch 9 - Train Loss: 0.0018, Train Acc: 0.9215, Val Loss: 0.0015, Val Acc: 0.9325\n",
      "Epoch 10 - Train Loss: 0.0016, Train Acc: 0.9284, Val Loss: 0.0015, Val Acc: 0.9371\n",
      "Epoch 11 - Train Loss: 0.0015, Train Acc: 0.9309, Val Loss: 0.0014, Val Acc: 0.9392\n",
      "Epoch 12 - Train Loss: 0.0015, Train Acc: 0.9309, Val Loss: 0.0013, Val Acc: 0.9432\n",
      "Epoch 13 - Train Loss: 0.0015, Train Acc: 0.9328, Val Loss: 0.0014, Val Acc: 0.9378\n",
      "Epoch 14 - Train Loss: 0.0015, Train Acc: 0.9349, Val Loss: 0.0012, Val Acc: 0.9486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 04:51:05,231] Trial 8 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Train Loss: 0.0169, Train Acc: 0.1532, Val Loss: 0.0149, Val Acc: 0.2882\n",
      "Epoch 1 - Train Loss: 0.0107, Train Acc: 0.4600, Val Loss: 0.0088, Val Acc: 0.5576\n",
      "Epoch 2 - Train Loss: 0.0080, Train Acc: 0.5929, Val Loss: 0.0074, Val Acc: 0.6310\n",
      "Epoch 3 - Train Loss: 0.0066, Train Acc: 0.6719, Val Loss: 0.0059, Val Acc: 0.7018\n",
      "Epoch 4 - Train Loss: 0.0056, Train Acc: 0.7233, Val Loss: 0.0070, Val Acc: 0.6642\n",
      "Epoch 5 - Train Loss: 0.0046, Train Acc: 0.7718, Val Loss: 0.0046, Val Acc: 0.7864\n",
      "Epoch 6 - Train Loss: 0.0043, Train Acc: 0.7934, Val Loss: 0.0041, Val Acc: 0.8065\n",
      "Epoch 7 - Train Loss: 0.0040, Train Acc: 0.8130, Val Loss: 0.0040, Val Acc: 0.8111\n",
      "Epoch 8 - Train Loss: 0.0037, Train Acc: 0.8272, Val Loss: 0.0035, Val Acc: 0.8358\n",
      "Epoch 9 - Train Loss: 0.0035, Train Acc: 0.8360, Val Loss: 0.0034, Val Acc: 0.8488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 04:52:50,746] Trial 9 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Train Loss: 0.0060, Train Acc: 0.7150, Val Loss: 0.0039, Val Acc: 0.8203\n",
      "Epoch 1 - Train Loss: 0.0034, Train Acc: 0.8468, Val Loss: 0.0046, Val Acc: 0.7664\n",
      "Epoch 2 - Train Loss: 0.0026, Train Acc: 0.8853, Val Loss: 0.0031, Val Acc: 0.8685\n",
      "Epoch 3 - Train Loss: 0.0020, Train Acc: 0.9098, Val Loss: 0.0039, Val Acc: 0.8586\n",
      "Epoch 4 - Train Loss: 0.0017, Train Acc: 0.9236, Val Loss: 0.0014, Val Acc: 0.9411\n",
      "Epoch 5 - Train Loss: 0.0013, Train Acc: 0.9408, Val Loss: 0.0011, Val Acc: 0.9563\n",
      "Epoch 6 - Train Loss: 0.0013, Train Acc: 0.9444, Val Loss: 0.0012, Val Acc: 0.9488\n",
      "Epoch 7 - Train Loss: 0.0012, Train Acc: 0.9488, Val Loss: 0.0010, Val Acc: 0.9568\n",
      "Epoch 8 - Train Loss: 0.0012, Train Acc: 0.9496, Val Loss: 0.0009, Val Acc: 0.9600\n",
      "Epoch 9 - Train Loss: 0.0011, Train Acc: 0.9520, Val Loss: 0.0009, Val Acc: 0.9626\n",
      "Epoch 10 - Train Loss: 0.0009, Train Acc: 0.9594, Val Loss: 0.0008, Val Acc: 0.9657\n",
      "Epoch 11 - Train Loss: 0.0009, Train Acc: 0.9601, Val Loss: 0.0007, Val Acc: 0.9717\n",
      "Epoch 12 - Train Loss: 0.0009, Train Acc: 0.9620, Val Loss: 0.0008, Val Acc: 0.9668\n",
      "Epoch 13 - Train Loss: 0.0009, Train Acc: 0.9616, Val Loss: 0.0008, Val Acc: 0.9670\n",
      "Epoch 14 - Train Loss: 0.0008, Train Acc: 0.9635, Val Loss: 0.0008, Val Acc: 0.9679\n",
      "Epoch 15 - Train Loss: 0.0008, Train Acc: 0.9653, Val Loss: 0.0007, Val Acc: 0.9723\n",
      "Epoch 16 - Train Loss: 0.0008, Train Acc: 0.9659, Val Loss: 0.0008, Val Acc: 0.9688\n",
      "Epoch 17 - Train Loss: 0.0008, Train Acc: 0.9676, Val Loss: 0.0006, Val Acc: 0.9732\n",
      "Epoch 18 - Train Loss: 0.0007, Train Acc: 0.9670, Val Loss: 0.0008, Val Acc: 0.9671\n",
      "Epoch 19 - Train Loss: 0.0007, Train Acc: 0.9672, Val Loss: 0.0006, Val Acc: 0.9741\n",
      "Epoch 20 - Train Loss: 0.0007, Train Acc: 0.9693, Val Loss: 0.0007, Val Acc: 0.9714\n",
      "Epoch 21 - Train Loss: 0.0007, Train Acc: 0.9698, Val Loss: 0.0007, Val Acc: 0.9734\n",
      "Epoch 22 - Train Loss: 0.0007, Train Acc: 0.9691, Val Loss: 0.0006, Val Acc: 0.9750\n",
      "Epoch 23 - Train Loss: 0.0007, Train Acc: 0.9697, Val Loss: 0.0006, Val Acc: 0.9747\n",
      "Epoch 24 - Train Loss: 0.0007, Train Acc: 0.9702, Val Loss: 0.0006, Val Acc: 0.9757\n",
      "Epoch 25 - Train Loss: 0.0007, Train Acc: 0.9706, Val Loss: 0.0006, Val Acc: 0.9771\n",
      "Epoch 26 - Train Loss: 0.0007, Train Acc: 0.9709, Val Loss: 0.0006, Val Acc: 0.9742\n",
      "Epoch 27 - Train Loss: 0.0007, Train Acc: 0.9714, Val Loss: 0.0006, Val Acc: 0.9756\n",
      "Epoch 28 - Train Loss: 0.0007, Train Acc: 0.9703, Val Loss: 0.0006, Val Acc: 0.9750\n",
      "Epoch 29 - Train Loss: 0.0006, Train Acc: 0.9714, Val Loss: 0.0005, Val Acc: 0.9770\n",
      "Epoch 30 - Train Loss: 0.0006, Train Acc: 0.9722, Val Loss: 0.0006, Val Acc: 0.9757\n",
      "Epoch 31 - Train Loss: 0.0006, Train Acc: 0.9716, Val Loss: 0.0006, Val Acc: 0.9765\n",
      "Epoch 32 - Train Loss: 0.0007, Train Acc: 0.9715, Val Loss: 0.0006, Val Acc: 0.9754\n",
      "Epoch 33 - Train Loss: 0.0006, Train Acc: 0.9719, Val Loss: 0.0006, Val Acc: 0.9756\n",
      "Epoch 34 - Train Loss: 0.0006, Train Acc: 0.9715, Val Loss: 0.0006, Val Acc: 0.9747\n",
      "Epoch 35 - Train Loss: 0.0006, Train Acc: 0.9724, Val Loss: 0.0006, Val Acc: 0.9756\n",
      "Epoch 36 - Train Loss: 0.0006, Train Acc: 0.9726, Val Loss: 0.0006, Val Acc: 0.9759\n",
      "Epoch 37 - Train Loss: 0.0006, Train Acc: 0.9724, Val Loss: 0.0006, Val Acc: 0.9750\n",
      "Epoch 38 - Train Loss: 0.0006, Train Acc: 0.9727, Val Loss: 0.0005, Val Acc: 0.9766\n",
      "Epoch 39 - Train Loss: 0.0006, Train Acc: 0.9715, Val Loss: 0.0006, Val Acc: 0.9766\n",
      "Epoch 40 - Train Loss: 0.0006, Train Acc: 0.9714, Val Loss: 0.0006, Val Acc: 0.9772\n",
      "Epoch 41 - Train Loss: 0.0006, Train Acc: 0.9728, Val Loss: 0.0006, Val Acc: 0.9767\n",
      "Epoch 42 - Train Loss: 0.0006, Train Acc: 0.9717, Val Loss: 0.0006, Val Acc: 0.9760\n",
      "Epoch 43 - Train Loss: 0.0006, Train Acc: 0.9725, Val Loss: 0.0006, Val Acc: 0.9764\n",
      "Epoch 44 - Train Loss: 0.0006, Train Acc: 0.9723, Val Loss: 0.0006, Val Acc: 0.9765\n",
      "Epoch 45 - Train Loss: 0.0006, Train Acc: 0.9725, Val Loss: 0.0006, Val Acc: 0.9765\n",
      "Epoch 46 - Train Loss: 0.0006, Train Acc: 0.9725, Val Loss: 0.0006, Val Acc: 0.9749\n",
      "Epoch 47 - Train Loss: 0.0006, Train Acc: 0.9727, Val Loss: 0.0006, Val Acc: 0.9757\n",
      "Epoch 48 - Train Loss: 0.0006, Train Acc: 0.9728, Val Loss: 0.0006, Val Acc: 0.9768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 05:01:02,151] Trial 10 finished with value: 0.0005481958617178834 and parameters: {'learning_rate': 0.006491704311097269, 'conv_dropout_rate': 0.18833395847198703, 'linear_dropout_rate': 0.3753777439429381, 'batch_norm': True}. Best is trial 6 with value: 0.0004020875215758292.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Train Loss: 0.0006, Train Acc: 0.9734, Val Loss: 0.0006, Val Acc: 0.9764\n",
      "Epoch 0 - Train Loss: 0.0052, Train Acc: 0.7531, Val Loss: 0.0053, Val Acc: 0.8134\n",
      "Epoch 1 - Train Loss: 0.0027, Train Acc: 0.8800, Val Loss: 0.0049, Val Acc: 0.7770\n",
      "Epoch 2 - Train Loss: 0.0020, Train Acc: 0.9109, Val Loss: 0.0019, Val Acc: 0.9169\n",
      "Epoch 3 - Train Loss: 0.0016, Train Acc: 0.9322, Val Loss: 0.0022, Val Acc: 0.9013\n",
      "Epoch 4 - Train Loss: 0.0014, Train Acc: 0.9401, Val Loss: 0.0020, Val Acc: 0.9152\n",
      "Epoch 5 - Train Loss: 0.0010, Train Acc: 0.9579, Val Loss: 0.0012, Val Acc: 0.9527\n",
      "Epoch 6 - Train Loss: 0.0009, Train Acc: 0.9611, Val Loss: 0.0011, Val Acc: 0.9547\n",
      "Epoch 7 - Train Loss: 0.0009, Train Acc: 0.9624, Val Loss: 0.0009, Val Acc: 0.9620\n",
      "Epoch 8 - Train Loss: 0.0008, Train Acc: 0.9646, Val Loss: 0.0008, Val Acc: 0.9675\n",
      "Epoch 9 - Train Loss: 0.0007, Train Acc: 0.9676, Val Loss: 0.0008, Val Acc: 0.9675\n",
      "Epoch 10 - Train Loss: 0.0006, Train Acc: 0.9727, Val Loss: 0.0005, Val Acc: 0.9786\n",
      "Epoch 11 - Train Loss: 0.0006, Train Acc: 0.9751, Val Loss: 0.0006, Val Acc: 0.9737\n",
      "Epoch 12 - Train Loss: 0.0006, Train Acc: 0.9756, Val Loss: 0.0005, Val Acc: 0.9768\n",
      "Epoch 13 - Train Loss: 0.0006, Train Acc: 0.9756, Val Loss: 0.0005, Val Acc: 0.9777\n",
      "Epoch 14 - Train Loss: 0.0005, Train Acc: 0.9770, Val Loss: 0.0006, Val Acc: 0.9757\n",
      "Epoch 15 - Train Loss: 0.0005, Train Acc: 0.9795, Val Loss: 0.0005, Val Acc: 0.9810\n",
      "Epoch 16 - Train Loss: 0.0005, Train Acc: 0.9803, Val Loss: 0.0005, Val Acc: 0.9796\n",
      "Epoch 17 - Train Loss: 0.0005, Train Acc: 0.9804, Val Loss: 0.0005, Val Acc: 0.9808\n",
      "Epoch 18 - Train Loss: 0.0004, Train Acc: 0.9807, Val Loss: 0.0005, Val Acc: 0.9809\n",
      "Epoch 19 - Train Loss: 0.0004, Train Acc: 0.9812, Val Loss: 0.0004, Val Acc: 0.9825\n",
      "Epoch 20 - Train Loss: 0.0004, Train Acc: 0.9827, Val Loss: 0.0004, Val Acc: 0.9823\n",
      "Epoch 21 - Train Loss: 0.0004, Train Acc: 0.9835, Val Loss: 0.0004, Val Acc: 0.9836\n",
      "Epoch 22 - Train Loss: 0.0004, Train Acc: 0.9832, Val Loss: 0.0004, Val Acc: 0.9829\n",
      "Epoch 23 - Train Loss: 0.0004, Train Acc: 0.9833, Val Loss: 0.0004, Val Acc: 0.9825\n",
      "Epoch 24 - Train Loss: 0.0004, Train Acc: 0.9838, Val Loss: 0.0004, Val Acc: 0.9830\n",
      "Epoch 25 - Train Loss: 0.0004, Train Acc: 0.9844, Val Loss: 0.0004, Val Acc: 0.9837\n",
      "Epoch 26 - Train Loss: 0.0004, Train Acc: 0.9840, Val Loss: 0.0004, Val Acc: 0.9833\n",
      "Epoch 27 - Train Loss: 0.0004, Train Acc: 0.9841, Val Loss: 0.0004, Val Acc: 0.9827\n",
      "Epoch 28 - Train Loss: 0.0004, Train Acc: 0.9845, Val Loss: 0.0004, Val Acc: 0.9839\n",
      "Epoch 29 - Train Loss: 0.0004, Train Acc: 0.9846, Val Loss: 0.0004, Val Acc: 0.9841\n",
      "Epoch 30 - Train Loss: 0.0004, Train Acc: 0.9842, Val Loss: 0.0004, Val Acc: 0.9836\n",
      "Epoch 31 - Train Loss: 0.0004, Train Acc: 0.9848, Val Loss: 0.0004, Val Acc: 0.9823\n",
      "Epoch 32 - Train Loss: 0.0004, Train Acc: 0.9845, Val Loss: 0.0004, Val Acc: 0.9841\n",
      "Epoch 33 - Train Loss: 0.0003, Train Acc: 0.9852, Val Loss: 0.0004, Val Acc: 0.9846\n",
      "Epoch 34 - Train Loss: 0.0003, Train Acc: 0.9856, Val Loss: 0.0004, Val Acc: 0.9850\n",
      "Epoch 35 - Train Loss: 0.0003, Train Acc: 0.9855, Val Loss: 0.0004, Val Acc: 0.9848\n",
      "Epoch 36 - Train Loss: 0.0003, Train Acc: 0.9846, Val Loss: 0.0004, Val Acc: 0.9842\n",
      "Epoch 37 - Train Loss: 0.0003, Train Acc: 0.9854, Val Loss: 0.0004, Val Acc: 0.9845\n",
      "Epoch 38 - Train Loss: 0.0003, Train Acc: 0.9857, Val Loss: 0.0004, Val Acc: 0.9847\n",
      "Epoch 39 - Train Loss: 0.0003, Train Acc: 0.9851, Val Loss: 0.0004, Val Acc: 0.9847\n",
      "Epoch 40 - Train Loss: 0.0003, Train Acc: 0.9854, Val Loss: 0.0004, Val Acc: 0.9844\n",
      "Epoch 41 - Train Loss: 0.0003, Train Acc: 0.9854, Val Loss: 0.0004, Val Acc: 0.9840\n",
      "Epoch 42 - Train Loss: 0.0003, Train Acc: 0.9859, Val Loss: 0.0004, Val Acc: 0.9845\n",
      "Epoch 43 - Train Loss: 0.0003, Train Acc: 0.9856, Val Loss: 0.0004, Val Acc: 0.9844\n",
      "Epoch 44 - Train Loss: 0.0003, Train Acc: 0.9857, Val Loss: 0.0004, Val Acc: 0.9851\n",
      "Epoch 45 - Train Loss: 0.0003, Train Acc: 0.9864, Val Loss: 0.0004, Val Acc: 0.9840\n",
      "Epoch 46 - Train Loss: 0.0003, Train Acc: 0.9857, Val Loss: 0.0004, Val Acc: 0.9840\n",
      "Epoch 47 - Train Loss: 0.0003, Train Acc: 0.9858, Val Loss: 0.0004, Val Acc: 0.9832\n",
      "Epoch 48 - Train Loss: 0.0003, Train Acc: 0.9857, Val Loss: 0.0004, Val Acc: 0.9846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 05:09:17,340] Trial 11 finished with value: 0.0003886282458918721 and parameters: {'learning_rate': 0.007014789802614366, 'conv_dropout_rate': 0.07514924857101961, 'linear_dropout_rate': 0.44773875130567264, 'batch_norm': True}. Best is trial 11 with value: 0.0003886282458918721.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Train Loss: 0.0003, Train Acc: 0.9857, Val Loss: 0.0004, Val Acc: 0.9845\n",
      "Epoch 0 - Train Loss: 0.0053, Train Acc: 0.7516, Val Loss: 0.0072, Val Acc: 0.7703\n",
      "Epoch 1 - Train Loss: 0.0027, Train Acc: 0.8786, Val Loss: 0.0076, Val Acc: 0.7336\n",
      "Epoch 2 - Train Loss: 0.0021, Train Acc: 0.9099, Val Loss: 0.0021, Val Acc: 0.9045\n",
      "Epoch 3 - Train Loss: 0.0016, Train Acc: 0.9296, Val Loss: 0.0014, Val Acc: 0.9404\n",
      "Epoch 4 - Train Loss: 0.0014, Train Acc: 0.9395, Val Loss: 0.0027, Val Acc: 0.8898\n",
      "Epoch 5 - Train Loss: 0.0010, Train Acc: 0.9575, Val Loss: 0.0015, Val Acc: 0.9335\n",
      "Epoch 6 - Train Loss: 0.0009, Train Acc: 0.9594, Val Loss: 0.0009, Val Acc: 0.9635\n",
      "Epoch 7 - Train Loss: 0.0009, Train Acc: 0.9631, Val Loss: 0.0009, Val Acc: 0.9632\n",
      "Epoch 8 - Train Loss: 0.0008, Train Acc: 0.9635, Val Loss: 0.0009, Val Acc: 0.9604\n",
      "Epoch 9 - Train Loss: 0.0008, Train Acc: 0.9662, Val Loss: 0.0007, Val Acc: 0.9712\n",
      "Epoch 10 - Train Loss: 0.0006, Train Acc: 0.9729, Val Loss: 0.0005, Val Acc: 0.9773\n",
      "Epoch 11 - Train Loss: 0.0006, Train Acc: 0.9743, Val Loss: 0.0006, Val Acc: 0.9757\n",
      "Epoch 12 - Train Loss: 0.0006, Train Acc: 0.9748, Val Loss: 0.0006, Val Acc: 0.9757\n",
      "Epoch 13 - Train Loss: 0.0006, Train Acc: 0.9748, Val Loss: 0.0005, Val Acc: 0.9791\n",
      "Epoch 14 - Train Loss: 0.0005, Train Acc: 0.9766, Val Loss: 0.0005, Val Acc: 0.9790\n",
      "Epoch 15 - Train Loss: 0.0005, Train Acc: 0.9790, Val Loss: 0.0004, Val Acc: 0.9817\n",
      "Epoch 16 - Train Loss: 0.0005, Train Acc: 0.9797, Val Loss: 0.0005, Val Acc: 0.9791\n",
      "Epoch 17 - Train Loss: 0.0005, Train Acc: 0.9801, Val Loss: 0.0004, Val Acc: 0.9823\n",
      "Epoch 18 - Train Loss: 0.0005, Train Acc: 0.9794, Val Loss: 0.0005, Val Acc: 0.9817\n",
      "Epoch 19 - Train Loss: 0.0004, Train Acc: 0.9810, Val Loss: 0.0004, Val Acc: 0.9819\n",
      "Epoch 20 - Train Loss: 0.0004, Train Acc: 0.9821, Val Loss: 0.0004, Val Acc: 0.9816\n",
      "Epoch 21 - Train Loss: 0.0004, Train Acc: 0.9826, Val Loss: 0.0004, Val Acc: 0.9823\n",
      "Epoch 22 - Train Loss: 0.0004, Train Acc: 0.9826, Val Loss: 0.0004, Val Acc: 0.9821\n",
      "Epoch 23 - Train Loss: 0.0004, Train Acc: 0.9829, Val Loss: 0.0004, Val Acc: 0.9831\n",
      "Epoch 24 - Train Loss: 0.0004, Train Acc: 0.9826, Val Loss: 0.0004, Val Acc: 0.9840\n",
      "Epoch 25 - Train Loss: 0.0004, Train Acc: 0.9836, Val Loss: 0.0004, Val Acc: 0.9840\n",
      "Epoch 26 - Train Loss: 0.0004, Train Acc: 0.9836, Val Loss: 0.0004, Val Acc: 0.9844\n",
      "Epoch 27 - Train Loss: 0.0004, Train Acc: 0.9836, Val Loss: 0.0004, Val Acc: 0.9836\n",
      "Epoch 28 - Train Loss: 0.0004, Train Acc: 0.9838, Val Loss: 0.0004, Val Acc: 0.9839\n",
      "Epoch 29 - Train Loss: 0.0004, Train Acc: 0.9843, Val Loss: 0.0004, Val Acc: 0.9849\n",
      "Epoch 30 - Train Loss: 0.0004, Train Acc: 0.9850, Val Loss: 0.0004, Val Acc: 0.9849\n",
      "Epoch 31 - Train Loss: 0.0004, Train Acc: 0.9839, Val Loss: 0.0004, Val Acc: 0.9840\n",
      "Epoch 32 - Train Loss: 0.0004, Train Acc: 0.9853, Val Loss: 0.0004, Val Acc: 0.9846\n",
      "Epoch 33 - Train Loss: 0.0003, Train Acc: 0.9847, Val Loss: 0.0004, Val Acc: 0.9836\n",
      "Epoch 34 - Train Loss: 0.0003, Train Acc: 0.9845, Val Loss: 0.0004, Val Acc: 0.9842\n",
      "Epoch 35 - Train Loss: 0.0003, Train Acc: 0.9854, Val Loss: 0.0004, Val Acc: 0.9836\n",
      "Epoch 36 - Train Loss: 0.0004, Train Acc: 0.9846, Val Loss: 0.0004, Val Acc: 0.9845\n",
      "Epoch 37 - Train Loss: 0.0004, Train Acc: 0.9848, Val Loss: 0.0004, Val Acc: 0.9841\n",
      "Epoch 38 - Train Loss: 0.0004, Train Acc: 0.9849, Val Loss: 0.0004, Val Acc: 0.9852\n",
      "Epoch 39 - Train Loss: 0.0003, Train Acc: 0.9849, Val Loss: 0.0004, Val Acc: 0.9851\n",
      "Epoch 40 - Train Loss: 0.0003, Train Acc: 0.9848, Val Loss: 0.0004, Val Acc: 0.9850\n",
      "Epoch 41 - Train Loss: 0.0004, Train Acc: 0.9851, Val Loss: 0.0004, Val Acc: 0.9844\n",
      "Epoch 42 - Train Loss: 0.0004, Train Acc: 0.9847, Val Loss: 0.0004, Val Acc: 0.9852\n",
      "Epoch 43 - Train Loss: 0.0003, Train Acc: 0.9848, Val Loss: 0.0004, Val Acc: 0.9848\n",
      "Epoch 44 - Train Loss: 0.0004, Train Acc: 0.9844, Val Loss: 0.0004, Val Acc: 0.9857\n",
      "Epoch 45 - Train Loss: 0.0003, Train Acc: 0.9858, Val Loss: 0.0004, Val Acc: 0.9848\n",
      "Epoch 46 - Train Loss: 0.0003, Train Acc: 0.9853, Val Loss: 0.0004, Val Acc: 0.9840\n",
      "Epoch 47 - Train Loss: 0.0003, Train Acc: 0.9854, Val Loss: 0.0004, Val Acc: 0.9838\n",
      "Epoch 48 - Train Loss: 0.0003, Train Acc: 0.9851, Val Loss: 0.0004, Val Acc: 0.9841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 05:17:31,831] Trial 12 finished with value: 0.00037542887183590704 and parameters: {'learning_rate': 0.007216512714841624, 'conv_dropout_rate': 0.0832547297556066, 'linear_dropout_rate': 0.4949536782614232, 'batch_norm': True}. Best is trial 12 with value: 0.00037542887183590704.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Train Loss: 0.0003, Train Acc: 0.9852, Val Loss: 0.0004, Val Acc: 0.9850\n",
      "Epoch 0 - Train Loss: 0.0050, Train Acc: 0.7642, Val Loss: 0.0062, Val Acc: 0.7630\n",
      "Epoch 1 - Train Loss: 0.0025, Train Acc: 0.8924, Val Loss: 0.0043, Val Acc: 0.8122\n",
      "Epoch 2 - Train Loss: 0.0018, Train Acc: 0.9219, Val Loss: 0.0022, Val Acc: 0.9055\n",
      "Epoch 3 - Train Loss: 0.0014, Train Acc: 0.9380, Val Loss: 0.0023, Val Acc: 0.9004\n",
      "Epoch 4 - Train Loss: 0.0012, Train Acc: 0.9465, Val Loss: 0.0021, Val Acc: 0.9113\n",
      "Epoch 5 - Train Loss: 0.0009, Train Acc: 0.9630, Val Loss: 0.0011, Val Acc: 0.9514\n",
      "Epoch 6 - Train Loss: 0.0008, Train Acc: 0.9658, Val Loss: 0.0008, Val Acc: 0.9677\n",
      "Epoch 7 - Train Loss: 0.0007, Train Acc: 0.9684, Val Loss: 0.0009, Val Acc: 0.9617\n",
      "Epoch 8 - Train Loss: 0.0007, Train Acc: 0.9699, Val Loss: 0.0006, Val Acc: 0.9741\n",
      "Epoch 9 - Train Loss: 0.0006, Train Acc: 0.9710, Val Loss: 0.0007, Val Acc: 0.9705\n",
      "Epoch 10 - Train Loss: 0.0005, Train Acc: 0.9776, Val Loss: 0.0005, Val Acc: 0.9816\n",
      "Epoch 11 - Train Loss: 0.0005, Train Acc: 0.9793, Val Loss: 0.0005, Val Acc: 0.9802\n",
      "Epoch 12 - Train Loss: 0.0005, Train Acc: 0.9800, Val Loss: 0.0005, Val Acc: 0.9801\n",
      "Epoch 13 - Train Loss: 0.0005, Train Acc: 0.9803, Val Loss: 0.0004, Val Acc: 0.9804\n",
      "Epoch 14 - Train Loss: 0.0005, Train Acc: 0.9800, Val Loss: 0.0005, Val Acc: 0.9818\n",
      "Epoch 15 - Train Loss: 0.0004, Train Acc: 0.9836, Val Loss: 0.0004, Val Acc: 0.9844\n",
      "Epoch 16 - Train Loss: 0.0004, Train Acc: 0.9843, Val Loss: 0.0004, Val Acc: 0.9828\n",
      "Epoch 17 - Train Loss: 0.0004, Train Acc: 0.9846, Val Loss: 0.0004, Val Acc: 0.9819\n",
      "Epoch 18 - Train Loss: 0.0004, Train Acc: 0.9847, Val Loss: 0.0005, Val Acc: 0.9815\n",
      "Epoch 19 - Train Loss: 0.0003, Train Acc: 0.9852, Val Loss: 0.0004, Val Acc: 0.9835\n",
      "Epoch 20 - Train Loss: 0.0003, Train Acc: 0.9865, Val Loss: 0.0004, Val Acc: 0.9841\n",
      "Epoch 21 - Train Loss: 0.0003, Train Acc: 0.9869, Val Loss: 0.0004, Val Acc: 0.9838\n",
      "Epoch 22 - Train Loss: 0.0003, Train Acc: 0.9874, Val Loss: 0.0004, Val Acc: 0.9847\n",
      "Epoch 23 - Train Loss: 0.0003, Train Acc: 0.9866, Val Loss: 0.0004, Val Acc: 0.9857\n",
      "Epoch 24 - Train Loss: 0.0003, Train Acc: 0.9873, Val Loss: 0.0004, Val Acc: 0.9846\n",
      "Epoch 25 - Train Loss: 0.0003, Train Acc: 0.9884, Val Loss: 0.0004, Val Acc: 0.9861\n",
      "Epoch 26 - Train Loss: 0.0003, Train Acc: 0.9883, Val Loss: 0.0004, Val Acc: 0.9857\n",
      "Epoch 27 - Train Loss: 0.0003, Train Acc: 0.9887, Val Loss: 0.0004, Val Acc: 0.9856\n",
      "Epoch 28 - Train Loss: 0.0003, Train Acc: 0.9881, Val Loss: 0.0004, Val Acc: 0.9849\n",
      "Epoch 29 - Train Loss: 0.0003, Train Acc: 0.9889, Val Loss: 0.0004, Val Acc: 0.9860\n",
      "Epoch 30 - Train Loss: 0.0003, Train Acc: 0.9883, Val Loss: 0.0004, Val Acc: 0.9856\n",
      "Epoch 31 - Train Loss: 0.0003, Train Acc: 0.9887, Val Loss: 0.0004, Val Acc: 0.9858\n",
      "Epoch 32 - Train Loss: 0.0003, Train Acc: 0.9887, Val Loss: 0.0004, Val Acc: 0.9855\n",
      "Epoch 33 - Train Loss: 0.0002, Train Acc: 0.9894, Val Loss: 0.0004, Val Acc: 0.9859\n",
      "Epoch 34 - Train Loss: 0.0003, Train Acc: 0.9891, Val Loss: 0.0004, Val Acc: 0.9863\n",
      "Epoch 35 - Train Loss: 0.0003, Train Acc: 0.9893, Val Loss: 0.0004, Val Acc: 0.9868\n",
      "Epoch 36 - Train Loss: 0.0003, Train Acc: 0.9890, Val Loss: 0.0004, Val Acc: 0.9861\n",
      "Epoch 37 - Train Loss: 0.0002, Train Acc: 0.9894, Val Loss: 0.0004, Val Acc: 0.9858\n",
      "Epoch 38 - Train Loss: 0.0003, Train Acc: 0.9894, Val Loss: 0.0004, Val Acc: 0.9867\n",
      "Epoch 39 - Train Loss: 0.0003, Train Acc: 0.9888, Val Loss: 0.0004, Val Acc: 0.9853\n",
      "Epoch 40 - Train Loss: 0.0002, Train Acc: 0.9891, Val Loss: 0.0004, Val Acc: 0.9862\n",
      "Epoch 41 - Train Loss: 0.0003, Train Acc: 0.9893, Val Loss: 0.0004, Val Acc: 0.9859\n",
      "Epoch 42 - Train Loss: 0.0002, Train Acc: 0.9893, Val Loss: 0.0004, Val Acc: 0.9865\n",
      "Epoch 43 - Train Loss: 0.0002, Train Acc: 0.9896, Val Loss: 0.0004, Val Acc: 0.9861\n",
      "Epoch 44 - Train Loss: 0.0002, Train Acc: 0.9894, Val Loss: 0.0004, Val Acc: 0.9860\n",
      "Epoch 45 - Train Loss: 0.0002, Train Acc: 0.9901, Val Loss: 0.0004, Val Acc: 0.9866\n",
      "Epoch 46 - Train Loss: 0.0002, Train Acc: 0.9892, Val Loss: 0.0004, Val Acc: 0.9863\n",
      "Epoch 47 - Train Loss: 0.0002, Train Acc: 0.9893, Val Loss: 0.0004, Val Acc: 0.9855\n",
      "Epoch 48 - Train Loss: 0.0002, Train Acc: 0.9897, Val Loss: 0.0004, Val Acc: 0.9859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 05:25:47,321] Trial 13 finished with value: 0.0003564444387138712 and parameters: {'learning_rate': 0.007121757982838747, 'conv_dropout_rate': 0.04634591803439729, 'linear_dropout_rate': 0.48366048573965087, 'batch_norm': True}. Best is trial 13 with value: 0.0003564444387138712.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Train Loss: 0.0002, Train Acc: 0.9894, Val Loss: 0.0004, Val Acc: 0.9860\n",
      "Epoch 0 - Train Loss: 0.0045, Train Acc: 0.7934, Val Loss: 0.0074, Val Acc: 0.7609\n",
      "Epoch 1 - Train Loss: 0.0022, Train Acc: 0.9051, Val Loss: 0.0034, Val Acc: 0.8477\n",
      "Epoch 2 - Train Loss: 0.0015, Train Acc: 0.9333, Val Loss: 0.0026, Val Acc: 0.8809\n",
      "Epoch 3 - Train Loss: 0.0012, Train Acc: 0.9468, Val Loss: 0.0077, Val Acc: 0.8362\n",
      "Epoch 4 - Train Loss: 0.0010, Train Acc: 0.9555, Val Loss: 0.0014, Val Acc: 0.9411\n",
      "Epoch 5 - Train Loss: 0.0007, Train Acc: 0.9712, Val Loss: 0.0009, Val Acc: 0.9651\n",
      "Epoch 6 - Train Loss: 0.0006, Train Acc: 0.9748, Val Loss: 0.0009, Val Acc: 0.9626\n",
      "Epoch 7 - Train Loss: 0.0005, Train Acc: 0.9773, Val Loss: 0.0017, Val Acc: 0.9321\n",
      "Epoch 8 - Train Loss: 0.0005, Train Acc: 0.9785, Val Loss: 0.0007, Val Acc: 0.9718\n",
      "Epoch 9 - Train Loss: 0.0005, Train Acc: 0.9795, Val Loss: 0.0006, Val Acc: 0.9750\n",
      "Epoch 10 - Train Loss: 0.0003, Train Acc: 0.9872, Val Loss: 0.0005, Val Acc: 0.9784\n",
      "Epoch 11 - Train Loss: 0.0003, Train Acc: 0.9883, Val Loss: 0.0006, Val Acc: 0.9764\n",
      "Epoch 12 - Train Loss: 0.0003, Train Acc: 0.9891, Val Loss: 0.0005, Val Acc: 0.9823\n",
      "Epoch 13 - Train Loss: 0.0003, Train Acc: 0.9891, Val Loss: 0.0005, Val Acc: 0.9807\n",
      "Epoch 14 - Train Loss: 0.0002, Train Acc: 0.9905, Val Loss: 0.0006, Val Acc: 0.9788\n",
      "Epoch 15 - Train Loss: 0.0002, Train Acc: 0.9932, Val Loss: 0.0004, Val Acc: 0.9830\n",
      "Epoch 16 - Train Loss: 0.0001, Train Acc: 0.9938, Val Loss: 0.0005, Val Acc: 0.9823\n",
      "Epoch 17 - Train Loss: 0.0001, Train Acc: 0.9942, Val Loss: 0.0005, Val Acc: 0.9826\n",
      "Epoch 18 - Train Loss: 0.0001, Train Acc: 0.9940, Val Loss: 0.0004, Val Acc: 0.9841\n",
      "Epoch 19 - Train Loss: 0.0001, Train Acc: 0.9949, Val Loss: 0.0004, Val Acc: 0.9834\n",
      "Epoch 20 - Train Loss: 0.0001, Train Acc: 0.9958, Val Loss: 0.0005, Val Acc: 0.9840\n",
      "Epoch 21 - Train Loss: 0.0001, Train Acc: 0.9959, Val Loss: 0.0004, Val Acc: 0.9849\n",
      "Epoch 22 - Train Loss: 0.0001, Train Acc: 0.9960, Val Loss: 0.0004, Val Acc: 0.9848\n",
      "Epoch 23 - Train Loss: 0.0001, Train Acc: 0.9961, Val Loss: 0.0004, Val Acc: 0.9846\n",
      "Epoch 24 - Train Loss: 0.0001, Train Acc: 0.9963, Val Loss: 0.0004, Val Acc: 0.9849\n",
      "Epoch 25 - Train Loss: 0.0001, Train Acc: 0.9968, Val Loss: 0.0004, Val Acc: 0.9851\n",
      "Epoch 26 - Train Loss: 0.0001, Train Acc: 0.9969, Val Loss: 0.0004, Val Acc: 0.9853\n",
      "Epoch 27 - Train Loss: 0.0001, Train Acc: 0.9974, Val Loss: 0.0004, Val Acc: 0.9850\n",
      "Epoch 28 - Train Loss: 0.0001, Train Acc: 0.9972, Val Loss: 0.0004, Val Acc: 0.9844\n",
      "Epoch 29 - Train Loss: 0.0001, Train Acc: 0.9974, Val Loss: 0.0004, Val Acc: 0.9856\n",
      "Epoch 30 - Train Loss: 0.0001, Train Acc: 0.9973, Val Loss: 0.0004, Val Acc: 0.9848\n",
      "Epoch 31 - Train Loss: 0.0001, Train Acc: 0.9974, Val Loss: 0.0004, Val Acc: 0.9849\n",
      "Epoch 32 - Train Loss: 0.0001, Train Acc: 0.9975, Val Loss: 0.0004, Val Acc: 0.9857\n",
      "Epoch 33 - Train Loss: 0.0001, Train Acc: 0.9976, Val Loss: 0.0004, Val Acc: 0.9849\n",
      "Epoch 34 - Train Loss: 0.0001, Train Acc: 0.9978, Val Loss: 0.0004, Val Acc: 0.9852\n",
      "Epoch 35 - Train Loss: 0.0001, Train Acc: 0.9977, Val Loss: 0.0004, Val Acc: 0.9859\n",
      "Epoch 36 - Train Loss: 0.0001, Train Acc: 0.9976, Val Loss: 0.0004, Val Acc: 0.9853\n",
      "Epoch 37 - Train Loss: 0.0001, Train Acc: 0.9976, Val Loss: 0.0004, Val Acc: 0.9845\n",
      "Epoch 38 - Train Loss: 0.0001, Train Acc: 0.9976, Val Loss: 0.0004, Val Acc: 0.9855\n",
      "Epoch 39 - Train Loss: 0.0001, Train Acc: 0.9975, Val Loss: 0.0004, Val Acc: 0.9851\n",
      "Epoch 40 - Train Loss: 0.0001, Train Acc: 0.9977, Val Loss: 0.0004, Val Acc: 0.9853\n",
      "Epoch 41 - Train Loss: 0.0001, Train Acc: 0.9977, Val Loss: 0.0004, Val Acc: 0.9844\n",
      "Epoch 42 - Train Loss: 0.0001, Train Acc: 0.9977, Val Loss: 0.0004, Val Acc: 0.9846\n",
      "Epoch 43 - Train Loss: 0.0001, Train Acc: 0.9978, Val Loss: 0.0004, Val Acc: 0.9851\n",
      "Epoch 44 - Train Loss: 0.0001, Train Acc: 0.9977, Val Loss: 0.0004, Val Acc: 0.9841\n",
      "Epoch 45 - Train Loss: 0.0001, Train Acc: 0.9978, Val Loss: 0.0004, Val Acc: 0.9859\n",
      "Epoch 46 - Train Loss: 0.0001, Train Acc: 0.9978, Val Loss: 0.0004, Val Acc: 0.9851\n",
      "Epoch 47 - Train Loss: 0.0001, Train Acc: 0.9978, Val Loss: 0.0004, Val Acc: 0.9853\n",
      "Epoch 48 - Train Loss: 0.0001, Train Acc: 0.9977, Val Loss: 0.0004, Val Acc: 0.9856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 05:34:01,927] Trial 14 finished with value: 0.0004107147593089178 and parameters: {'learning_rate': 0.007646488208012506, 'conv_dropout_rate': 4.872334976029069e-05, 'linear_dropout_rate': 0.37566760732978577, 'batch_norm': True}. Best is trial 13 with value: 0.0003564444387138712.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Train Loss: 0.0001, Train Acc: 0.9977, Val Loss: 0.0004, Val Acc: 0.9856\n",
      "Epoch 0 - Train Loss: 0.0053, Train Acc: 0.7494, Val Loss: 0.0066, Val Acc: 0.7390\n",
      "Epoch 1 - Train Loss: 0.0027, Train Acc: 0.8807, Val Loss: 0.0045, Val Acc: 0.8030\n",
      "Epoch 2 - Train Loss: 0.0020, Train Acc: 0.9134, Val Loss: 0.0022, Val Acc: 0.9027\n",
      "Epoch 3 - Train Loss: 0.0016, Train Acc: 0.9307, Val Loss: 0.0022, Val Acc: 0.9033\n",
      "Epoch 4 - Train Loss: 0.0014, Train Acc: 0.9395, Val Loss: 0.0010, Val Acc: 0.9596\n",
      "Epoch 5 - Train Loss: 0.0010, Train Acc: 0.9567, Val Loss: 0.0008, Val Acc: 0.9685\n",
      "Epoch 6 - Train Loss: 0.0009, Train Acc: 0.9603, Val Loss: 0.0011, Val Acc: 0.9559\n",
      "Epoch 7 - Train Loss: 0.0009, Train Acc: 0.9624, Val Loss: 0.0010, Val Acc: 0.9611\n",
      "Epoch 8 - Train Loss: 0.0008, Train Acc: 0.9642, Val Loss: 0.0008, Val Acc: 0.9677\n",
      "Epoch 9 - Train Loss: 0.0008, Train Acc: 0.9666, Val Loss: 0.0009, Val Acc: 0.9651\n",
      "Epoch 10 - Train Loss: 0.0006, Train Acc: 0.9721, Val Loss: 0.0005, Val Acc: 0.9804\n",
      "Epoch 11 - Train Loss: 0.0006, Train Acc: 0.9744, Val Loss: 0.0005, Val Acc: 0.9792\n",
      "Epoch 12 - Train Loss: 0.0006, Train Acc: 0.9746, Val Loss: 0.0005, Val Acc: 0.9803\n",
      "Epoch 13 - Train Loss: 0.0006, Train Acc: 0.9745, Val Loss: 0.0006, Val Acc: 0.9784\n",
      "Epoch 14 - Train Loss: 0.0006, Train Acc: 0.9759, Val Loss: 0.0005, Val Acc: 0.9809\n",
      "Epoch 15 - Train Loss: 0.0005, Train Acc: 0.9784, Val Loss: 0.0004, Val Acc: 0.9839\n",
      "Epoch 16 - Train Loss: 0.0005, Train Acc: 0.9800, Val Loss: 0.0005, Val Acc: 0.9811\n",
      "Epoch 17 - Train Loss: 0.0005, Train Acc: 0.9804, Val Loss: 0.0005, Val Acc: 0.9814\n",
      "Epoch 18 - Train Loss: 0.0004, Train Acc: 0.9806, Val Loss: 0.0005, Val Acc: 0.9814\n",
      "Epoch 19 - Train Loss: 0.0004, Train Acc: 0.9807, Val Loss: 0.0005, Val Acc: 0.9823\n",
      "Epoch 20 - Train Loss: 0.0004, Train Acc: 0.9824, Val Loss: 0.0004, Val Acc: 0.9836\n",
      "Epoch 21 - Train Loss: 0.0004, Train Acc: 0.9828, Val Loss: 0.0004, Val Acc: 0.9850\n",
      "Epoch 22 - Train Loss: 0.0004, Train Acc: 0.9828, Val Loss: 0.0004, Val Acc: 0.9859\n",
      "Epoch 23 - Train Loss: 0.0004, Train Acc: 0.9820, Val Loss: 0.0004, Val Acc: 0.9855\n",
      "Epoch 24 - Train Loss: 0.0004, Train Acc: 0.9831, Val Loss: 0.0004, Val Acc: 0.9851\n",
      "Epoch 25 - Train Loss: 0.0004, Train Acc: 0.9840, Val Loss: 0.0004, Val Acc: 0.9855\n",
      "Epoch 26 - Train Loss: 0.0004, Train Acc: 0.9834, Val Loss: 0.0004, Val Acc: 0.9856\n",
      "Epoch 27 - Train Loss: 0.0004, Train Acc: 0.9840, Val Loss: 0.0004, Val Acc: 0.9851\n",
      "Epoch 28 - Train Loss: 0.0004, Train Acc: 0.9838, Val Loss: 0.0004, Val Acc: 0.9849\n",
      "Epoch 29 - Train Loss: 0.0004, Train Acc: 0.9836, Val Loss: 0.0004, Val Acc: 0.9853\n",
      "Epoch 30 - Train Loss: 0.0004, Train Acc: 0.9840, Val Loss: 0.0004, Val Acc: 0.9856\n",
      "Epoch 31 - Train Loss: 0.0004, Train Acc: 0.9844, Val Loss: 0.0004, Val Acc: 0.9858\n",
      "Epoch 32 - Train Loss: 0.0004, Train Acc: 0.9843, Val Loss: 0.0004, Val Acc: 0.9852\n",
      "Epoch 33 - Train Loss: 0.0003, Train Acc: 0.9851, Val Loss: 0.0004, Val Acc: 0.9855\n",
      "Epoch 34 - Train Loss: 0.0004, Train Acc: 0.9843, Val Loss: 0.0004, Val Acc: 0.9861\n",
      "Epoch 35 - Train Loss: 0.0003, Train Acc: 0.9855, Val Loss: 0.0004, Val Acc: 0.9858\n",
      "Epoch 36 - Train Loss: 0.0004, Train Acc: 0.9848, Val Loss: 0.0004, Val Acc: 0.9859\n",
      "Epoch 37 - Train Loss: 0.0003, Train Acc: 0.9848, Val Loss: 0.0004, Val Acc: 0.9852\n",
      "Epoch 38 - Train Loss: 0.0003, Train Acc: 0.9850, Val Loss: 0.0004, Val Acc: 0.9867\n",
      "Epoch 39 - Train Loss: 0.0003, Train Acc: 0.9849, Val Loss: 0.0004, Val Acc: 0.9850\n",
      "Epoch 40 - Train Loss: 0.0003, Train Acc: 0.9856, Val Loss: 0.0004, Val Acc: 0.9862\n",
      "Epoch 41 - Train Loss: 0.0004, Train Acc: 0.9853, Val Loss: 0.0004, Val Acc: 0.9859\n",
      "Epoch 42 - Train Loss: 0.0003, Train Acc: 0.9853, Val Loss: 0.0004, Val Acc: 0.9856\n",
      "Epoch 43 - Train Loss: 0.0003, Train Acc: 0.9849, Val Loss: 0.0004, Val Acc: 0.9853\n",
      "Epoch 44 - Train Loss: 0.0003, Train Acc: 0.9849, Val Loss: 0.0004, Val Acc: 0.9845\n",
      "Epoch 45 - Train Loss: 0.0003, Train Acc: 0.9853, Val Loss: 0.0004, Val Acc: 0.9858\n",
      "Epoch 46 - Train Loss: 0.0003, Train Acc: 0.9850, Val Loss: 0.0004, Val Acc: 0.9858\n",
      "Epoch 47 - Train Loss: 0.0003, Train Acc: 0.9854, Val Loss: 0.0004, Val Acc: 0.9853\n",
      "Epoch 48 - Train Loss: 0.0003, Train Acc: 0.9856, Val Loss: 0.0004, Val Acc: 0.9863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 05:42:16,204] Trial 15 finished with value: 0.0003806538293372746 and parameters: {'learning_rate': 0.005567942894254945, 'conv_dropout_rate': 0.06747710200093883, 'linear_dropout_rate': 0.4964171908954898, 'batch_norm': True}. Best is trial 13 with value: 0.0003564444387138712.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Train Loss: 0.0003, Train Acc: 0.9856, Val Loss: 0.0004, Val Acc: 0.9854\n",
      "Epoch 0 - Train Loss: 0.0045, Train Acc: 0.7916, Val Loss: 0.0050, Val Acc: 0.7754\n",
      "Epoch 1 - Train Loss: 0.0021, Train Acc: 0.9069, Val Loss: 0.0064, Val Acc: 0.7472\n",
      "Epoch 2 - Train Loss: 0.0015, Train Acc: 0.9340, Val Loss: 0.0020, Val Acc: 0.9112\n",
      "Epoch 3 - Train Loss: 0.0012, Train Acc: 0.9485, Val Loss: 0.0020, Val Acc: 0.9193\n",
      "Epoch 4 - Train Loss: 0.0011, Train Acc: 0.9548, Val Loss: 0.0012, Val Acc: 0.9481\n",
      "Epoch 5 - Train Loss: 0.0007, Train Acc: 0.9714, Val Loss: 0.0010, Val Acc: 0.9613\n",
      "Epoch 6 - Train Loss: 0.0006, Train Acc: 0.9736, Val Loss: 0.0006, Val Acc: 0.9766\n",
      "Epoch 7 - Train Loss: 0.0005, Train Acc: 0.9765, Val Loss: 0.0011, Val Acc: 0.9543\n",
      "Epoch 8 - Train Loss: 0.0005, Train Acc: 0.9769, Val Loss: 0.0006, Val Acc: 0.9745\n",
      "Epoch 9 - Train Loss: 0.0005, Train Acc: 0.9783, Val Loss: 0.0006, Val Acc: 0.9750\n",
      "Epoch 10 - Train Loss: 0.0003, Train Acc: 0.9851, Val Loss: 0.0005, Val Acc: 0.9792\n",
      "Epoch 11 - Train Loss: 0.0003, Train Acc: 0.9867, Val Loss: 0.0005, Val Acc: 0.9797\n",
      "Epoch 12 - Train Loss: 0.0003, Train Acc: 0.9876, Val Loss: 0.0005, Val Acc: 0.9823\n",
      "Epoch 13 - Train Loss: 0.0003, Train Acc: 0.9873, Val Loss: 0.0005, Val Acc: 0.9794\n",
      "Epoch 14 - Train Loss: 0.0003, Train Acc: 0.9879, Val Loss: 0.0006, Val Acc: 0.9780\n",
      "Epoch 15 - Train Loss: 0.0002, Train Acc: 0.9907, Val Loss: 0.0004, Val Acc: 0.9860\n",
      "Epoch 16 - Train Loss: 0.0002, Train Acc: 0.9916, Val Loss: 0.0004, Val Acc: 0.9841\n",
      "Epoch 17 - Train Loss: 0.0002, Train Acc: 0.9918, Val Loss: 0.0004, Val Acc: 0.9845\n",
      "Epoch 18 - Train Loss: 0.0002, Train Acc: 0.9911, Val Loss: 0.0004, Val Acc: 0.9843\n",
      "Epoch 19 - Train Loss: 0.0002, Train Acc: 0.9923, Val Loss: 0.0004, Val Acc: 0.9864\n",
      "Epoch 20 - Train Loss: 0.0002, Train Acc: 0.9935, Val Loss: 0.0004, Val Acc: 0.9851\n",
      "Epoch 21 - Train Loss: 0.0002, Train Acc: 0.9933, Val Loss: 0.0004, Val Acc: 0.9855\n",
      "Epoch 22 - Train Loss: 0.0001, Train Acc: 0.9940, Val Loss: 0.0004, Val Acc: 0.9863\n",
      "Epoch 23 - Train Loss: 0.0001, Train Acc: 0.9940, Val Loss: 0.0004, Val Acc: 0.9859\n",
      "Epoch 24 - Train Loss: 0.0001, Train Acc: 0.9942, Val Loss: 0.0004, Val Acc: 0.9872\n",
      "Epoch 25 - Train Loss: 0.0001, Train Acc: 0.9946, Val Loss: 0.0004, Val Acc: 0.9868\n",
      "Epoch 26 - Train Loss: 0.0001, Train Acc: 0.9944, Val Loss: 0.0004, Val Acc: 0.9870\n",
      "Epoch 27 - Train Loss: 0.0001, Train Acc: 0.9948, Val Loss: 0.0004, Val Acc: 0.9861\n",
      "Epoch 28 - Train Loss: 0.0001, Train Acc: 0.9949, Val Loss: 0.0004, Val Acc: 0.9876\n",
      "Epoch 29 - Train Loss: 0.0001, Train Acc: 0.9951, Val Loss: 0.0004, Val Acc: 0.9872\n",
      "Epoch 30 - Train Loss: 0.0001, Train Acc: 0.9953, Val Loss: 0.0004, Val Acc: 0.9867\n",
      "Epoch 31 - Train Loss: 0.0001, Train Acc: 0.9952, Val Loss: 0.0004, Val Acc: 0.9870\n",
      "Epoch 32 - Train Loss: 0.0001, Train Acc: 0.9951, Val Loss: 0.0004, Val Acc: 0.9868\n",
      "Epoch 33 - Train Loss: 0.0001, Train Acc: 0.9955, Val Loss: 0.0004, Val Acc: 0.9874\n",
      "Epoch 34 - Train Loss: 0.0001, Train Acc: 0.9954, Val Loss: 0.0004, Val Acc: 0.9868\n",
      "Epoch 35 - Train Loss: 0.0001, Train Acc: 0.9957, Val Loss: 0.0004, Val Acc: 0.9872\n",
      "Epoch 36 - Train Loss: 0.0001, Train Acc: 0.9957, Val Loss: 0.0004, Val Acc: 0.9865\n",
      "Epoch 37 - Train Loss: 0.0001, Train Acc: 0.9955, Val Loss: 0.0004, Val Acc: 0.9867\n",
      "Epoch 38 - Train Loss: 0.0001, Train Acc: 0.9954, Val Loss: 0.0004, Val Acc: 0.9872\n",
      "Epoch 39 - Train Loss: 0.0001, Train Acc: 0.9958, Val Loss: 0.0004, Val Acc: 0.9868\n",
      "Epoch 40 - Train Loss: 0.0001, Train Acc: 0.9956, Val Loss: 0.0004, Val Acc: 0.9869\n",
      "Epoch 41 - Train Loss: 0.0001, Train Acc: 0.9955, Val Loss: 0.0004, Val Acc: 0.9873\n",
      "Epoch 42 - Train Loss: 0.0001, Train Acc: 0.9956, Val Loss: 0.0004, Val Acc: 0.9870\n",
      "Epoch 43 - Train Loss: 0.0001, Train Acc: 0.9963, Val Loss: 0.0004, Val Acc: 0.9867\n",
      "Epoch 44 - Train Loss: 0.0001, Train Acc: 0.9957, Val Loss: 0.0004, Val Acc: 0.9878\n",
      "Epoch 45 - Train Loss: 0.0001, Train Acc: 0.9958, Val Loss: 0.0004, Val Acc: 0.9871\n",
      "Epoch 46 - Train Loss: 0.0001, Train Acc: 0.9958, Val Loss: 0.0004, Val Acc: 0.9870\n",
      "Epoch 47 - Train Loss: 0.0001, Train Acc: 0.9960, Val Loss: 0.0004, Val Acc: 0.9867\n",
      "Epoch 48 - Train Loss: 0.0001, Train Acc: 0.9957, Val Loss: 0.0004, Val Acc: 0.9872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 05:50:29,791] Trial 16 finished with value: 0.0003695331326015934 and parameters: {'learning_rate': 0.007948149436976618, 'conv_dropout_rate': 0.013880918387740866, 'linear_dropout_rate': 0.3500517791833062, 'batch_norm': True}. Best is trial 13 with value: 0.0003564444387138712.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Train Loss: 0.0001, Train Acc: 0.9959, Val Loss: 0.0004, Val Acc: 0.9872\n",
      "Epoch 0 - Train Loss: 0.0045, Train Acc: 0.7933, Val Loss: 0.0066, Val Acc: 0.7561\n",
      "Epoch 1 - Train Loss: 0.0021, Train Acc: 0.9041, Val Loss: 0.0025, Val Acc: 0.8899\n",
      "Epoch 2 - Train Loss: 0.0016, Train Acc: 0.9319, Val Loss: 0.0033, Val Acc: 0.8768\n",
      "Epoch 3 - Train Loss: 0.0013, Train Acc: 0.9446, Val Loss: 0.0011, Val Acc: 0.9566\n",
      "Epoch 4 - Train Loss: 0.0010, Train Acc: 0.9566, Val Loss: 0.0023, Val Acc: 0.9090\n",
      "Epoch 5 - Train Loss: 0.0006, Train Acc: 0.9724, Val Loss: 0.0010, Val Acc: 0.9563\n",
      "Epoch 6 - Train Loss: 0.0006, Train Acc: 0.9746, Val Loss: 0.0007, Val Acc: 0.9736\n",
      "Epoch 7 - Train Loss: 0.0005, Train Acc: 0.9778, Val Loss: 0.0010, Val Acc: 0.9569\n",
      "Epoch 8 - Train Loss: 0.0005, Train Acc: 0.9785, Val Loss: 0.0007, Val Acc: 0.9729\n",
      "Epoch 9 - Train Loss: 0.0004, Train Acc: 0.9803, Val Loss: 0.0006, Val Acc: 0.9740\n",
      "Epoch 10 - Train Loss: 0.0003, Train Acc: 0.9870, Val Loss: 0.0005, Val Acc: 0.9782\n",
      "Epoch 11 - Train Loss: 0.0003, Train Acc: 0.9894, Val Loss: 0.0006, Val Acc: 0.9770\n",
      "Epoch 12 - Train Loss: 0.0002, Train Acc: 0.9901, Val Loss: 0.0005, Val Acc: 0.9823\n",
      "Epoch 13 - Train Loss: 0.0003, Train Acc: 0.9889, Val Loss: 0.0007, Val Acc: 0.9745\n",
      "Epoch 14 - Train Loss: 0.0003, Train Acc: 0.9885, Val Loss: 0.0005, Val Acc: 0.9784\n",
      "Epoch 15 - Train Loss: 0.0002, Train Acc: 0.9931, Val Loss: 0.0004, Val Acc: 0.9836\n",
      "Epoch 16 - Train Loss: 0.0002, Train Acc: 0.9940, Val Loss: 0.0005, Val Acc: 0.9821\n",
      "Epoch 17 - Train Loss: 0.0001, Train Acc: 0.9943, Val Loss: 0.0005, Val Acc: 0.9827\n",
      "Epoch 18 - Train Loss: 0.0001, Train Acc: 0.9941, Val Loss: 0.0005, Val Acc: 0.9831\n",
      "Epoch 19 - Train Loss: 0.0001, Train Acc: 0.9953, Val Loss: 0.0005, Val Acc: 0.9817\n",
      "Epoch 20 - Train Loss: 0.0001, Train Acc: 0.9964, Val Loss: 0.0005, Val Acc: 0.9834\n",
      "Epoch 21 - Train Loss: 0.0001, Train Acc: 0.9962, Val Loss: 0.0004, Val Acc: 0.9846\n",
      "Epoch 22 - Train Loss: 0.0001, Train Acc: 0.9966, Val Loss: 0.0005, Val Acc: 0.9835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 05:54:26,841] Trial 17 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Train Loss: 0.0047, Train Acc: 0.7813, Val Loss: 0.0034, Val Acc: 0.8494\n",
      "Epoch 1 - Train Loss: 0.0022, Train Acc: 0.8990, Val Loss: 0.0038, Val Acc: 0.8404\n",
      "Epoch 2 - Train Loss: 0.0016, Train Acc: 0.9280, Val Loss: 0.0024, Val Acc: 0.8923\n",
      "Epoch 3 - Train Loss: 0.0013, Train Acc: 0.9423, Val Loss: 0.0024, Val Acc: 0.9023\n",
      "Epoch 4 - Train Loss: 0.0011, Train Acc: 0.9515, Val Loss: 0.0031, Val Acc: 0.8729\n",
      "Epoch 5 - Train Loss: 0.0008, Train Acc: 0.9662, Val Loss: 0.0009, Val Acc: 0.9620\n",
      "Epoch 6 - Train Loss: 0.0007, Train Acc: 0.9691, Val Loss: 0.0011, Val Acc: 0.9559\n",
      "Epoch 7 - Train Loss: 0.0007, Train Acc: 0.9713, Val Loss: 0.0009, Val Acc: 0.9617\n",
      "Epoch 8 - Train Loss: 0.0006, Train Acc: 0.9725, Val Loss: 0.0007, Val Acc: 0.9708\n",
      "Epoch 9 - Train Loss: 0.0006, Train Acc: 0.9750, Val Loss: 0.0007, Val Acc: 0.9694\n",
      "Epoch 10 - Train Loss: 0.0004, Train Acc: 0.9800, Val Loss: 0.0005, Val Acc: 0.9795\n",
      "Epoch 11 - Train Loss: 0.0004, Train Acc: 0.9821, Val Loss: 0.0005, Val Acc: 0.9790\n",
      "Epoch 12 - Train Loss: 0.0004, Train Acc: 0.9833, Val Loss: 0.0005, Val Acc: 0.9802\n",
      "Epoch 13 - Train Loss: 0.0004, Train Acc: 0.9827, Val Loss: 0.0005, Val Acc: 0.9771\n",
      "Epoch 14 - Train Loss: 0.0004, Train Acc: 0.9845, Val Loss: 0.0005, Val Acc: 0.9799\n",
      "Epoch 15 - Train Loss: 0.0003, Train Acc: 0.9866, Val Loss: 0.0004, Val Acc: 0.9831\n",
      "Epoch 16 - Train Loss: 0.0003, Train Acc: 0.9868, Val Loss: 0.0005, Val Acc: 0.9815\n",
      "Epoch 17 - Train Loss: 0.0003, Train Acc: 0.9875, Val Loss: 0.0004, Val Acc: 0.9842\n",
      "Epoch 18 - Train Loss: 0.0003, Train Acc: 0.9868, Val Loss: 0.0004, Val Acc: 0.9821\n",
      "Epoch 19 - Train Loss: 0.0003, Train Acc: 0.9876, Val Loss: 0.0004, Val Acc: 0.9845\n",
      "Epoch 20 - Train Loss: 0.0002, Train Acc: 0.9897, Val Loss: 0.0004, Val Acc: 0.9829\n",
      "Epoch 21 - Train Loss: 0.0002, Train Acc: 0.9891, Val Loss: 0.0004, Val Acc: 0.9841\n",
      "Epoch 22 - Train Loss: 0.0002, Train Acc: 0.9899, Val Loss: 0.0004, Val Acc: 0.9862\n",
      "Epoch 23 - Train Loss: 0.0002, Train Acc: 0.9900, Val Loss: 0.0004, Val Acc: 0.9851\n",
      "Epoch 24 - Train Loss: 0.0002, Train Acc: 0.9904, Val Loss: 0.0004, Val Acc: 0.9846\n",
      "Epoch 25 - Train Loss: 0.0002, Train Acc: 0.9912, Val Loss: 0.0004, Val Acc: 0.9858\n",
      "Epoch 26 - Train Loss: 0.0002, Train Acc: 0.9906, Val Loss: 0.0004, Val Acc: 0.9862\n",
      "Epoch 27 - Train Loss: 0.0002, Train Acc: 0.9908, Val Loss: 0.0004, Val Acc: 0.9853\n",
      "Epoch 28 - Train Loss: 0.0002, Train Acc: 0.9912, Val Loss: 0.0004, Val Acc: 0.9848\n",
      "Epoch 29 - Train Loss: 0.0002, Train Acc: 0.9915, Val Loss: 0.0004, Val Acc: 0.9866\n",
      "Epoch 30 - Train Loss: 0.0002, Train Acc: 0.9916, Val Loss: 0.0004, Val Acc: 0.9849\n",
      "Epoch 31 - Train Loss: 0.0002, Train Acc: 0.9920, Val Loss: 0.0004, Val Acc: 0.9856\n",
      "Epoch 32 - Train Loss: 0.0002, Train Acc: 0.9914, Val Loss: 0.0004, Val Acc: 0.9845\n",
      "Epoch 33 - Train Loss: 0.0002, Train Acc: 0.9918, Val Loss: 0.0004, Val Acc: 0.9865\n",
      "Epoch 34 - Train Loss: 0.0002, Train Acc: 0.9918, Val Loss: 0.0004, Val Acc: 0.9857\n",
      "Epoch 35 - Train Loss: 0.0002, Train Acc: 0.9920, Val Loss: 0.0004, Val Acc: 0.9861\n",
      "Epoch 36 - Train Loss: 0.0002, Train Acc: 0.9917, Val Loss: 0.0004, Val Acc: 0.9856\n",
      "Epoch 37 - Train Loss: 0.0002, Train Acc: 0.9917, Val Loss: 0.0004, Val Acc: 0.9852\n",
      "Epoch 38 - Train Loss: 0.0002, Train Acc: 0.9918, Val Loss: 0.0004, Val Acc: 0.9861\n",
      "Epoch 39 - Train Loss: 0.0002, Train Acc: 0.9917, Val Loss: 0.0004, Val Acc: 0.9858\n",
      "Epoch 40 - Train Loss: 0.0002, Train Acc: 0.9921, Val Loss: 0.0004, Val Acc: 0.9859\n",
      "Epoch 41 - Train Loss: 0.0002, Train Acc: 0.9924, Val Loss: 0.0004, Val Acc: 0.9852\n",
      "Epoch 42 - Train Loss: 0.0002, Train Acc: 0.9919, Val Loss: 0.0004, Val Acc: 0.9856\n",
      "Epoch 43 - Train Loss: 0.0002, Train Acc: 0.9918, Val Loss: 0.0004, Val Acc: 0.9852\n",
      "Epoch 44 - Train Loss: 0.0002, Train Acc: 0.9922, Val Loss: 0.0004, Val Acc: 0.9854\n",
      "Epoch 45 - Train Loss: 0.0002, Train Acc: 0.9922, Val Loss: 0.0004, Val Acc: 0.9865\n",
      "Epoch 46 - Train Loss: 0.0002, Train Acc: 0.9925, Val Loss: 0.0004, Val Acc: 0.9854\n",
      "Epoch 47 - Train Loss: 0.0002, Train Acc: 0.9918, Val Loss: 0.0004, Val Acc: 0.9855\n",
      "Epoch 48 - Train Loss: 0.0002, Train Acc: 0.9918, Val Loss: 0.0004, Val Acc: 0.9861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 06:02:42,112] Trial 18 finished with value: 0.0003542077033313178 and parameters: {'learning_rate': 0.005042864129910946, 'conv_dropout_rate': 0.03573425332070472, 'linear_dropout_rate': 0.1767250122287085, 'batch_norm': True}. Best is trial 18 with value: 0.0003542077033313178.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Train Loss: 0.0002, Train Acc: 0.9916, Val Loss: 0.0004, Val Acc: 0.9856\n",
      "Epoch 0 - Train Loss: 0.0052, Train Acc: 0.7553, Val Loss: 0.0035, Val Acc: 0.8468\n",
      "Epoch 1 - Train Loss: 0.0026, Train Acc: 0.8821, Val Loss: 0.0029, Val Acc: 0.8703\n",
      "Epoch 2 - Train Loss: 0.0019, Train Acc: 0.9156, Val Loss: 0.0021, Val Acc: 0.9059\n",
      "Epoch 3 - Train Loss: 0.0015, Train Acc: 0.9327, Val Loss: 0.0039, Val Acc: 0.8441\n",
      "Epoch 4 - Train Loss: 0.0013, Train Acc: 0.9432, Val Loss: 0.0015, Val Acc: 0.9361\n",
      "Epoch 5 - Train Loss: 0.0010, Train Acc: 0.9575, Val Loss: 0.0010, Val Acc: 0.9575\n",
      "Epoch 6 - Train Loss: 0.0009, Train Acc: 0.9617, Val Loss: 0.0009, Val Acc: 0.9609\n",
      "Epoch 7 - Train Loss: 0.0008, Train Acc: 0.9639, Val Loss: 0.0009, Val Acc: 0.9614\n",
      "Epoch 8 - Train Loss: 0.0008, Train Acc: 0.9655, Val Loss: 0.0009, Val Acc: 0.9603\n",
      "Epoch 9 - Train Loss: 0.0007, Train Acc: 0.9673, Val Loss: 0.0008, Val Acc: 0.9644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 06:04:31,194] Trial 19 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Train Loss: 0.0057, Train Acc: 0.7277, Val Loss: 0.0045, Val Acc: 0.7940\n",
      "Epoch 1 - Train Loss: 0.0031, Train Acc: 0.8576, Val Loss: 0.0034, Val Acc: 0.8443\n",
      "Epoch 2 - Train Loss: 0.0023, Train Acc: 0.8952, Val Loss: 0.0032, Val Acc: 0.8658\n",
      "Epoch 3 - Train Loss: 0.0018, Train Acc: 0.9178, Val Loss: 0.0050, Val Acc: 0.8331\n",
      "Epoch 4 - Train Loss: 0.0016, Train Acc: 0.9293, Val Loss: 0.0013, Val Acc: 0.9406\n",
      "Epoch 5 - Train Loss: 0.0012, Train Acc: 0.9470, Val Loss: 0.0012, Val Acc: 0.9496\n",
      "Epoch 6 - Train Loss: 0.0011, Train Acc: 0.9502, Val Loss: 0.0009, Val Acc: 0.9641\n",
      "Epoch 7 - Train Loss: 0.0011, Train Acc: 0.9524, Val Loss: 0.0010, Val Acc: 0.9556\n",
      "Epoch 8 - Train Loss: 0.0010, Train Acc: 0.9548, Val Loss: 0.0010, Val Acc: 0.9544\n",
      "Epoch 9 - Train Loss: 0.0010, Train Acc: 0.9569, Val Loss: 0.0007, Val Acc: 0.9692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 06:06:20,309] Trial 20 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Train Loss: 0.0053, Train Acc: 0.7470, Val Loss: 0.0040, Val Acc: 0.8288\n",
      "Epoch 1 - Train Loss: 0.0027, Train Acc: 0.8771, Val Loss: 0.0033, Val Acc: 0.8632\n",
      "Epoch 2 - Train Loss: 0.0020, Train Acc: 0.9119, Val Loss: 0.0024, Val Acc: 0.8956\n",
      "Epoch 3 - Train Loss: 0.0016, Train Acc: 0.9289, Val Loss: 0.0133, Val Acc: 0.6998\n",
      "Epoch 4 - Train Loss: 0.0014, Train Acc: 0.9383, Val Loss: 0.0017, Val Acc: 0.9237\n",
      "Epoch 5 - Train Loss: 0.0010, Train Acc: 0.9553, Val Loss: 0.0010, Val Acc: 0.9601\n",
      "Epoch 6 - Train Loss: 0.0009, Train Acc: 0.9594, Val Loss: 0.0010, Val Acc: 0.9577\n",
      "Epoch 7 - Train Loss: 0.0009, Train Acc: 0.9602, Val Loss: 0.0009, Val Acc: 0.9622\n",
      "Epoch 8 - Train Loss: 0.0009, Train Acc: 0.9617, Val Loss: 0.0010, Val Acc: 0.9585\n",
      "Epoch 9 - Train Loss: 0.0008, Train Acc: 0.9645, Val Loss: 0.0010, Val Acc: 0.9576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 06:08:09,383] Trial 21 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Train Loss: 0.0048, Train Acc: 0.7740, Val Loss: 0.0036, Val Acc: 0.8310\n",
      "Epoch 1 - Train Loss: 0.0023, Train Acc: 0.8965, Val Loss: 0.0054, Val Acc: 0.7766\n",
      "Epoch 2 - Train Loss: 0.0017, Train Acc: 0.9252, Val Loss: 0.0021, Val Acc: 0.9120\n",
      "Epoch 3 - Train Loss: 0.0014, Train Acc: 0.9408, Val Loss: 0.0028, Val Acc: 0.8800\n",
      "Epoch 4 - Train Loss: 0.0011, Train Acc: 0.9500, Val Loss: 0.0016, Val Acc: 0.9308\n",
      "Epoch 5 - Train Loss: 0.0008, Train Acc: 0.9656, Val Loss: 0.0007, Val Acc: 0.9668\n",
      "Epoch 6 - Train Loss: 0.0007, Train Acc: 0.9679, Val Loss: 0.0008, Val Acc: 0.9657\n",
      "Epoch 7 - Train Loss: 0.0007, Train Acc: 0.9703, Val Loss: 0.0011, Val Acc: 0.9548\n",
      "Epoch 8 - Train Loss: 0.0006, Train Acc: 0.9725, Val Loss: 0.0007, Val Acc: 0.9706\n",
      "Epoch 9 - Train Loss: 0.0006, Train Acc: 0.9739, Val Loss: 0.0006, Val Acc: 0.9752\n",
      "Epoch 10 - Train Loss: 0.0005, Train Acc: 0.9794, Val Loss: 0.0005, Val Acc: 0.9793\n",
      "Epoch 11 - Train Loss: 0.0004, Train Acc: 0.9816, Val Loss: 0.0005, Val Acc: 0.9773\n",
      "Epoch 12 - Train Loss: 0.0004, Train Acc: 0.9817, Val Loss: 0.0005, Val Acc: 0.9798\n",
      "Epoch 13 - Train Loss: 0.0004, Train Acc: 0.9825, Val Loss: 0.0004, Val Acc: 0.9816\n",
      "Epoch 14 - Train Loss: 0.0004, Train Acc: 0.9835, Val Loss: 0.0006, Val Acc: 0.9778\n",
      "Epoch 15 - Train Loss: 0.0003, Train Acc: 0.9856, Val Loss: 0.0004, Val Acc: 0.9845\n",
      "Epoch 16 - Train Loss: 0.0003, Train Acc: 0.9864, Val Loss: 0.0005, Val Acc: 0.9810\n",
      "Epoch 17 - Train Loss: 0.0003, Train Acc: 0.9870, Val Loss: 0.0004, Val Acc: 0.9826\n",
      "Epoch 18 - Train Loss: 0.0003, Train Acc: 0.9869, Val Loss: 0.0004, Val Acc: 0.9827\n",
      "Epoch 19 - Train Loss: 0.0003, Train Acc: 0.9873, Val Loss: 0.0004, Val Acc: 0.9830\n",
      "Epoch 20 - Train Loss: 0.0003, Train Acc: 0.9891, Val Loss: 0.0004, Val Acc: 0.9828\n",
      "Epoch 21 - Train Loss: 0.0003, Train Acc: 0.9893, Val Loss: 0.0004, Val Acc: 0.9847\n",
      "Epoch 22 - Train Loss: 0.0002, Train Acc: 0.9895, Val Loss: 0.0004, Val Acc: 0.9853\n",
      "Epoch 23 - Train Loss: 0.0002, Train Acc: 0.9891, Val Loss: 0.0004, Val Acc: 0.9856\n",
      "Epoch 24 - Train Loss: 0.0002, Train Acc: 0.9895, Val Loss: 0.0004, Val Acc: 0.9847\n",
      "Epoch 25 - Train Loss: 0.0002, Train Acc: 0.9901, Val Loss: 0.0004, Val Acc: 0.9852\n",
      "Epoch 26 - Train Loss: 0.0002, Train Acc: 0.9904, Val Loss: 0.0004, Val Acc: 0.9852\n",
      "Epoch 27 - Train Loss: 0.0002, Train Acc: 0.9903, Val Loss: 0.0004, Val Acc: 0.9851\n",
      "Epoch 28 - Train Loss: 0.0002, Train Acc: 0.9902, Val Loss: 0.0004, Val Acc: 0.9848\n",
      "Epoch 29 - Train Loss: 0.0002, Train Acc: 0.9911, Val Loss: 0.0004, Val Acc: 0.9855\n",
      "Epoch 30 - Train Loss: 0.0002, Train Acc: 0.9906, Val Loss: 0.0004, Val Acc: 0.9851\n",
      "Epoch 31 - Train Loss: 0.0002, Train Acc: 0.9909, Val Loss: 0.0004, Val Acc: 0.9859\n",
      "Epoch 32 - Train Loss: 0.0002, Train Acc: 0.9913, Val Loss: 0.0004, Val Acc: 0.9852\n",
      "Epoch 33 - Train Loss: 0.0002, Train Acc: 0.9913, Val Loss: 0.0004, Val Acc: 0.9854\n",
      "Epoch 34 - Train Loss: 0.0002, Train Acc: 0.9913, Val Loss: 0.0004, Val Acc: 0.9856\n",
      "Epoch 35 - Train Loss: 0.0002, Train Acc: 0.9917, Val Loss: 0.0003, Val Acc: 0.9870\n",
      "Epoch 36 - Train Loss: 0.0002, Train Acc: 0.9909, Val Loss: 0.0004, Val Acc: 0.9857\n",
      "Epoch 37 - Train Loss: 0.0002, Train Acc: 0.9915, Val Loss: 0.0004, Val Acc: 0.9856\n",
      "Epoch 38 - Train Loss: 0.0002, Train Acc: 0.9917, Val Loss: 0.0004, Val Acc: 0.9860\n",
      "Epoch 39 - Train Loss: 0.0002, Train Acc: 0.9912, Val Loss: 0.0004, Val Acc: 0.9859\n",
      "Epoch 40 - Train Loss: 0.0002, Train Acc: 0.9915, Val Loss: 0.0004, Val Acc: 0.9862\n",
      "Epoch 41 - Train Loss: 0.0002, Train Acc: 0.9918, Val Loss: 0.0004, Val Acc: 0.9853\n",
      "Epoch 42 - Train Loss: 0.0002, Train Acc: 0.9918, Val Loss: 0.0004, Val Acc: 0.9859\n",
      "Epoch 43 - Train Loss: 0.0002, Train Acc: 0.9921, Val Loss: 0.0004, Val Acc: 0.9857\n",
      "Epoch 44 - Train Loss: 0.0002, Train Acc: 0.9918, Val Loss: 0.0004, Val Acc: 0.9848\n",
      "Epoch 45 - Train Loss: 0.0002, Train Acc: 0.9921, Val Loss: 0.0004, Val Acc: 0.9856\n",
      "Epoch 46 - Train Loss: 0.0002, Train Acc: 0.9920, Val Loss: 0.0004, Val Acc: 0.9850\n",
      "Epoch 47 - Train Loss: 0.0002, Train Acc: 0.9918, Val Loss: 0.0004, Val Acc: 0.9852\n",
      "Epoch 48 - Train Loss: 0.0002, Train Acc: 0.9921, Val Loss: 0.0003, Val Acc: 0.9865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 06:16:23,354] Trial 22 finished with value: 0.0003472898064310797 and parameters: {'learning_rate': 0.005512591195633239, 'conv_dropout_rate': 0.03315120699911577, 'linear_dropout_rate': 0.3282834047386613, 'batch_norm': True}. Best is trial 22 with value: 0.0003472898064310797.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Train Loss: 0.0002, Train Acc: 0.9914, Val Loss: 0.0004, Val Acc: 0.9846\n",
      "Epoch 0 - Train Loss: 0.0049, Train Acc: 0.7706, Val Loss: 0.0041, Val Acc: 0.8110\n",
      "Epoch 1 - Train Loss: 0.0024, Train Acc: 0.8947, Val Loss: 0.0035, Val Acc: 0.8381\n",
      "Epoch 2 - Train Loss: 0.0017, Train Acc: 0.9241, Val Loss: 0.0027, Val Acc: 0.8882\n",
      "Epoch 3 - Train Loss: 0.0014, Train Acc: 0.9393, Val Loss: 0.0045, Val Acc: 0.8398\n",
      "Epoch 4 - Train Loss: 0.0012, Train Acc: 0.9474, Val Loss: 0.0023, Val Acc: 0.9014\n",
      "Epoch 5 - Train Loss: 0.0008, Train Acc: 0.9639, Val Loss: 0.0008, Val Acc: 0.9677\n",
      "Epoch 6 - Train Loss: 0.0008, Train Acc: 0.9664, Val Loss: 0.0009, Val Acc: 0.9622\n",
      "Epoch 7 - Train Loss: 0.0007, Train Acc: 0.9679, Val Loss: 0.0008, Val Acc: 0.9641\n",
      "Epoch 8 - Train Loss: 0.0007, Train Acc: 0.9708, Val Loss: 0.0007, Val Acc: 0.9716\n",
      "Epoch 9 - Train Loss: 0.0006, Train Acc: 0.9725, Val Loss: 0.0008, Val Acc: 0.9685\n",
      "Epoch 10 - Train Loss: 0.0005, Train Acc: 0.9784, Val Loss: 0.0005, Val Acc: 0.9788\n",
      "Epoch 11 - Train Loss: 0.0005, Train Acc: 0.9794, Val Loss: 0.0006, Val Acc: 0.9773\n",
      "Epoch 12 - Train Loss: 0.0005, Train Acc: 0.9803, Val Loss: 0.0005, Val Acc: 0.9800\n",
      "Epoch 13 - Train Loss: 0.0005, Train Acc: 0.9801, Val Loss: 0.0005, Val Acc: 0.9791\n",
      "Epoch 14 - Train Loss: 0.0004, Train Acc: 0.9813, Val Loss: 0.0005, Val Acc: 0.9803\n",
      "Epoch 15 - Train Loss: 0.0004, Train Acc: 0.9838, Val Loss: 0.0004, Val Acc: 0.9825\n",
      "Epoch 16 - Train Loss: 0.0004, Train Acc: 0.9843, Val Loss: 0.0005, Val Acc: 0.9816\n",
      "Epoch 17 - Train Loss: 0.0004, Train Acc: 0.9847, Val Loss: 0.0006, Val Acc: 0.9736\n",
      "Epoch 18 - Train Loss: 0.0004, Train Acc: 0.9847, Val Loss: 0.0005, Val Acc: 0.9809\n",
      "Epoch 19 - Train Loss: 0.0003, Train Acc: 0.9853, Val Loss: 0.0004, Val Acc: 0.9829\n",
      "Epoch 20 - Train Loss: 0.0003, Train Acc: 0.9866, Val Loss: 0.0004, Val Acc: 0.9822\n",
      "Epoch 21 - Train Loss: 0.0003, Train Acc: 0.9874, Val Loss: 0.0004, Val Acc: 0.9845\n",
      "Epoch 22 - Train Loss: 0.0003, Train Acc: 0.9873, Val Loss: 0.0004, Val Acc: 0.9846\n",
      "Epoch 23 - Train Loss: 0.0003, Train Acc: 0.9865, Val Loss: 0.0004, Val Acc: 0.9849\n",
      "Epoch 24 - Train Loss: 0.0003, Train Acc: 0.9879, Val Loss: 0.0004, Val Acc: 0.9850\n",
      "Epoch 25 - Train Loss: 0.0003, Train Acc: 0.9883, Val Loss: 0.0004, Val Acc: 0.9843\n",
      "Epoch 26 - Train Loss: 0.0003, Train Acc: 0.9879, Val Loss: 0.0004, Val Acc: 0.9846\n",
      "Epoch 27 - Train Loss: 0.0003, Train Acc: 0.9885, Val Loss: 0.0004, Val Acc: 0.9850\n",
      "Epoch 28 - Train Loss: 0.0003, Train Acc: 0.9891, Val Loss: 0.0004, Val Acc: 0.9838\n",
      "Epoch 29 - Train Loss: 0.0003, Train Acc: 0.9890, Val Loss: 0.0004, Val Acc: 0.9847\n",
      "Epoch 30 - Train Loss: 0.0003, Train Acc: 0.9889, Val Loss: 0.0004, Val Acc: 0.9841\n",
      "Epoch 31 - Train Loss: 0.0003, Train Acc: 0.9889, Val Loss: 0.0004, Val Acc: 0.9855\n",
      "Epoch 32 - Train Loss: 0.0003, Train Acc: 0.9893, Val Loss: 0.0004, Val Acc: 0.9841\n",
      "Epoch 33 - Train Loss: 0.0002, Train Acc: 0.9890, Val Loss: 0.0004, Val Acc: 0.9845\n",
      "Epoch 34 - Train Loss: 0.0003, Train Acc: 0.9889, Val Loss: 0.0004, Val Acc: 0.9847\n",
      "Epoch 35 - Train Loss: 0.0002, Train Acc: 0.9891, Val Loss: 0.0004, Val Acc: 0.9847\n",
      "Epoch 36 - Train Loss: 0.0003, Train Acc: 0.9888, Val Loss: 0.0004, Val Acc: 0.9854\n",
      "Epoch 37 - Train Loss: 0.0003, Train Acc: 0.9889, Val Loss: 0.0004, Val Acc: 0.9839\n",
      "Epoch 38 - Train Loss: 0.0003, Train Acc: 0.9894, Val Loss: 0.0004, Val Acc: 0.9850\n",
      "Epoch 39 - Train Loss: 0.0002, Train Acc: 0.9895, Val Loss: 0.0004, Val Acc: 0.9843\n",
      "Epoch 40 - Train Loss: 0.0003, Train Acc: 0.9893, Val Loss: 0.0004, Val Acc: 0.9847\n",
      "Epoch 41 - Train Loss: 0.0003, Train Acc: 0.9891, Val Loss: 0.0004, Val Acc: 0.9844\n",
      "Epoch 42 - Train Loss: 0.0002, Train Acc: 0.9895, Val Loss: 0.0004, Val Acc: 0.9852\n",
      "Epoch 43 - Train Loss: 0.0002, Train Acc: 0.9889, Val Loss: 0.0004, Val Acc: 0.9848\n",
      "Epoch 44 - Train Loss: 0.0002, Train Acc: 0.9896, Val Loss: 0.0004, Val Acc: 0.9834\n",
      "Epoch 45 - Train Loss: 0.0002, Train Acc: 0.9897, Val Loss: 0.0004, Val Acc: 0.9853\n",
      "Epoch 46 - Train Loss: 0.0002, Train Acc: 0.9894, Val Loss: 0.0004, Val Acc: 0.9841\n",
      "Epoch 47 - Train Loss: 0.0002, Train Acc: 0.9895, Val Loss: 0.0004, Val Acc: 0.9851\n",
      "Epoch 48 - Train Loss: 0.0002, Train Acc: 0.9893, Val Loss: 0.0004, Val Acc: 0.9852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 06:24:38,932] Trial 23 finished with value: 0.0003613458312090869 and parameters: {'learning_rate': 0.005749106264373852, 'conv_dropout_rate': 0.04618773097744379, 'linear_dropout_rate': 0.3105204746598783, 'batch_norm': True}. Best is trial 22 with value: 0.0003472898064310797.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Train Loss: 0.0002, Train Acc: 0.9891, Val Loss: 0.0004, Val Acc: 0.9843\n",
      "Epoch 0 - Train Loss: 0.0046, Train Acc: 0.7881, Val Loss: 0.0066, Val Acc: 0.7724\n",
      "Epoch 1 - Train Loss: 0.0022, Train Acc: 0.9016, Val Loss: 0.0124, Val Acc: 0.6504\n",
      "Epoch 2 - Train Loss: 0.0016, Train Acc: 0.9260, Val Loss: 0.0020, Val Acc: 0.9057\n",
      "Epoch 3 - Train Loss: 0.0012, Train Acc: 0.9451, Val Loss: 0.0097, Val Acc: 0.8056\n",
      "Epoch 4 - Train Loss: 0.0010, Train Acc: 0.9536, Val Loss: 0.0016, Val Acc: 0.9283\n",
      "Epoch 5 - Train Loss: 0.0007, Train Acc: 0.9688, Val Loss: 0.0008, Val Acc: 0.9649\n",
      "Epoch 6 - Train Loss: 0.0007, Train Acc: 0.9714, Val Loss: 0.0010, Val Acc: 0.9595\n",
      "Epoch 7 - Train Loss: 0.0006, Train Acc: 0.9730, Val Loss: 0.0009, Val Acc: 0.9610\n",
      "Epoch 8 - Train Loss: 0.0006, Train Acc: 0.9753, Val Loss: 0.0007, Val Acc: 0.9714\n",
      "Epoch 9 - Train Loss: 0.0005, Train Acc: 0.9776, Val Loss: 0.0006, Val Acc: 0.9743\n",
      "Epoch 10 - Train Loss: 0.0004, Train Acc: 0.9831, Val Loss: 0.0005, Val Acc: 0.9803\n",
      "Epoch 11 - Train Loss: 0.0004, Train Acc: 0.9844, Val Loss: 0.0006, Val Acc: 0.9764\n",
      "Epoch 12 - Train Loss: 0.0004, Train Acc: 0.9851, Val Loss: 0.0005, Val Acc: 0.9810\n",
      "Epoch 13 - Train Loss: 0.0003, Train Acc: 0.9849, Val Loss: 0.0005, Val Acc: 0.9812\n",
      "Epoch 14 - Train Loss: 0.0003, Train Acc: 0.9867, Val Loss: 0.0006, Val Acc: 0.9765\n",
      "Epoch 15 - Train Loss: 0.0003, Train Acc: 0.9883, Val Loss: 0.0004, Val Acc: 0.9836\n",
      "Epoch 16 - Train Loss: 0.0002, Train Acc: 0.9893, Val Loss: 0.0004, Val Acc: 0.9828\n",
      "Epoch 17 - Train Loss: 0.0003, Train Acc: 0.9893, Val Loss: 0.0005, Val Acc: 0.9822\n",
      "Epoch 18 - Train Loss: 0.0002, Train Acc: 0.9898, Val Loss: 0.0004, Val Acc: 0.9839\n",
      "Epoch 19 - Train Loss: 0.0002, Train Acc: 0.9905, Val Loss: 0.0004, Val Acc: 0.9835\n",
      "Epoch 20 - Train Loss: 0.0002, Train Acc: 0.9918, Val Loss: 0.0004, Val Acc: 0.9840\n",
      "Epoch 21 - Train Loss: 0.0002, Train Acc: 0.9916, Val Loss: 0.0004, Val Acc: 0.9859\n",
      "Epoch 22 - Train Loss: 0.0002, Train Acc: 0.9919, Val Loss: 0.0004, Val Acc: 0.9856\n",
      "Epoch 23 - Train Loss: 0.0002, Train Acc: 0.9919, Val Loss: 0.0004, Val Acc: 0.9846\n",
      "Epoch 24 - Train Loss: 0.0002, Train Acc: 0.9929, Val Loss: 0.0004, Val Acc: 0.9846\n",
      "Epoch 25 - Train Loss: 0.0002, Train Acc: 0.9928, Val Loss: 0.0004, Val Acc: 0.9847\n",
      "Epoch 26 - Train Loss: 0.0002, Train Acc: 0.9927, Val Loss: 0.0004, Val Acc: 0.9854\n",
      "Epoch 27 - Train Loss: 0.0002, Train Acc: 0.9931, Val Loss: 0.0004, Val Acc: 0.9851\n",
      "Epoch 28 - Train Loss: 0.0002, Train Acc: 0.9933, Val Loss: 0.0004, Val Acc: 0.9849\n",
      "Epoch 29 - Train Loss: 0.0002, Train Acc: 0.9931, Val Loss: 0.0004, Val Acc: 0.9851\n",
      "Epoch 30 - Train Loss: 0.0002, Train Acc: 0.9930, Val Loss: 0.0004, Val Acc: 0.9853\n",
      "Epoch 31 - Train Loss: 0.0002, Train Acc: 0.9933, Val Loss: 0.0004, Val Acc: 0.9858\n",
      "Epoch 32 - Train Loss: 0.0002, Train Acc: 0.9934, Val Loss: 0.0004, Val Acc: 0.9855\n",
      "Epoch 33 - Train Loss: 0.0002, Train Acc: 0.9935, Val Loss: 0.0004, Val Acc: 0.9860\n",
      "Epoch 34 - Train Loss: 0.0002, Train Acc: 0.9935, Val Loss: 0.0004, Val Acc: 0.9860\n",
      "Epoch 35 - Train Loss: 0.0002, Train Acc: 0.9940, Val Loss: 0.0004, Val Acc: 0.9863\n",
      "Epoch 36 - Train Loss: 0.0002, Train Acc: 0.9936, Val Loss: 0.0004, Val Acc: 0.9866\n",
      "Epoch 37 - Train Loss: 0.0002, Train Acc: 0.9932, Val Loss: 0.0004, Val Acc: 0.9861\n",
      "Epoch 38 - Train Loss: 0.0002, Train Acc: 0.9937, Val Loss: 0.0004, Val Acc: 0.9866\n",
      "Epoch 39 - Train Loss: 0.0002, Train Acc: 0.9936, Val Loss: 0.0004, Val Acc: 0.9862\n",
      "Epoch 40 - Train Loss: 0.0002, Train Acc: 0.9940, Val Loss: 0.0004, Val Acc: 0.9860\n",
      "Epoch 41 - Train Loss: 0.0002, Train Acc: 0.9937, Val Loss: 0.0004, Val Acc: 0.9856\n",
      "Epoch 42 - Train Loss: 0.0001, Train Acc: 0.9938, Val Loss: 0.0004, Val Acc: 0.9855\n",
      "Epoch 43 - Train Loss: 0.0002, Train Acc: 0.9936, Val Loss: 0.0004, Val Acc: 0.9854\n",
      "Epoch 44 - Train Loss: 0.0001, Train Acc: 0.9940, Val Loss: 0.0004, Val Acc: 0.9858\n",
      "Epoch 45 - Train Loss: 0.0002, Train Acc: 0.9940, Val Loss: 0.0004, Val Acc: 0.9864\n",
      "Epoch 46 - Train Loss: 0.0001, Train Acc: 0.9939, Val Loss: 0.0004, Val Acc: 0.9858\n",
      "Epoch 47 - Train Loss: 0.0001, Train Acc: 0.9941, Val Loss: 0.0004, Val Acc: 0.9860\n",
      "Epoch 48 - Train Loss: 0.0002, Train Acc: 0.9940, Val Loss: 0.0004, Val Acc: 0.9861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 06:32:54,395] Trial 24 finished with value: 0.00036741238105708867 and parameters: {'learning_rate': 0.005334789431648513, 'conv_dropout_rate': 0.027618367841358068, 'linear_dropout_rate': 0.1940965654873974, 'batch_norm': True}. Best is trial 22 with value: 0.0003472898064310797.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Train Loss: 0.0002, Train Acc: 0.9936, Val Loss: 0.0004, Val Acc: 0.9864\n",
      "Epoch 0 - Train Loss: 0.0064, Train Acc: 0.6975, Val Loss: 0.0042, Val Acc: 0.8125\n",
      "Epoch 1 - Train Loss: 0.0034, Train Acc: 0.8476, Val Loss: 0.0070, Val Acc: 0.6982\n",
      "Epoch 2 - Train Loss: 0.0025, Train Acc: 0.8877, Val Loss: 0.0029, Val Acc: 0.8605\n",
      "Epoch 3 - Train Loss: 0.0021, Train Acc: 0.9069, Val Loss: 0.0032, Val Acc: 0.8673\n",
      "Epoch 4 - Train Loss: 0.0018, Train Acc: 0.9194, Val Loss: 0.0018, Val Acc: 0.9221\n",
      "Epoch 5 - Train Loss: 0.0014, Train Acc: 0.9386, Val Loss: 0.0022, Val Acc: 0.9025\n",
      "Epoch 6 - Train Loss: 0.0013, Train Acc: 0.9441, Val Loss: 0.0011, Val Acc: 0.9527\n",
      "Epoch 7 - Train Loss: 0.0012, Train Acc: 0.9462, Val Loss: 0.0013, Val Acc: 0.9445\n",
      "Epoch 8 - Train Loss: 0.0012, Train Acc: 0.9496, Val Loss: 0.0011, Val Acc: 0.9542\n",
      "Epoch 9 - Train Loss: 0.0011, Train Acc: 0.9513, Val Loss: 0.0010, Val Acc: 0.9546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 06:34:42,973] Trial 25 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Train Loss: 0.0050, Train Acc: 0.7660, Val Loss: 0.0034, Val Acc: 0.8495\n",
      "Epoch 1 - Train Loss: 0.0025, Train Acc: 0.8876, Val Loss: 0.0031, Val Acc: 0.8632\n",
      "Epoch 2 - Train Loss: 0.0018, Train Acc: 0.9200, Val Loss: 0.0021, Val Acc: 0.9132\n",
      "Epoch 3 - Train Loss: 0.0014, Train Acc: 0.9374, Val Loss: 0.0014, Val Acc: 0.9384\n",
      "Epoch 4 - Train Loss: 0.0012, Train Acc: 0.9454, Val Loss: 0.0013, Val Acc: 0.9426\n",
      "Epoch 5 - Train Loss: 0.0009, Train Acc: 0.9613, Val Loss: 0.0008, Val Acc: 0.9667\n",
      "Epoch 6 - Train Loss: 0.0008, Train Acc: 0.9659, Val Loss: 0.0007, Val Acc: 0.9681\n",
      "Epoch 7 - Train Loss: 0.0008, Train Acc: 0.9662, Val Loss: 0.0008, Val Acc: 0.9672\n",
      "Epoch 8 - Train Loss: 0.0007, Train Acc: 0.9676, Val Loss: 0.0007, Val Acc: 0.9715\n",
      "Epoch 9 - Train Loss: 0.0007, Train Acc: 0.9697, Val Loss: 0.0007, Val Acc: 0.9700\n",
      "Epoch 10 - Train Loss: 0.0006, Train Acc: 0.9756, Val Loss: 0.0005, Val Acc: 0.9800\n",
      "Epoch 11 - Train Loss: 0.0005, Train Acc: 0.9775, Val Loss: 0.0005, Val Acc: 0.9779\n",
      "Epoch 12 - Train Loss: 0.0005, Train Acc: 0.9783, Val Loss: 0.0005, Val Acc: 0.9779\n",
      "Epoch 13 - Train Loss: 0.0005, Train Acc: 0.9782, Val Loss: 0.0005, Val Acc: 0.9792\n",
      "Epoch 14 - Train Loss: 0.0005, Train Acc: 0.9786, Val Loss: 0.0005, Val Acc: 0.9777\n",
      "Epoch 15 - Train Loss: 0.0004, Train Acc: 0.9813, Val Loss: 0.0004, Val Acc: 0.9820\n",
      "Epoch 16 - Train Loss: 0.0004, Train Acc: 0.9815, Val Loss: 0.0005, Val Acc: 0.9778\n",
      "Epoch 17 - Train Loss: 0.0004, Train Acc: 0.9828, Val Loss: 0.0005, Val Acc: 0.9806\n",
      "Epoch 18 - Train Loss: 0.0004, Train Acc: 0.9826, Val Loss: 0.0005, Val Acc: 0.9800\n",
      "Epoch 19 - Train Loss: 0.0004, Train Acc: 0.9830, Val Loss: 0.0004, Val Acc: 0.9836\n",
      "Epoch 20 - Train Loss: 0.0004, Train Acc: 0.9845, Val Loss: 0.0004, Val Acc: 0.9817\n",
      "Epoch 21 - Train Loss: 0.0004, Train Acc: 0.9846, Val Loss: 0.0004, Val Acc: 0.9832\n",
      "Epoch 22 - Train Loss: 0.0003, Train Acc: 0.9850, Val Loss: 0.0004, Val Acc: 0.9829\n",
      "Epoch 23 - Train Loss: 0.0003, Train Acc: 0.9844, Val Loss: 0.0004, Val Acc: 0.9831\n",
      "Epoch 24 - Train Loss: 0.0003, Train Acc: 0.9847, Val Loss: 0.0004, Val Acc: 0.9834\n",
      "Epoch 25 - Train Loss: 0.0003, Train Acc: 0.9853, Val Loss: 0.0004, Val Acc: 0.9837\n",
      "Epoch 26 - Train Loss: 0.0003, Train Acc: 0.9855, Val Loss: 0.0004, Val Acc: 0.9831\n",
      "Epoch 27 - Train Loss: 0.0003, Train Acc: 0.9856, Val Loss: 0.0004, Val Acc: 0.9843\n",
      "Epoch 28 - Train Loss: 0.0003, Train Acc: 0.9860, Val Loss: 0.0004, Val Acc: 0.9823\n",
      "Epoch 29 - Train Loss: 0.0003, Train Acc: 0.9864, Val Loss: 0.0004, Val Acc: 0.9842\n",
      "Epoch 30 - Train Loss: 0.0003, Train Acc: 0.9863, Val Loss: 0.0004, Val Acc: 0.9839\n",
      "Epoch 31 - Train Loss: 0.0003, Train Acc: 0.9864, Val Loss: 0.0004, Val Acc: 0.9833\n",
      "Epoch 32 - Train Loss: 0.0003, Train Acc: 0.9862, Val Loss: 0.0004, Val Acc: 0.9833\n",
      "Epoch 33 - Train Loss: 0.0003, Train Acc: 0.9859, Val Loss: 0.0004, Val Acc: 0.9839\n",
      "Epoch 34 - Train Loss: 0.0003, Train Acc: 0.9865, Val Loss: 0.0004, Val Acc: 0.9847\n",
      "Epoch 35 - Train Loss: 0.0003, Train Acc: 0.9869, Val Loss: 0.0004, Val Acc: 0.9840\n",
      "Epoch 36 - Train Loss: 0.0003, Train Acc: 0.9866, Val Loss: 0.0004, Val Acc: 0.9841\n",
      "Epoch 37 - Train Loss: 0.0003, Train Acc: 0.9868, Val Loss: 0.0004, Val Acc: 0.9843\n",
      "Epoch 38 - Train Loss: 0.0003, Train Acc: 0.9869, Val Loss: 0.0004, Val Acc: 0.9851\n",
      "Epoch 39 - Train Loss: 0.0003, Train Acc: 0.9864, Val Loss: 0.0004, Val Acc: 0.9841\n",
      "Epoch 40 - Train Loss: 0.0003, Train Acc: 0.9865, Val Loss: 0.0004, Val Acc: 0.9845\n",
      "Epoch 41 - Train Loss: 0.0003, Train Acc: 0.9866, Val Loss: 0.0004, Val Acc: 0.9838\n",
      "Epoch 42 - Train Loss: 0.0003, Train Acc: 0.9872, Val Loss: 0.0004, Val Acc: 0.9841\n",
      "Epoch 43 - Train Loss: 0.0003, Train Acc: 0.9869, Val Loss: 0.0004, Val Acc: 0.9841\n",
      "Epoch 44 - Train Loss: 0.0003, Train Acc: 0.9862, Val Loss: 0.0004, Val Acc: 0.9848\n",
      "Epoch 45 - Train Loss: 0.0003, Train Acc: 0.9869, Val Loss: 0.0004, Val Acc: 0.9848\n",
      "Epoch 46 - Train Loss: 0.0003, Train Acc: 0.9870, Val Loss: 0.0004, Val Acc: 0.9843\n",
      "Epoch 47 - Train Loss: 0.0003, Train Acc: 0.9872, Val Loss: 0.0004, Val Acc: 0.9839\n",
      "Epoch 48 - Train Loss: 0.0003, Train Acc: 0.9866, Val Loss: 0.0004, Val Acc: 0.9848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 06:42:59,508] Trial 26 finished with value: 0.00036559579493638064 and parameters: {'learning_rate': 0.006180948287441055, 'conv_dropout_rate': 0.09356182520249587, 'linear_dropout_rate': 0.011207982219551549, 'batch_norm': True}. Best is trial 22 with value: 0.0003472898064310797.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Train Loss: 0.0003, Train Acc: 0.9869, Val Loss: 0.0004, Val Acc: 0.9847\n",
      "Epoch 0 - Train Loss: 0.0171, Train Acc: 0.1407, Val Loss: 0.0172, Val Acc: 0.1431\n",
      "Epoch 1 - Train Loss: 0.0131, Train Acc: 0.3474, Val Loss: 0.0114, Val Acc: 0.4423\n",
      "Epoch 2 - Train Loss: 0.0088, Train Acc: 0.5519, Val Loss: 0.0095, Val Acc: 0.5213\n",
      "Epoch 3 - Train Loss: 0.0077, Train Acc: 0.6031, Val Loss: 0.0069, Val Acc: 0.6552\n",
      "Epoch 4 - Train Loss: 0.0067, Train Acc: 0.6583, Val Loss: 0.0080, Val Acc: 0.6058\n",
      "Epoch 5 - Train Loss: 0.0057, Train Acc: 0.7142, Val Loss: 0.0055, Val Acc: 0.7286\n",
      "Epoch 6 - Train Loss: 0.0052, Train Acc: 0.7385, Val Loss: 0.0052, Val Acc: 0.7236\n",
      "Epoch 7 - Train Loss: 0.0048, Train Acc: 0.7648, Val Loss: 0.0043, Val Acc: 0.7945\n",
      "Epoch 8 - Train Loss: 0.0044, Train Acc: 0.7859, Val Loss: 0.0045, Val Acc: 0.7698\n",
      "Epoch 9 - Train Loss: 0.0042, Train Acc: 0.7991, Val Loss: 0.0038, Val Acc: 0.8183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 06:44:45,336] Trial 27 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Train Loss: 0.0046, Train Acc: 0.7874, Val Loss: 0.0111, Val Acc: 0.7067\n",
      "Epoch 1 - Train Loss: 0.0022, Train Acc: 0.9033, Val Loss: 0.0113, Val Acc: 0.7268\n",
      "Epoch 2 - Train Loss: 0.0016, Train Acc: 0.9304, Val Loss: 0.0039, Val Acc: 0.8327\n",
      "Epoch 3 - Train Loss: 0.0013, Train Acc: 0.9433, Val Loss: 0.0041, Val Acc: 0.8519\n",
      "Epoch 4 - Train Loss: 0.0010, Train Acc: 0.9537, Val Loss: 0.0014, Val Acc: 0.9368\n",
      "Epoch 5 - Train Loss: 0.0007, Train Acc: 0.9690, Val Loss: 0.0010, Val Acc: 0.9592\n",
      "Epoch 6 - Train Loss: 0.0007, Train Acc: 0.9718, Val Loss: 0.0007, Val Acc: 0.9736\n",
      "Epoch 7 - Train Loss: 0.0006, Train Acc: 0.9743, Val Loss: 0.0007, Val Acc: 0.9698\n",
      "Epoch 8 - Train Loss: 0.0005, Train Acc: 0.9757, Val Loss: 0.0006, Val Acc: 0.9734\n",
      "Epoch 9 - Train Loss: 0.0005, Train Acc: 0.9781, Val Loss: 0.0007, Val Acc: 0.9723\n",
      "Epoch 10 - Train Loss: 0.0004, Train Acc: 0.9828, Val Loss: 0.0005, Val Acc: 0.9814\n",
      "Epoch 11 - Train Loss: 0.0003, Train Acc: 0.9845, Val Loss: 0.0005, Val Acc: 0.9808\n",
      "Epoch 12 - Train Loss: 0.0003, Train Acc: 0.9857, Val Loss: 0.0006, Val Acc: 0.9783\n",
      "Epoch 13 - Train Loss: 0.0003, Train Acc: 0.9853, Val Loss: 0.0004, Val Acc: 0.9816\n",
      "Epoch 14 - Train Loss: 0.0003, Train Acc: 0.9862, Val Loss: 0.0004, Val Acc: 0.9845\n",
      "Epoch 15 - Train Loss: 0.0002, Train Acc: 0.9894, Val Loss: 0.0004, Val Acc: 0.9851\n",
      "Epoch 16 - Train Loss: 0.0002, Train Acc: 0.9899, Val Loss: 0.0004, Val Acc: 0.9834\n",
      "Epoch 17 - Train Loss: 0.0002, Train Acc: 0.9905, Val Loss: 0.0004, Val Acc: 0.9850\n",
      "Epoch 18 - Train Loss: 0.0002, Train Acc: 0.9902, Val Loss: 0.0004, Val Acc: 0.9846\n",
      "Epoch 19 - Train Loss: 0.0002, Train Acc: 0.9908, Val Loss: 0.0004, Val Acc: 0.9827\n",
      "Epoch 20 - Train Loss: 0.0002, Train Acc: 0.9921, Val Loss: 0.0004, Val Acc: 0.9852\n",
      "Epoch 21 - Train Loss: 0.0002, Train Acc: 0.9924, Val Loss: 0.0004, Val Acc: 0.9858\n",
      "Epoch 22 - Train Loss: 0.0002, Train Acc: 0.9927, Val Loss: 0.0003, Val Acc: 0.9859\n",
      "Epoch 23 - Train Loss: 0.0002, Train Acc: 0.9923, Val Loss: 0.0003, Val Acc: 0.9871\n",
      "Epoch 24 - Train Loss: 0.0002, Train Acc: 0.9930, Val Loss: 0.0004, Val Acc: 0.9864\n",
      "Epoch 25 - Train Loss: 0.0002, Train Acc: 0.9929, Val Loss: 0.0003, Val Acc: 0.9860\n",
      "Epoch 26 - Train Loss: 0.0002, Train Acc: 0.9935, Val Loss: 0.0003, Val Acc: 0.9857\n",
      "Epoch 27 - Train Loss: 0.0002, Train Acc: 0.9932, Val Loss: 0.0003, Val Acc: 0.9870\n",
      "Epoch 28 - Train Loss: 0.0002, Train Acc: 0.9935, Val Loss: 0.0004, Val Acc: 0.9860\n",
      "Epoch 29 - Train Loss: 0.0002, Train Acc: 0.9936, Val Loss: 0.0003, Val Acc: 0.9870\n",
      "Epoch 30 - Train Loss: 0.0001, Train Acc: 0.9941, Val Loss: 0.0003, Val Acc: 0.9862\n",
      "Epoch 31 - Train Loss: 0.0001, Train Acc: 0.9942, Val Loss: 0.0003, Val Acc: 0.9872\n",
      "Epoch 32 - Train Loss: 0.0001, Train Acc: 0.9940, Val Loss: 0.0003, Val Acc: 0.9868\n",
      "Epoch 33 - Train Loss: 0.0001, Train Acc: 0.9939, Val Loss: 0.0003, Val Acc: 0.9859\n",
      "Epoch 34 - Train Loss: 0.0001, Train Acc: 0.9941, Val Loss: 0.0003, Val Acc: 0.9874\n",
      "Epoch 35 - Train Loss: 0.0001, Train Acc: 0.9942, Val Loss: 0.0003, Val Acc: 0.9877\n",
      "Epoch 36 - Train Loss: 0.0001, Train Acc: 0.9942, Val Loss: 0.0003, Val Acc: 0.9862\n",
      "Epoch 37 - Train Loss: 0.0001, Train Acc: 0.9942, Val Loss: 0.0003, Val Acc: 0.9865\n",
      "Epoch 38 - Train Loss: 0.0001, Train Acc: 0.9945, Val Loss: 0.0003, Val Acc: 0.9874\n",
      "Epoch 39 - Train Loss: 0.0001, Train Acc: 0.9943, Val Loss: 0.0003, Val Acc: 0.9862\n",
      "Epoch 40 - Train Loss: 0.0001, Train Acc: 0.9943, Val Loss: 0.0003, Val Acc: 0.9875\n",
      "Epoch 41 - Train Loss: 0.0002, Train Acc: 0.9940, Val Loss: 0.0003, Val Acc: 0.9868\n",
      "Epoch 42 - Train Loss: 0.0001, Train Acc: 0.9942, Val Loss: 0.0003, Val Acc: 0.9868\n",
      "Epoch 43 - Train Loss: 0.0001, Train Acc: 0.9944, Val Loss: 0.0003, Val Acc: 0.9871\n",
      "Epoch 44 - Train Loss: 0.0001, Train Acc: 0.9947, Val Loss: 0.0003, Val Acc: 0.9861\n",
      "Epoch 45 - Train Loss: 0.0001, Train Acc: 0.9947, Val Loss: 0.0003, Val Acc: 0.9861\n",
      "Epoch 46 - Train Loss: 0.0001, Train Acc: 0.9947, Val Loss: 0.0003, Val Acc: 0.9870\n",
      "Epoch 47 - Train Loss: 0.0001, Train Acc: 0.9948, Val Loss: 0.0003, Val Acc: 0.9869\n",
      "Epoch 48 - Train Loss: 0.0001, Train Acc: 0.9943, Val Loss: 0.0003, Val Acc: 0.9877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 06:52:58,145] Trial 28 finished with value: 0.0003236441649516105 and parameters: {'learning_rate': 0.006932958222033313, 'conv_dropout_rate': 0.028018749034249144, 'linear_dropout_rate': 0.2204751279746272, 'batch_norm': True}. Best is trial 28 with value: 0.0003236441649516105.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Train Loss: 0.0001, Train Acc: 0.9943, Val Loss: 0.0003, Val Acc: 0.9868\n",
      "Epoch 0 - Train Loss: 0.0050, Train Acc: 0.7653, Val Loss: 0.0044, Val Acc: 0.7967\n",
      "Epoch 1 - Train Loss: 0.0024, Train Acc: 0.8902, Val Loss: 0.0046, Val Acc: 0.8125\n",
      "Epoch 2 - Train Loss: 0.0018, Train Acc: 0.9207, Val Loss: 0.0028, Val Acc: 0.8729\n",
      "Epoch 3 - Train Loss: 0.0014, Train Acc: 0.9383, Val Loss: 0.0023, Val Acc: 0.9092\n",
      "Epoch 4 - Train Loss: 0.0012, Train Acc: 0.9460, Val Loss: 0.0020, Val Acc: 0.9119\n",
      "Epoch 5 - Train Loss: 0.0009, Train Acc: 0.9626, Val Loss: 0.0013, Val Acc: 0.9440\n",
      "Epoch 6 - Train Loss: 0.0008, Train Acc: 0.9661, Val Loss: 0.0008, Val Acc: 0.9633\n",
      "Epoch 7 - Train Loss: 0.0007, Train Acc: 0.9682, Val Loss: 0.0012, Val Acc: 0.9486\n",
      "Epoch 8 - Train Loss: 0.0007, Train Acc: 0.9699, Val Loss: 0.0007, Val Acc: 0.9701\n",
      "Epoch 9 - Train Loss: 0.0006, Train Acc: 0.9715, Val Loss: 0.0008, Val Acc: 0.9673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 06:54:46,965] Trial 29 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Train Loss: 0.0169, Train Acc: 0.1559, Val Loss: 0.0148, Val Acc: 0.2448\n",
      "Epoch 1 - Train Loss: 0.0116, Train Acc: 0.4142, Val Loss: 0.0089, Val Acc: 0.5597\n",
      "Epoch 2 - Train Loss: 0.0081, Train Acc: 0.5917, Val Loss: 0.0078, Val Acc: 0.6051\n",
      "Epoch 3 - Train Loss: 0.0070, Train Acc: 0.6513, Val Loss: 0.0063, Val Acc: 0.6947\n",
      "Epoch 4 - Train Loss: 0.0060, Train Acc: 0.7089, Val Loss: 0.0053, Val Acc: 0.7398\n",
      "Epoch 5 - Train Loss: 0.0047, Train Acc: 0.7717, Val Loss: 0.0049, Val Acc: 0.7680\n",
      "Epoch 6 - Train Loss: 0.0042, Train Acc: 0.7988, Val Loss: 0.0041, Val Acc: 0.8073\n",
      "Epoch 7 - Train Loss: 0.0038, Train Acc: 0.8194, Val Loss: 0.0035, Val Acc: 0.8303\n",
      "Epoch 8 - Train Loss: 0.0035, Train Acc: 0.8318, Val Loss: 0.0035, Val Acc: 0.8345\n",
      "Epoch 9 - Train Loss: 0.0033, Train Acc: 0.8459, Val Loss: 0.0030, Val Acc: 0.8588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 06:56:31,908] Trial 30 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.006932958222033313, 'conv_dropout_rate': 0.028018749034249144, 'linear_dropout_rate': 0.2204751279746272, 'batch_norm': True}\n",
      "Best Value: 0.0003236441649516105\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.MedianPruner(n_startup_trials=3, n_warmup_steps=10),\n",
    "                           storage=storage_url, study_name=study_name, load_if_exists=True)\n",
    "study.optimize(objective, n_trials = 30)\n",
    "\n",
    "# 結果の出力\n",
    "print(\"Best Parameters:\", study.best_params)\n",
    "print(\"Best Value:\", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e96a1076-0bc1-46e2-a200-7df518a932d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Trial Number: 28\n",
      "Best Parameters: {'learning_rate': 0.006932958222033313, 'conv_dropout_rate': 0.028018749034249144, 'linear_dropout_rate': 0.2204751279746272, 'batch_norm': True}\n"
     ]
    }
   ],
   "source": [
    "# 保存済みのOptuna Studyをロード\n",
    "# study_name=\"250129_PathMNIST_2blocks\"\n",
    "study = optuna.load_study(study_name=study_name, storage=storage_url)\n",
    "\n",
    "# ベストトライアルを取得\n",
    "best_trial = study.best_trial\n",
    "print(f\"Best Trial Number: {best_trial.number}\")\n",
    "print(f\"Best Parameters: {best_trial.params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34ddc090-fa68-413d-b889-06b23b83cd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0037, Test Accuracy: 0.8925\n"
     ]
    }
   ],
   "source": [
    "# ベストトライアルのモデルパスを取得\n",
    "best_model_path = best_trial.user_attrs.get(\"model_path\", None)\n",
    "\n",
    "# モデル定義\n",
    "model = SmallVGG_3blocks(\n",
    "    in_channels=3,\n",
    "    num_classes=9,\n",
    "    conv_dropout_rate=best_trial.params[\"conv_dropout_rate\"],\n",
    "    linear_dropout_rate=best_trial.params[\"linear_dropout_rate\"],\n",
    "    batch_norm=best_trial.params[\"batch_norm\"],\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=best_trial.params[\"learning_rate\"],  momentum=0.9)\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# モデルロードと評価\n",
    "model.load_state_dict(torch.load(best_model_path, weights_only=True))\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "test_correct = 0\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        labels = labels.squeeze().long()  # ラベルを1Dに変換\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        test_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "test_acc = test_correct / len(test_loader.dataset)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db47496-b467-48df-8fa8-aafbbad73e17",
   "metadata": {},
   "source": [
    "## 結果のまとめ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108c87da-99dc-41b0-ba4f-88c4aa6e41e9",
   "metadata": {},
   "source": [
    "| Model   | learning_rate | conv_dropout_rate | linear_dropout_rate | batch_norm | Val Loss | Test Loss | Test Accuracy |\n",
    "|---------|--------------|-------------------|----------------------|------------|---------------|-----------|--------------|\n",
    "| 2blocks | 0.0067       | 0.0342            | 0.2343               | True       | 0.0005        | 0.0061    | 0.8703       |\n",
    "| 3blocks | 0.0069       | 0.0280            | 0.2205               | True       | 0.0003        | 0.0037    | 0.8925       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432a31a5-1243-40b8-b375-43cc87ee3b57",
   "metadata": {
    "id": "NGdSA5p6cwwZ"
   },
   "source": [
    "- 今回は3blocks構成のモデルで検証データのLoss：0.0003、テストデータのLoss：0.0037、Accuracy：0.8925と一番良い性能が出た。\n",
    "- 前回のパラメータ調整では、2blocks構成で各畳み込み層の後にbatch_normを追加し、linear_dropout_rate(0.4)にしたモデルにおいて、検証データのLoss：0.0006, テストデータでのLoss：0.0041、Accuracy：0.8957と高かった。\n",
    "- Optunaチューニングを用いた結果も同等の性能が確認された。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
